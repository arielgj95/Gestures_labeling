{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### LIBRARIES LOAD\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "#from pytorch_modelsize import SizeEstimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq\n",
    "import os\n",
    "import time\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import codecs\n",
    "import hickle as hkl\n",
    "import pickle\n",
    "import functools\n",
    "import scipy\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "model = CrossEncoder('cross-encoder/stsb-roberta-large')  #stsb-roberta-large\n",
    "#print(model.config.model_name_or_path)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#model = model.to(device)\n",
    "print(device)\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD REFERENCE SENTENCES FOR EACH GESTURE AND DESCRIPTION OF GESTURES. RUN THIS CELL AT THE START\n",
    "\n",
    "reference_sentence_greet = [\"hello\", \"hey there\", \"I greet you\", \"Greetings!\", \"Namaste\", \"good day\", \"goodbye\", \"see you\"]\n",
    "reference_sentence_faster = [\"this is boring, go faster\", \"please talk faster\", \"continue please\", \"I have no time, finish the speech please\"]\n",
    "reference_sentence_goaway = [\"let me alone\", \"do not disturb me\", \"I need some privacy\", \"go away\", \"you annoyed me\"]\n",
    "reference_sentence_cigarette = [\"do you like to smoke?\", \"smoking is not healthy\", \"cigarette\", \"where are cigarettes\",\n",
    "                                \"smoking is the cause of many diseases\", \"I like to smoke\", \"smoking\"]\n",
    "reference_sentence_idk = [\"I don't know\", \"I don't understand\"]\n",
    "reference_sentence_no = [\"I don't agree\", \"I don't like it\", \"no\", \"I don't want\", \"it's not ok\", \"It is not the same for me\"] #never #I can't, I' wasn't\n",
    "reference_sentence_yes = [\"yes.\", \"yeah.\", \"I agree\", \"It is the same for me\", \"I like it\", \"ok\", \"I want it\"] #right\n",
    "reference_sentence_deitic = [\"see there\", \"see that\", \"look at your left\", \"look at your right\", \"look over your head\", \"look behind\"] #you have, look this thing. pay attention because sometimes this and that referes to something that is not present\n",
    "reference_sentence_stop = [\"slow down\", \"don't go further\", \"stop\", \"go slower\", \"stop talking\", \"talk slower\"]\n",
    "reference_sentence_adaptor = [\"my arm\", \"my hand\", \"my head\",\"my knee\", \"my leg\", \"my stomach\", \"my chest\",\n",
    "                             \"my feet\", \"my eyes\", \"me\", \"I am\"] #I have\n",
    "reference_sentence_telephone = [\"I have to do a call\", \"I want to call my friend\", \"telephone number\", \"call me\"] #pay attention that call is not related to phone\n",
    "reference_sentence_come = [\"come to me\", \"come here\", \"come closer\"]\n",
    "reference_sentence_drink = [\"do you want a drink?\", \"I want to drink\", \"I want some water\", \"can you give me a drink?\",\n",
    "                            \"let's take a drink\", \"I am thirsty\",\"let's take a drink\"]\n",
    "reference_sentence_exulting = [\"I won!\", \"I am very happy!\"]\n",
    "reference_sentence_link = [\"they are a couple\", \"they are lovers    \", \"they are similar\", \"they understand each other\"]\n",
    "reference_sentence_apologize = [\"excuse me\", \"please forgive me\", \"I am sorry\"]\n",
    "reference_sentence_attention = [\"pay attention\", \"listen me\", \"look at here\"]\n",
    "reference_sentence_beg = [\"please listen me\", \"I beg you\"]\n",
    "reference_sentence_careless = [\"I don't care\", \"It's not my problem\"]\n",
    "reference_sentence_impossible = [\"How is it possible?\", \"It's impossible\"]\n",
    "reference_sentence_walk= [\"I like to walk\", \"do you like to walk?\", \"I like trekking\", \"I want to do some trekking\",\n",
    "                           \"I like hiking\", \"I often go out for a walk\"]\n",
    "reference_sentence_run = [\"I like to run\",\"do you like to run?\", \"where you will run?\", \"I often run\", \"I want to run later\"] #go,go\n",
    "reference_sentence_thin = [\"that person is thin\", \"I'm thin\", \"that object is thin\"]\n",
    "reference_sentence_later = [\"let's meet later\", \"see you tomorrow\", \"next week\", \"next month\", \"next year\", \"do it later\"]\n",
    "reference_sentence_lot = [\"it is very\", \"there is a lot\", \"it is too much\"]\n",
    "reference_sentence_time = [\"what time is it?\", \"can you tell me the time\", \"It's late\", \"you are late\"]\n",
    "reference_sentence_clap = [\"you did very well!\", \"compliments!\", \"congratulations!\"]\n",
    "reference_sentence_ita = [\"What do you want?\", \"what are you saying?\"]\n",
    "\n",
    "\n",
    "all_refs = {\"r_greet\":reference_sentence_greet, \"r_faster\":reference_sentence_faster, \"r_goaway\":reference_sentence_goaway,\n",
    "           \"r_cigarette\":reference_sentence_cigarette, \"r_idk\":reference_sentence_idk, \"r_no\":reference_sentence_no,\n",
    "           \"r_yes\":reference_sentence_yes, \"r_deitic\":reference_sentence_deitic,\"r_run\":reference_sentence_run,\n",
    "           \"r_stop\":reference_sentence_stop, \"r_adaptor\":reference_sentence_adaptor, \"r_telephone\":reference_sentence_telephone,\n",
    "           \"r_come\":reference_sentence_come, \"r_drink\":reference_sentence_drink, \"r_exulting\":reference_sentence_exulting,\n",
    "            \"r_link\":reference_sentence_link, \"r_apologize\":reference_sentence_apologize, \"r_attention\":reference_sentence_attention,\n",
    "            \"r_beg\":reference_sentence_beg, \"r_careless\":reference_sentence_careless, \"r_impossible\":reference_sentence_impossible,\n",
    "            \"r_walk\":reference_sentence_walk,\"r_thin\":reference_sentence_thin, \"r_later\":reference_sentence_later,\n",
    "            \"r_lot\":reference_sentence_lot, \"r_time\":reference_sentence_time, \"r_clap\":reference_sentence_clap,\"r_ita\":reference_sentence_ita}\n",
    "\n",
    "topic_gesture_description = {\"Greeting\":(\"r_greet\", \"the gesture that you use when you greet people\"), \"Go faster\":(\"r_faster\", \"the gesture that you use when you want someone to \" \\\n",
    "                        \"talk faster (for example by keeping one or two hands ahead, palms facing you and wrist rotating them recursively)\"), \"Go away\":(\"r_goaway\", \"the \" \\\n",
    "                        \"gesture that you use when you want someone to go away from you (for example by keeping one or two hands ahead, palms facing you and by moving the hand back and forth\" \\\n",
    "                        \"starting from the palm facing down)\"), \"Cigarette\":(\"r_cigarette\",\"the gesture that you do when you want to show smoking intentions (bring two fingers\" \\\n",
    "                        \"close to the mouth)\"), \"I don't know\":(\"r_idk\", \"moving the shoulders up and down or move both the arms in front of you with palms facing up, used when you want to\" \\\n",
    "                        \"to show that you don't know something\"), \"No\":(\"r_no\", \"moving the head or a finger left and right when you want to say no\"), \"Yes\":(\"r_yes\",\"moving the head up and \" \\\n",
    "                        \"down when you are agreeing for something or you want to say yes\"), \"Pointing something\":(\"r_deitic\",\"the gesture that you use for indicating something (with head, finger, etc.)\"),\n",
    "                        \"Pointing yourself\":(\"r_adaptor\",\"the gesture that you use to point yourself or a part of your body\"), \"Run\": (\"r_run\",\"moving arms aback and forth while\" \\\n",
    "                        \"maintaining an angle of 90° with the elbow to simulate a running movement\"), \"Walk\":(\"r_walk\",\"moving the index and the middle fingers back and forth to simulate\" \\\n",
    "                        \"a walking movement\"), \"Stop\":(\"r_stop\",\"gesture that you make with arms to tell someone that he as to slow down or stop to do something (for example with arms ahead,\" \\\n",
    "                        \"back of the hand facing you and while moving back and forth)\"), \"Call\":(\"r_telephone\", \"the gesture that you make to simulate a phone call, for instance by keeping\" \\\n",
    "                        \"up the thumb and pinky fingers of one hand and by bringing the hand close to an ear\"), \"Come to me\":(\"r_come\", \"the gesture that you make to someone to tell him/her\" \\\n",
    "                        \"to get close, for instance by keeping an arm straight, palm facing down and my moving banck and forth the fingers\"), \"Drinking\":(\"r_drink\", \"the gesture that you make\" \\\n",
    "                        \"to simulate someone drinking, for instance by keeping up the thumb and pinky fingers of one hand and by bringing the hand close to the mouth\"), \"Exulting\":(\"r_exulting\",\n",
    "                        \"the gesture that you make when you are exulting (for example by putting both your arms up and by putting your keeping your hands in the shape of a fist)\"), \"They are related\": \\\n",
    "                        (\"r_link\",\"the gesture that you make to tell that 2 people or objects are related (for example by making 2 fists and by bringing them close to each other)\"), \"Apologize\":(\"r_apologize\",\n",
    "                        \"the gesture that you make when you ask someone to forgive you, for instance by tilting the upper body or by keeping arms up, palms facing up and elbows bent so that they\" \\\n",
    "                        \"form a 90° angle\"), \"Attention please\":(\"r_attention\",\"the gesture that you make when you ask attention to a person or group of people, for instance by keeping up the\" \\\n",
    "                        \" index while in front of you\"), \"Begging you\":(\"r_beg\",\"the gesture that you make when you are begging someone, for instance by keeping arms in a pray position\"), \"I don't care\": \\\n",
    "                        (\"r_careless\",\"the gesture that you make when you are not interested in something (for example by bringing the back of one hand under the chin and by moving it back and forth)\"), \\\n",
    "                        \"It's impossible\": (\"r_impossible\",\"the gesture that you make when you don't believe that something is possible, for instance by bending one elbow in a 90° \" \\\n",
    "                        \"angle, palm of that arm facing up, and by keeping up only the index and the thumb fingers\"), \"It's thin\": (\"r_thin\",\"keeping only the pinkie of one hand up while maintaining the other\" \\\n",
    "                        \"indexes down and the elbow bent to say that something is thin\"), \"I'll do it later\": (\"r_later\", \"keeping a hand in front of you with only the index up to communicate to someone\" \\\n",
    "                        \"that somthing will happen later and not now\"), \"It's a lot\":(\"r_lot\",\"the gesture that you make when you think that something is a lot, for example by keeping a hand in front of you\" \\\n",
    "                        \"with palm facing you, and by moving it up and down\"), \"It's late\":(\"r_time\", \"indicating the wrist with an index to say that it's late for something\"), \"Congratulations\": (\"r_clap\",\n",
    "                        \"clapping with hands to congratulate with someone\"), \"What are you saying?\":(\"r_ita\", \"keep one hand in front of you, palm facing you, the thumb is over the other fingers and the hand\" \\\n",
    "                        \"is moving back and forth. Used, especially in italy, to tell someone that what is he saying does not make any sense\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### READ AND SAVE A FILE OF SENTENCES CREATED BY CHATGPT\n",
    "\n",
    "def save_sentences(data, filename=\"sentences.pkl\",path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"): #saves data with a given filename in a path\n",
    "    #if not os.path.exists(os.path.join(path,filename)):\n",
    "    print(data)\n",
    "    pickle.dump(data, open(os.path.join(path,filename), 'wb'))\n",
    "    #else:\n",
    "    #    print(\"the file already exists\")\n",
    "\n",
    "def read_sentences(names=[]): #It reads a text file that contains sentences generated by chat-gpt\n",
    "                                  #one or more lines with the format name: sentence\n",
    "    #the person pronounced, while idx is an incremental number that is incremented each time a person talks\n",
    "    path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\generated_sentences_general_4.txt\"\n",
    "    #data = pickle.load(open(path,'rb'))\n",
    "    #sentence = data.strip()\n",
    "    #print(sentence)\n",
    "    sentences = []\n",
    "    counter=0\n",
    "    with open(path, \"r\") as f:\n",
    "        while line := f.readline():\n",
    "            if line == '\\n':\n",
    "                continue\n",
    "            else:\n",
    "                #sentence_splits = line.split('.')[1:] #remove the number of the sentence\n",
    "                #sentence = '.'.join(sentence_splits)\n",
    "                sentence = line\n",
    "                print(sentence,\"\\n\")\n",
    "                sentences.append(sentence)\n",
    "                counter += 1\n",
    "        #lines = [line.rstrip() for line in f]\n",
    "\n",
    "        #data = f.read()\n",
    "        #sentences = data.split('\\n')\n",
    "        #print(len(sentences))\n",
    "        #print(data)\n",
    "        #for line in f:\n",
    "            #sentence = line.strip()\n",
    "            #print(sentence)\n",
    "           # counter +=1\n",
    "            #sentence = sentence.split('.')[1:] # sentences are linke \" 1. sentence \\n \", we take just the sentence part\n",
    "            #print(sentence)\n",
    "            #sentences.append(sentence)\n",
    "    print(counter)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "all_sentences =  read_sentences()\n",
    "'''\n",
    "counter = 0\n",
    "for i in range(len(all_sentences)):\n",
    "    for j in range(len(all_sentences)):\n",
    "        if all_sentences[i] == all_sentences[j]:\n",
    "            #print(all_sentences[i])\n",
    "            counter+=1\n",
    "print(counter)\n",
    "'''\n",
    "#print(all_sentences, len(all_sentences), \"\\n\", all_sentences[1])\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "#print(len(all_sentences), all_sentences)\n",
    "save_sentences(all_sentences,filename=\"sentences_gpt_2.pkl\",path=path)\n",
    "#filename = \"sentences_gpt.pkl\"\n",
    "#data = pickle.load(open(os.path.join(path,filename),'rb'))\n",
    "#print(data, len(data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### READ AND SAVE A FILE OF SUBTITLES ABOUT A CONVERSATION OF TWO PEOPLE\n",
    "\n",
    "def save_sentences(data, filename=\"sentences.pkl\",path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"): #saves data with a given filename in a path\n",
    "    if not os.path.exists(os.path.join(path,filename)):\n",
    "        print(data)\n",
    "        pickle.dump(data, open(os.path.join(path,filename), 'wb'))\n",
    "    else:\n",
    "        print(\"the file already exists\")\n",
    "\n",
    "def read_conversations(names=[]): #It reads a text file that contains subtitles referred to a conversation between 2 or more people. It expects that each time a person talks, we have\n",
    "                                  #one or more lines with the format name: sentence\n",
    "    #the person pronounced, while idx is an incremental number that is incremented each time a person talks\n",
    "    path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\[English] 1.5 HOUR English Conversation Lesson [DownSub.com].txt\"\n",
    "    names = ['Vanessa', 'Dan']\n",
    "    sentences = dict((name,[]) for name in names)\n",
    "    #this dictionary has names of actors as keys, as value they have a tuple (sentence, idx) where sentence is the sentence that\n",
    "\n",
    "    with open(path, \"r\", encoding='utf-8') as f:\n",
    "        subs = [line.rstrip() for line in f if line.rstrip() != '']  #removes \\n and \"\\\" before apostroph\n",
    "        i = 0\n",
    "        turn = 1 # counter that counts the turns ( turn = turn + 1 each time a person finish to talk and the other starts)\n",
    "        while i<len(subs):\n",
    "            actual_name_list = [name for name in names if name+\":\" in subs[i]] #is the person that is currently talking\n",
    "            if len(actual_name_list) == 1: # A person is talking. if the person is the same in the next rows, the sentence is composed by all the the words in these rows put together\n",
    "                name = actual_name_list[0]\n",
    "                sentence = subs[i][len(actual_name_list[0])+1:]    #remove \"Name:\"\n",
    "                #print(i,len(subs)-1)\n",
    "                if i < len(subs)-1: #avoid exceeding the list of subtitles\n",
    "                    i = i + 1\n",
    "                    next_name_list = [name for name in names if name+\":\" in subs[i]]\n",
    "                    while len(next_name_list) == 0 and i != len(subs)-1 or (len(next_name_list)==1 and next_name_list[0] == name):  #same person is talking\n",
    "                        sentence = sentence + ' ' + subs[i]\n",
    "                        i = i + 1\n",
    "                        next_name_list = [name for name in names if name+\":\" in subs[i]]\n",
    "                        #print( len(next_name_list), next_name_list, name, i != len(subs), i , len(subs)-1)\n",
    "                    #print(\"OUT!\")\n",
    "\n",
    "                sentences[name].append((sentence,turn))\n",
    "                turn = turn + 1\n",
    "                #print(i,len(subs)-1)\n",
    "            else: # If we have for any reason more names or no names at the start of the sentence, we skip it\n",
    "                i = i+1\n",
    "    return sentences\n",
    "\n",
    "all_sentences = read_conversations()\n",
    "print(all_sentences)\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "save_sentences(all_sentences,path=path)\n",
    "filename = \"sentences.pkl\"\n",
    "data = pickle.load(open(os.path.join(path,filename),'rb'))\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### CREATE A 300 SENTENCES VERSION OF FINAL RESULT\n",
    "import pickle, random\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "#filename_best = \"new_all_processed_sentences_best.pkl\" #it contains all the possible results; a results is the best words match for each topic for each possible group of words inside the sentence, for all the sentences\n",
    "filename_1 = \"all_similarities.pkl\"\n",
    "filename_2 = \"all_similarities_old.pkl\"\n",
    "res_1 = \"all_similarities_300.pkl\"\n",
    "res_2 = \"all_similarities_300_old.pkl\"\n",
    "new_sentences_file_1 = \"sentences_gpt_300.pkl\"\n",
    "new_sentences_file_2 = \"sentences_gpt_300_old.pkl\"\n",
    "randomized_file_1 = \"sentences_gpt_randomized.pkl\"\n",
    "randomized_file_2 = \"sentences_gpt_randomized_old.pkl\"\n",
    "marco_file = \"Marco_Limone_labelled_sentences_final.pkl\"\n",
    "final_res = \"labelled_sentences_final_old.pkl\"\n",
    "mixed_results = \"all_similarities_600.pkl\"\n",
    "\n",
    "if(os.path.exists(os.path.join(path,new_sentences_file_2))):    #load data already processed\n",
    "    sentences = pickle.load(open(os.path.join(path,new_sentences_file_2),'rb'))\n",
    "else:\n",
    "    print(\"invalid sentences path\")\n",
    "\n",
    "if(os.path.exists(os.path.join(path,marco_file))):    #load data already processed\n",
    "    marco_sentences = pickle.load(open(os.path.join(path,marco_file),'rb'))\n",
    "else:\n",
    "    print(\"invalid moving window labels path\")\n",
    "\n",
    "if(os.path.exists(os.path.join(path,res_2))):    #load data already processed\n",
    "    labels_old = pickle.load(open(os.path.join(path,res_2),'rb'))\n",
    "    print(\"loaded old similarities\")\n",
    "else:\n",
    "    print(\"invalid moving window labels path\")\n",
    "\n",
    "if(os.path.exists(os.path.join(path,res_1))):    #load data already processed\n",
    "    labels = pickle.load(open(os.path.join(path,res_1),'rb'))\n",
    "    print(\"loaded similarities new\")\n",
    "else:\n",
    "    print(\"invalid moving window labels path\")\n",
    "\n",
    "if(os.path.exists(os.path.join(path,randomized_file_2))):    #load data already processed\n",
    "    sentences_randomized = pickle.load(open(os.path.join(path,randomized_file_2),'rb'))\n",
    "else:\n",
    "    print(\"invalid moving window labels path\")\n",
    "\n",
    "unique_sentences = list(set(sentences_randomized))\n",
    "print(sentences[0])\n",
    "\n",
    "#all_sentences_labels = labels_old + labels\n",
    "#print(len(all_sentences_labels))\n",
    "#pickle.dump(all_sentences_labels, open(os.path.join(path, mixed_results),'wb'))\n",
    "#print(unique_sentences,len(unique_sentences))\n",
    "\n",
    "\n",
    "'''  ##### take 300 random sentences\n",
    "indexes = [i for i in range(len(labels))]\n",
    "shuffled_indexes = random.shuffle(indexes)\n",
    "new_res_list = list([labels[i] for i in indexes])[:300]\n",
    "all_sentences = []\n",
    "for i in range(len(new_res_list)):\n",
    "    all_sentences.append(new_res_list[i]['whole sentence'])\n",
    "\n",
    "pickle.dump(all_sentences, open(os.path.join(path,new_sentences_file),'wb'))\n",
    "'''\n",
    "    #print(new_res_list[i]['whole sentence'])\n",
    "#pickle.dump(new_res_list , open(os.path.join(path,res_2),'wb'))\n",
    "\n",
    "#print(all_sentences)\n",
    "#print(\"written\")\n",
    "\n",
    "#print(len(labels), len(new_res_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### take only a subset of sentences\n",
    "'''\n",
    "final_sentences = []\n",
    "#print(len(marco_sentences))\n",
    "for j,element in enumerate(marco_sentences):\n",
    "    sentence = element['sentence']\n",
    "    for label in labels_old:\n",
    "        if label['whole sentence'] == sentence:\n",
    "            final_sentences.append(element)\n",
    "        else:\n",
    "            continue\n",
    "#print(len(final_sentences))\n",
    "marco_sentences = final_sentences[30:47] + final_sentences[:13]\n",
    "#print(marco_sentences)\n",
    "#print(len(marco_sentences))\n",
    "for j,element in enumerate(marco_sentences):\n",
    "    element['sentence_index'] = j + 49*30\n",
    "#print(len(marco_sentences),marco_sentences)\n",
    "s = []\n",
    "for element in marco_sentences:\n",
    "    s.append(element['sentence'])\n",
    "print(len(s),len(set(s)))\n",
    "pickle.dump(marco_sentences, open(os.path.join(path, marco_file),'wb'))\n",
    "pickle.dump(marco_sentences, open(os.path.join(path, final_res),'wb'))\n",
    "'''\n",
    "'''\n",
    "#### count how many sentences of one file are the same of another file\n",
    "counter = 0\n",
    "for label in labels:\n",
    "    result_sentence = label['whole sentence']\n",
    "    for sentence in sentences:\n",
    "        if sentence == result_sentence:\n",
    "            #print(sentence, result_sentence)\n",
    "            counter += 1\n",
    "            break\n",
    "print(\"COUNTER:\",counter)\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### NEW REFERENCE SENTENCES FOR EACH GESTURE AND DESCRIPTION OF GESTURES. RUN THIS CELL ONE TIME BEFORE ANY COMPUTATION\n",
    "\n",
    "###STANDARD\n",
    "'''\n",
    "reference_sentence_greet = [\"hello\", \"goodbye\"] #['greeting']\n",
    "reference_sentence_idk = [\"I don't know\"]#[\"I don't know\"]\n",
    "reference_sentence_no = [\"no\"]\n",
    "reference_sentence_yes = [\"yes\"] #[\"yes\"] #right\n",
    "reference_sentence_deictic = [\"that object\", \"this object\", \"this place\", \"that place\"]\n",
    "reference_sentence_stop = [\"stop\"] #[\"stop\"]\n",
    "reference_sentence_run = [\"running\"] #[\"running\"] #go,go\n",
    "reference_sentence_apologize = [\"I apologize\"] #[\"I apologize\"]\n",
    "reference_sentence_beg = [\"I beg you\"] #[\"I beg you\"]\n",
    "reference_sentence_time = [\"It's late\"] #[\"it's late\"]\n",
    "reference_sentence_clap = [\"I praise you\"] #[\"I praise you\"]\n",
    "reference_sentence_exulting = [\"I am exulting\"] #[\"I am exulting\"]\n",
    "'''\n",
    "\n",
    "  ###ALICE\n",
    "'''\n",
    "reference_sentence_greet = [\"See you soon\",\"How are you? I haven’t seen you for a long time\",\"Hi! Are you here too?\",\"Have a good evening!\" ]#['greeting']  #[\"hello\", \"goodbye\"]\n",
    "reference_sentence_idk = [\"I don’t know where is it\",\"I have no idea\",\"I don’t know if I like it\",\"I don’t remember what time is it\"]#[\"I don't know\"]\n",
    "reference_sentence_no = [\"No, I’m not doing all myself\",\"We can’t keep spending money on useless items\",\"I won’t walk all day\",\"No, I won’t go to the club\"]\n",
    "reference_sentence_yes = [\"All right\",\"Yes\",\"Okay\",\"Yes I’m really happy\"] #[\"yes\"] #right\n",
    "reference_sentence_deictic = [\"We gotta get to the top of that mountain over there\",\"Would you hand me that pen?\",\n",
    "                              \"Take the flour in that cupboard\",\"Look at the sea!\"] #[\"that object\", \"this object\", \"this place\", \"that place\"]\n",
    "reference_sentence_stop = [\"Stop, I don’t understand anything\",\"Let’s just calm down\",\"Let’s stop and take a breath\",\"Stop, I’m exhausted.\"]#[\"stop\"]\n",
    "reference_sentence_run = [\"The train is leaving\", \"I have to run\",\"To lose weight I have to go for a run\",\"I ran all over the shops\"]#[\"running\"] #go,go\n",
    "reference_sentence_apologize = [\"I’m so sorry\",\"I didn’t think you were hurt\",\"I’m sorry I made you wait\", \"I'm sorry, I can't now.\"]#[\"I apologize\"]\n",
    "reference_sentence_beg = [\"Will you please help me with the computer exam?\",\"Would you please take me to the party?\",\n",
    "                          \"Would you please buy me an icecream?\", \"Would you like to go to Paris?\"] #[\"I beg you\"]\n",
    "reference_sentence_time = [\"can you hurry up and get dressed?\",\"The bus leaves in ten minutes\",\"Would you hurry up?\",\"I stay there one hour\"] #[\"it's late\"\"]\n",
    "reference_sentence_clap = [\"You were good to take such a high grade\",\"The competition was a success, congratulations really!\",\n",
    "                            \"The cake is delicious\",\"I’m so glad you could buy a house\"] #[\"I praise you\"]\n",
    "reference_sentence_exulting = [\"What a great goal we made!\",\"I finally graduated!\",\"I win first prize!\",\"I finally got home\"]#[\"I am exulting\"]\n",
    "'''\n",
    "### ALICE RANDOM\n",
    "\n",
    "reference_sentence_greet = [\"Hi! Are you here too?\"]\n",
    "reference_sentence_idk = [\"I don’t know if I like it\"]\n",
    "reference_sentence_no = [\"No, I’m not doing all myself\"]\n",
    "reference_sentence_yes = [\"Okay\"]\n",
    "reference_sentence_deictic = [\"Take the flour in that cupboard\"]\n",
    "reference_sentence_stop = [\"Stop, I’m exhausted.\"]#[\"stop\"]\n",
    "reference_sentence_run = [\"I ran all over the shops\"]#[\"running\"] #go,go\n",
    "reference_sentence_apologize = [\"I'm sorry, I can't now.\"]#[\"I apologize\"]\n",
    "reference_sentence_beg = [\"Will you please help me with the computer exam?\"] #[\"I beg you\"]\n",
    "reference_sentence_time = [\"Would you hurry up?\"] #[\"it's late\"\"]\n",
    "reference_sentence_clap = [\"congratulations really!\"] #[\"I praise you\"]\n",
    "reference_sentence_exulting = [\"I finally graduated!\"] #[\"I am exulting\"]\n",
    "\n",
    "  ###CARMINE NUOVE\n",
    "'''\n",
    "reference_sentence_greet = [\"Goodbye to everyone and thank you for listening.\", \"Good morning! So good to see you again!\",\n",
    "                            \"Hey! Hello! You've finally arrived.\", \"Hi everyone, I'm Carmine.\"]#['greeting']  #[\"hello\", \"goodbye\"]\n",
    "reference_sentence_idk = [\"I really have no idea where John is.\",\"I don't know what I'm going to do tomorrow.\",\n",
    "                          \"I'm speechless, I don't know what to say.\", \"Don’t ask me, I'm not an expert.\"] #[\"I don't know\"]\n",
    "reference_sentence_no = [\"What you have done is not good.\",\"I don't want anything more.\",\"I can't believe what I saw.\",\"I don't have what you ask for.\"]\n",
    "reference_sentence_yes = [\"Yes thank you, I will gladly accept!\",\"It's exactly as you say.\",\n",
    "                          \"If you insist I will come to the party.\",\"You have answered my question correctly.\"] #[\"yes\"] #right\n",
    "reference_sentence_deictic = [\"The building is exactly behind you.\",\"Look at what's going on there!\",\n",
    "                              \"Could you please get that book for me?\",\"Go there immediately and don't move.\"] #[\"that object\", \"this object\", \"this place\", \"that place\"]\n",
    "reference_sentence_stop = [\"Wait, you're talking too fast.\",\"Stop where you are, the floor is slippery.\",\n",
    "                           \"Quiet, listen, there's a noise upstairs.\",\"I don't want to know about it.\"]#[\"stop\"]\n",
    "reference_sentence_run = [\"Do you want to go for a run?\", \"I just got back from a good run.\",\"I finished second in the cross-country race.\",\n",
    "                          \"I hope to win the 100-meter flat race.\"] #[\"running\"] #go,go\n",
    "reference_sentence_apologize = [\"It's all my fault.\",\"I'm sorry, I guess I misunderstood.\",\"I'm terribly sorry for the delay.\",\n",
    "                                \"Ehy, it's okay, don't fret.\"]#[\"I apologize\"]\n",
    "reference_sentence_beg = [\"Please, I absolutely need you now.\",\"Let's hope for the best.\",\n",
    "                          \"Please stop it!\", \"God bless you!\"] #[\"I beg you\"]\n",
    "reference_sentence_time = [\"You’re late, have you seen what time it is?\",\"Hurry up, they are expecting us.\",\n",
    "                           \"You only have 2 minutes left for your presentation.\",\"It's time to get back to work.\"] #[\"it's late\"\"]\n",
    "reference_sentence_clap = [\"Congratulations, really an interesting speech.\",\"Best wishes to the bride and groom!\",\n",
    "                            \"You did a really good job.\",\"Bravo! Encore!\"] #[\"I praise you\"]\n",
    "reference_sentence_exulting = [\"Yay! The concert begins!\",\"I finally made it!\",\"My team has won the game!\",\"I didn't expect you to come!\"] #[\"I am exulting\"]\n",
    "'''\n",
    "### CARMINE NUOVE RANDOM\n",
    "'''\n",
    "reference_sentence_greet = [\"Hi everyone, I'm Carmine.\"]#['greeting']  #[\"hello\", \"goodbye\"]\n",
    "reference_sentence_idk = [\"I'm speechless, I don't know what to say.\"] #[\"I don't know\"]\n",
    "reference_sentence_no = [\"I can't believe what I saw.\"]\n",
    "reference_sentence_yes = [\"Yes thank you, I will gladly accept!\"] #[\"yes\"] #right\n",
    "reference_sentence_deictic = [\"Go there immediately and don't move.\"] #[\"that object\", \"this object\", \"this place\", \"that place\"]\n",
    "reference_sentence_stop = [\"I don't want to know about it.\"]#[\"stop\"]\n",
    "reference_sentence_run = [\"I finished second in the cross-country race.\"] #[\"running\"] #go,go\n",
    "reference_sentence_apologize = [\"It's all my fault.\"]#[\"I apologize\"]\n",
    "reference_sentence_beg = [\"God bless you!\"] #[\"I beg you\"]\n",
    "reference_sentence_time = [\"It's time to get back to work.\"] #[\"it's late\"\"]\n",
    "reference_sentence_clap = [\"Best wishes to the bride and groom!\"] #[\"I praise you\"]\n",
    "reference_sentence_exulting = [\"I finally made it!\"] #[\"I am exulting\"]\n",
    "'''\n",
    " ###LUCREZIA\n",
    "'''\n",
    "reference_sentence_greet = [\"Hello!\",\"See you soon!\",\"Goodbye!\",\"Come back soon!\"] #['greeting]  #[\"hello\", \"goodbye\"]\n",
    "reference_sentence_idk = [\"I have no idea...\",\"I don't know what to say...\",\"There's nothing I can do...\",\"Do as you please!\"]#[\"I don't know\"]\n",
    "reference_sentence_no = [\"It wasn't me!\",\"I don't know how it could've happened\",\"I don't like it.\",\"I don't feel like it!\"]\n",
    "reference_sentence_yes = [\"Okay, let's go!\",\"I really like it.\",\"You're right\",\"Got it, clear.\"] #[\"yes\"] #right\n",
    "reference_sentence_deictic = [\"Who is that?\",\"Go straight until there.\", \"Can you take the object on the table?\",\"Up there is the Big Dipper constellation.\"] #[\"that object\", \"this object\", \"this place\", \"that place\"]\n",
    "reference_sentence_stop = [\"Everyone stop!\",\"Watch out, the floor is wet!\",\"Don't move.\",\"I'm stopping you right now: I've already done it.\"]#[\"stop\"]\n",
    "reference_sentence_run = [\"Shall we go for a run?\",\"Are you ready to run?\",\"I just finished a nice treadmill run.\",\"You should get some exercise!\"]#[\"running\"] #go,go\n",
    "reference_sentence_apologize = [\"I won't do it anymore!\",\"I didn't mean to offend you.\",\"Don't take it personally!\", \"It's not my fault.\"]#[\"I apologize\"]\n",
    "reference_sentence_beg = [\"Will you come with me?\",\"Come on, please!\",\"Do it for me!\", \"Let's hope everything goes well!\"] #[\"I beg you\"]\n",
    "reference_sentence_time = [\"I've been waiting for you forever!\",\"So, are you ready? We need to go!\",\"Do you see what time it is?\",\"How much longer until we're done?\"] #[\"it's late\"\"]\n",
    "reference_sentence_clap = [\"Well done!\",\"I really enjoyed it!\",\"I couldn't have done better myself!\",\"I would've never expected that from you!\"] #[\"I praise you\"]\n",
    "reference_sentence_exulting = [\"We won!\",\"Let's go celebrate!\",\"I did it!\",\"Woohoo, it works!\"] #[\"I am exulting\"]\n",
    "'''\n",
    "### LUCREZIA RANDOM\n",
    "'''\n",
    "reference_sentence_greet = [\"Hello!\"]\n",
    "reference_sentence_idk = [\"Do as you please!\"]#[\"I don't know\"]\n",
    "reference_sentence_no = [\"I don't know how it could've happened\"]\n",
    "reference_sentence_yes = [\"Okay, let's go!\"] #[\"yes\"] #right\n",
    "reference_sentence_deictic = [\"Up there is the Big Dipper constellation.\"] #[\"that object\", \"this object\", \"this place\", \"that place\"]\n",
    "reference_sentence_stop = [\"I'm stopping you right now: I've already done it.\"]#[\"stop\"]\n",
    "reference_sentence_run = [\"You should get some exercise!\"]#[\"running\"] #go,go\n",
    "reference_sentence_apologize = [\"I didn't mean to offend you.\"]#[\"I apologize\"]\n",
    "reference_sentence_beg = [\"Will you come with me?\"] #[\"I beg you\"]\n",
    "reference_sentence_time = [\"I've been waiting for you forever!\"] #[\"it's late\"\"]\n",
    "reference_sentence_clap = [\"I would've never expected that from you!\"] #[\"I praise you\"]\n",
    "reference_sentence_exulting = [\"We won!\"] #[\"I am exulting\"]\n",
    "'''\n",
    "### ANTONIO\n",
    "'''\n",
    "reference_sentence_greet = ['hello','how are you','goodbye','hey there']\n",
    "reference_sentence_idk = [\"I don't know\",'maybe','who knows',\"I can't help you\"]\n",
    "reference_sentence_no = ['no','never','I did not','not at all']  #not at all. in LARGE, not at all  in BASE\n",
    "reference_sentence_yes = ['yes','sure','of course','with pleasure']\n",
    "reference_sentence_deictic = ['look at','there','that place','that object']\n",
    "reference_sentence_stop = ['stop','do not move','give me a break','stay away']\n",
    "reference_sentence_run = ['shall we go',\"let's go\",'have a walk','have a run']\n",
    "reference_sentence_apologize = ['sorry','I beg your pardon','apologizes',\"don't be mad\"]\n",
    "reference_sentence_beg = ['please','do it','do me a favour','help me']\n",
    "reference_sentence_time = [\"it's late\",\"it's time to\",'time is running','we have to go']\n",
    "reference_sentence_clap = ['congratulations','well done','bravo!','good job!'] #good job. in LARGE, good job! in BASE\n",
    "reference_sentence_exulting = ['I did it!','hooray!','I won!','I am the best']\n",
    "'''\n",
    "'''\n",
    "### ANTONIO RANDOM\n",
    "reference_sentence_greet = ['hello']\n",
    "reference_sentence_idk = [\"I don't know\"]\n",
    "reference_sentence_no = ['no']\n",
    "reference_sentence_yes = ['yes']\n",
    "reference_sentence_deictic = ['that object']\n",
    "reference_sentence_stop = ['do not move']\n",
    "reference_sentence_run = [\"let's go\"]\n",
    "reference_sentence_apologize = ['apologizes']\n",
    "reference_sentence_beg = ['please']\n",
    "reference_sentence_time = ['time is running']\n",
    "reference_sentence_clap = ['bravo!']\n",
    "reference_sentence_exulting = ['I did it!']\n",
    "'''\n",
    "'''\n",
    "Sentences can be related to the topics represented by the words: greeting someone, praise someone,\n",
    "                                        telling you don't know something, showing an object or a place,\n",
    "                                        showing agreement or disagreement, begging someone, telling or asking if it's late, telling to stop or slow down,  telling or asking for apologize,\n",
    "                                        telling a run story or asking someone to run with you, exulting for something.\"\n",
    "                                        '''\n",
    "\n",
    "all_refs = {\"r_greet\":reference_sentence_greet, \"r_clap\":reference_sentence_clap,  \"r_idk\":reference_sentence_idk, \"r_no\":reference_sentence_no,\n",
    "           \"r_yes\":reference_sentence_yes,\"r_stop\":reference_sentence_stop, \"r_beg\":reference_sentence_beg,\"r_time\":reference_sentence_time,\n",
    "           \"r_apologize\":reference_sentence_apologize, \"r_deitic\":reference_sentence_deictic,\"r_exulting\":reference_sentence_exulting,\"r_run\":reference_sentence_run}\n",
    "\n",
    "topic_gesture_description = {\"Greeting\":(\"r_greet\", \"the gesture that you use when you greet people\"), \\\n",
    "                             \"Run\": (\"r_run\",\"moving arms aback and forth while maintaining an angle of 90° with the elbow to simulate a running movement\"),\n",
    "                            \"I apologize\":(\"r_apologize\", \"the gesture that you make when you ask someone to forgive you, for instance by tilting the upper body or by keeping arms up, palms facing up and elbows bent so that they form a 90° angle\"),\\\n",
    "                             \"I am exulting\":(\"r_exulting\", \"the gesture that you make when you are exulting (for example by putting both your arms up and by putting your keeping your hands in the shape of a fist)\"), \\\n",
    "                             \"I praise you\": (\"r_clap\",\"clapping with hands to congratulate with someone\"), \\\n",
    "                             \"No\":(\"r_no\", \"moving the head or a finger left and right when you want to say no\"), \\\n",
    "                             \"Yes\":(\"r_yes\",\"moving the head up and down when you are agreeing for something or you want to say yes\"), \\\n",
    "                             \"Indicate a place or object\":(\"r_deitic\",\"the gesture that you use for indicating a place or object (with head, finger, etc.)\"),\n",
    "                             \"Stop\":(\"r_stop\",\"gesture that you make with arms to tell someone that he as to slow down or stop to do something (for example with arms ahead)\"), \\\n",
    "                             \"I don't know\":(\"r_idk\", \"moving the shoulders up and down or move both the arms in front of you with palms facing up, used when you want to to show that you don't know something\"), \\\n",
    "                             \"I beg you\":(\"r_beg\",\"the gesture that you make when you are begging someone, for instance by keeping arms in a pray position\"), \\\n",
    "                             \"C'mon, it's late\":(\"r_time\", \"indicating the wrist with an index to say that it's late for something\")}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### SENTENCE SIMILARITY COMPUTATION\n",
    "\n",
    "class Subs_classification:\n",
    "\n",
    "    # filename contains the subtitles filepath\n",
    "    # refs contains the dictionary with all the reference sentences\n",
    "    # model variable contains the model that will be used for the semantic similarity\n",
    "    # winsize represents the number of words inside the sentence that will be used as comparison\n",
    "        # with the reference sentences. Not necessary in the \"fullstop_classification\" mode\n",
    "    # stride indicates the amount of words skipped to traslate the window. Not necessary in \"fullstop_regression\" mode\n",
    "    # thresold represents the value over which we consider the prediction of the model \"valid\", i.e. when\n",
    "        # reference sentence and sentence are similar enough\n",
    "    # mode is a string that represents the modality that we want to use as comparison:\n",
    "        # \"fullstop_regression\" compares a reference sentence with a sentence between two full stop\n",
    "        # \"window_regression\" compares a reference sentence with a sentence made up by the words inside the window\n",
    "        # \"mixed_rgression\" compares a reference sentence with a sentence made up by the words inside the window but\n",
    "            # it also find relations between windows inside a sentence between two full stops. The last window does not take\n",
    "            # the words after the full stop\n",
    "\n",
    "    def __init__(self, filename = '', refs = {}, path = '', result_name = '',\n",
    "                 model = CrossEncoder('cross-encoder/stsb-roberta-base'),\n",
    "                 winsize = 5, stride = 2, threshold = 0.3, mode = \"fullstop_regression\"):\n",
    "\n",
    "        self.filepath = filename\n",
    "        self.subs = ''           #self.read_file(filename)\n",
    "        self.refs = refs\n",
    "        self.winsize = winsize\n",
    "        self.stride = stride\n",
    "        self.threshold = threshold\n",
    "        self.mode = mode\n",
    "        self.result_name = result_name\n",
    "        self.path = path\n",
    "\n",
    "    # used to read the \"filename\" file. It reads a row at time and then puts all the rows together\n",
    "    def read_file(self, filename):\n",
    "\n",
    "        with open(subtitles_path,\"r\",encoding='utf-8') as f:\n",
    "            subs = [line.rstrip() for line in f if line.rstrip()!='']  #removes \\n\n",
    "            subs_final = ''\n",
    "            #subs_final = [subs_final + s + ' ' for s in subs]\n",
    "            for el in subs:\n",
    "                subs_final = subs_final + el + ' '  #puts all the subs together\n",
    "            self.subs = subs_final\n",
    "            return self.subs\n",
    "\n",
    "\n",
    "    def sub_classifier(self, mode='', method='', sentences = '', type = 'conversation'):\n",
    "\n",
    "        if mode!='':\n",
    "            self.mode = mode\n",
    "        if method!='':\n",
    "            self.method = method\n",
    "\n",
    "        if self.mode == \"fullstop_regression\":\n",
    "            return self._fullstop_regression(self.method)\n",
    "        elif self.mode == \"window_regression\":\n",
    "            return self._window_regression(self.method)\n",
    "        elif self.mode == \"mixed_regression\":\n",
    "            return self._mixed_regression(self.method, sentences, type)\n",
    "        else:\n",
    "            print(\"Choose a right mode and a right method\")\n",
    "\n",
    "\n",
    "    def _append_results(self, thresholds_dict, value, ref, sentence, start_index):\n",
    "\n",
    "        thresholds_dict[\"value\"].append(value)\n",
    "        thresholds_dict[\"ref\"].append(ref)\n",
    "        thresholds_dict[\"sentence\"].append(sentence)\n",
    "        thresholds_dict['start_word_indexes'].append(start_index)\n",
    "\n",
    "        return thresholds_dict\n",
    "\n",
    "\n",
    "    def _best_threshold(self,local_th, all_th, sentence, start_index):\n",
    "\n",
    "        if len(local_th[\"value\"])!=0: #then we have at least one reference match\n",
    "            max_threshold = max(local_th[\"value\"])\n",
    "            index = local_th[\"value\"].index(max_threshold)\n",
    "            all_th = self._append_results(all_th, max_threshold,\n",
    "                                            local_th[\"ref\"][index], sentence, local_th['start_word_indexes'][index])\n",
    "        #else:\n",
    "            #all_th = self._append_results(all_th, None,  None, sentence)\n",
    "        local_th = {\"value\":[], \"ref\":[], \"sentence\":[], 'start_word_indexes':[]} #I don't need the local thresholds anymore\n",
    "\n",
    "\n",
    "        return local_th, all_th\n",
    "\n",
    "    def _window_regression(self, method = \"best_threshold\"):\n",
    "\n",
    "        subs = self.read_file(self.filepath)    #all the subtitles\n",
    "        splits = subs.split()[0:200]\n",
    "        #best_th = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "        all_best_thresholds = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "        local_thresholds = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "        all_local_th = {}\n",
    "\n",
    "        for i in range(int(len(splits)/self.stride)):\n",
    "            sentence = \" \".join(splits[i*self.stride : i*self.stride + self.winsize]) #sentence that will be compared with all the ref sentences\n",
    "            for key in self.refs.keys():    #each key represents a topic related to a specific gesture, each element in the key is a sentence that can generate that gesture\n",
    "                for ref_sentence in self.refs[key]:   #for each reference, do a comparison with a sentence inside subtitles. If they are similar (>threshold) then append the result to local_thresholds\n",
    "                    res = model.predict([sentence,ref_sentence])\n",
    "                    if res > self.threshold:\n",
    "                        local_thresholds = self._append_results(local_thresholds, res, ref_sentence, sentence)\n",
    "\n",
    "                if method == \"best_threshold_for_each_topic\":     #for each topic, find the reference that is the most similar to the sentence (if there is anyone) and put results in all_best_thresholds\n",
    "                    local_thresholds, all_best_thresholds = self._best_threshold(local_thresholds, all_best_thresholds, sentence)\n",
    "\n",
    "            if method == \"best_threshold\":  #for each window, find the best reference of the best topic that is the most similar to the sentence (if there is anyone\n",
    "                local_thresholds, all_best_thresholds = self._best_threshold(local_thresholds, all_best_thresholds, sentence)\n",
    "\n",
    "        if method == \"best_threshold\" or method == \"best_threshold_for_each_topic\":\n",
    "            return all_best_thresholds\n",
    "        elif method == \"all_thresholds\":\n",
    "            return local_thresholds\n",
    "\n",
    "\n",
    "    def _fullstop_regression(self, method = \"best_threshold\"):\n",
    "\n",
    "        subs = self.read_file(self.filepath)\n",
    "        splits = subs.split(\".\")[0:200]\n",
    "        all_best_thresholds = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "        local_thresholds = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "\n",
    "        for sentence in splits:\n",
    "            for key in self.refs.keys():\n",
    "                for ref_sentence in self.refs[key]:\n",
    "                    res = model.predict([sentence,ref_sentence])\n",
    "                    if res > self.threshold:\n",
    "                        local_thresholds = self._append_results(local_thresholds, res, ref_sentence, sentence)\n",
    "\n",
    "                if method == \"best_threshold_for_each_topic\":     #for each topic, find the reference that is the most similar to the sentence (if there is anyone) and put results in all_best_thresholds\n",
    "                    local_thresholds, all_best_thresholds = self._best_threshold(local_thresholds, all_best_thresholds, sentence)\n",
    "\n",
    "            if method == \"best_threshold\":  #for each window, find the best reference of the best topic that is the most similar to the sentence (if there is anyone\n",
    "                local_thresholds, all_best_thresholds = self._best_threshold(local_thresholds, all_best_thresholds, sentence)\n",
    "\n",
    "        if method == \"best_threshold\" or method == \"best_threshold_for_each_topic\":\n",
    "            return all_best_thresholds\n",
    "        elif method == \"all_thresholds\":\n",
    "            return local_thresholds\n",
    "\n",
    "\n",
    "    def _mixed_regression(self, method = \"best_threshold\", sentences='', type = 'sentences'):\n",
    "        #TODO save the starting index of the matched words to avoid later correction - DONE\n",
    "        #TODO (optional) add full stop separation in the conversation\n",
    "        #subs = self.read_file(self.filepath)\n",
    "        subs = sentences\n",
    "        counter = 1\n",
    "        #all_local_all_sentences=[]  #it contains all the local results for all possible windows for all the sentences\n",
    "        all_processed_sentences = []  #it contains all the local results for all possible windows for all the sentences\n",
    "        #print(subs)\n",
    "        if type == 'conversation':      # In this case sentences are of type (speaker,sentence,turn), otherwise we have only a list of sentences\n",
    "            conv_subs=[]\n",
    "            for key in list(subs.keys()):\n",
    "                speaker = (key,)\n",
    "                tuples = [speaker + element for element in subs[key]]\n",
    "                conv_subs.append(tuples)\n",
    "                #subs[key] = [ for (sentence,turn) in subs[key]]\n",
    "            #subs = conv_subs[0][0:10] + conv_subs[1][0:10] #take 10 sentences for 2 speakers\n",
    "            subs = conv_subs[1][:100] + conv_subs[0][:100]\n",
    "            #print(subs)\n",
    "\n",
    "\n",
    "        #print(subs)\n",
    "        #subs = subs[0:1] + subs[2:3] #+ subs[8:13]\n",
    "        sentences_processed = 0\n",
    "        all_topics_labelling_path = os.path.join(path,result_name)#\"all_processed_sentences_best.pkl\"\n",
    "        #filename_best = \"all_processed_sentences_best.pkl\"\n",
    "        #filename_local = \"all_processed_sentences_local.pkl\"\n",
    "        window_max_len = 10\n",
    "        #path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "\n",
    "        print(\"loading file with all matches...\")\n",
    "        if(os.path.exists(all_topics_labelling_path)):    #load data already processed\n",
    "            all_processed_sentences = pickle.load(open(all_topics_labelling_path,'rb'))\n",
    "            print(\"File with all matches loaded!\")\n",
    "        else:\n",
    "            print(\"No files were loaded\")\n",
    "        #   pickle.dump(all_processed_sentences, open(all_topics_labelling_path,'wb'))\n",
    "\n",
    "        '''\n",
    "        if(os.path.exists(os.path.join(path,filename_local))):    #load data already processed\n",
    "            all_local_all_sentences = pickle.load(open(os.path.join(path,filename_local),'rb'))\n",
    "        else:\n",
    "            pickle.dump(all_local_all_sentences, open(os.path.join(path,filename_local),'wb'))\n",
    "        print(\"both files loaded\")\n",
    "        '''\n",
    "        #print(all_best_all_sentences)\n",
    "\n",
    "        for sentence in subs:\n",
    "\n",
    "            #all_local_th = {}\n",
    "            all_topic_res = {}                                   #contains all the possible matches for all the possible winsize\n",
    "            best_th_topic = {\"value\":[], \"ref\":[], \"sentence\":[],'start_word_indexes':[]}\n",
    "            local_th_topic = {\"value\":[], \"ref\":[], \"sentence\":[],'start_word_indexes':[]}    #contains all the possible matches for a given winsize\n",
    "            #all_locals = {\"value\":[], \"ref\":[], \"sentence\":[],'start_word_indexes':[]}  #contains all the possible matches for a given winsize, but differently from local_th, it  does not become empty if method is \"best_threshold\" or \"best_threshold_for_each_topic\"\n",
    "            skip_flag = False\n",
    "            if type == 'conversation':\n",
    "                #print(sentence,len(sentence))\n",
    "                speaker = sentence[0]\n",
    "                turn = sentence[2]\n",
    "                sentence = sentence[1]\n",
    "                for i in range(len(all_processed_sentences)): #check if we already processed the sentence\n",
    "                    if((sentence,speaker,turn) == all_processed_sentences[i][\"whole sentence,speaker,turn\"]): #we already have this data\n",
    "                        skip_flag = True\n",
    "                        print(f\"skipped: {(sentence,speaker,turn)}\", \"counter\",counter)\n",
    "                        sentences_processed += 1\n",
    "                        counter += 1\n",
    "                if skip_flag == True: #if we already processed it, we go to the next sentence, otherwise we start the computations\n",
    "                        continue\n",
    "                else:\n",
    "                    all_topic_res[\"whole sentence,speaker,turn\"] = (sentence,speaker,turn)\n",
    "                    #all_best_th[\"whole sentence,speaker,turn\"] = (sentence,speaker,turn)\n",
    "            elif type == 'sentences':\n",
    "                for i in range(len(all_processed_sentences)):\n",
    "                    if(sentence == all_processed_sentences[i][\"whole sentence\"]):\n",
    "                        skip_flag = True\n",
    "                        print(f\"skipped: {sentence}\", \"counter\",counter)\n",
    "                        print(all_processed_sentences[i][\"processing_time\"])\n",
    "                        sentences_processed += 1\n",
    "                        counter += 1\n",
    "                        continue\n",
    "                if skip_flag == True:\n",
    "                    continue\n",
    "                else:\n",
    "                    all_topic_res[\"whole sentence\"] = sentence\n",
    "                    #all_best_th[\"whole sentence\"] = sentence\n",
    "            else:\n",
    "                print(\"Undefined element\")\n",
    "                return\n",
    "            start_time = time.perf_counter()\n",
    "            window_size = self.winsize\n",
    "            sentence_splits = sentence.split()\n",
    "            while window_size <= len(sentence_splits) and window_size <= window_max_len:\n",
    "                #print(window_size)\n",
    "                #print(self.winsize)\n",
    "                #print(f\"HERE:{sentence_splits}\")\n",
    "                #print(len(sentence_splits))\n",
    "                n_windows = int((len(sentence_splits) - window_size + 1) / self.stride)\n",
    "                #print(n_windows)\n",
    "                for i in range(n_windows):  #use the window inside the sentence\n",
    "                    sub_sentence = \" \".join(sentence_splits[i*self.stride : i*self.stride + window_size]) #sentence that will be compared with all the ref sentences\n",
    "                    start_index = i * self.stride\n",
    "                    for key in self.refs.keys():    #for all types of references\n",
    "                        for ref_sentence in self.refs[key]: #for all references\n",
    "                            #print(self.winsize, sub_sentence)\n",
    "                            res = model.predict([sub_sentence,ref_sentence])\n",
    "                            if res > self.threshold:    #then we have a match\n",
    "                                local_th_topic = self._append_results(local_th_topic, res, ref_sentence, sub_sentence, start_index) #put the result in local_th\n",
    "                                #all_locals = self._append_results(all_locals, res, ref_sentence, sub_sentence) #same operation but this list do not become empty if method is \"best_threshold_for_each_topic\" or \"best_threshold\". Used for saving purposes\n",
    "\n",
    "                        if method == \"best_threshold_for_each_topic\":     #for each topic, find the reference that is the most similar to the sentence (if there is anyone) and best_th_topic\n",
    "                            local_th_topic, best_th_topic = self._best_threshold(local_th_topic, best_th_topic, sub_sentence, start_index)\n",
    "\n",
    "\n",
    "                    if method == \"best_threshold\":  #for each window, find the best reference of the best topic that is the most similar to the sentence (if    there is anyone)\n",
    "                        local_th_topic, best_th_topic = self._best_threshold(local_th_topic, best_th_topic, sub_sentence, start_index)\n",
    "                        #print(best_th)\n",
    "\n",
    "                #all_local_th[f\"window {window_size}\"] = all_locals  #save results for each window size\n",
    "                all_topic_res[f\"window {window_size}\"] = best_th_topic\n",
    "                #best_th_topic['start_word_indexes'] =\n",
    "                local_th_topic = {\"value\":[], \"ref\":[], \"sentence\":[],'start_word_indexes':[]} #I don't need the local thresholds anymore\n",
    "                best_th_topic = {\"value\":[], \"ref\":[], \"sentence\":[],'start_word_indexes':[]} #I don't need the best thresholds anymore\n",
    "                window_size = window_size + 1\n",
    "            #print(all_best_th)\n",
    "            #all_local_all_sentences.append(all_local_th)\n",
    "            #all_best_all_sentences.append(all_best_th)\n",
    "            end_time = time.perf_counter()\n",
    "            processing_time = round(end_time-start_time, 4)\n",
    "            all_topic_res[\"processing_time\"] = processing_time\n",
    "            print(\"ELAPSED TIME:\",processing_time, \"counter\",counter)\n",
    "            counter += 1\n",
    "            all_processed_sentences.append(all_topic_res)\n",
    "            sentences_processed = sentences_processed + 1\n",
    "            print(\"writing to file...\")\n",
    "            pickle.dump(all_processed_sentences, open(all_topics_labelling_path,'wb'))\n",
    "            #pickle.dump(all_local_all_sentences, open(os.path.join(path,filename_local),'wb'))\n",
    "            if type == 'conversation':\n",
    "                print(f\"file written {sentence, speaker, turn}, sentence processed: {sentences_processed}\")\n",
    "            else:\n",
    "                print(f\"file written {sentence}\")\n",
    "\n",
    "            #print(f\"processed a sentence:{all_best_th}\")\n",
    "            #print(all_local_th)\n",
    "\n",
    "\n",
    "        if method == \"best_threshold\" or method == \"best_threshold_for_each_topic\":\n",
    "            return all_processed_sentences\n",
    "        #elif method == \"all_thresholds\":\n",
    "        #    return all_local_all_sentences\n",
    "\n",
    "\n",
    "th = 0\n",
    "#filename = \"sentences.pkl\"\n",
    "filename = \"sentences_gpt_300_old.pkl\"\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "result_name = \"all_similarities_Alice_random_large_old.pkl\"\n",
    "all_sentences = pickle.load(open(os.path.join(path,filename),'rb'))\n",
    "#results = pickle.load(open(os.path.join(path,result_name),'rb'))\n",
    "#pickle.dump(results[:133], open(os.path.join(path,result_name),'wb'))\n",
    "classifier = Subs_classification(filename = filename, refs = all_refs, path = path, result_name = result_name,\n",
    "                                  winsize = 1, stride = 1,threshold = th, mode = \"mixed_regression\")  #compute all the possible results\n",
    "result = classifier.sub_classifier(mode='', method=\"best_threshold_for_each_topic\", sentences = all_sentences , type = 'sentences')\n",
    "#print([r['processing_time'] for r in results[-4:]])\n",
    "#result = pickle.load(open(os.path.join(path,result_name),'rb'))\n",
    "#print(result[0])\n",
    "#print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### COMPUTATION OF BEST MATCHING WINDOW FOR EACH GESTURE\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "#this time the computation of the average window is slightly different: instead of choosing a threshold as true and then checking all the possible windows in the text\n",
    "#that respect that theshold to compute the average, this time by starting from a theshold from which we consider a result as valid:\n",
    "#-we set a window_size of 1 and we check all the results with window 1 that are greater than the threshold\n",
    "#-we then compute the average and the standard deviation, then avg-std\n",
    "#-we repeat this operation for all the possible window sizes (currently from 1 to 10)\n",
    "#-we choose, as avg window for a topic, the window which has the highest avg-std\n",
    "def best_windows(results, all_refs, new_th = 0, dim_max = 10, type = 'conversation'):\n",
    "\n",
    "    best_windows = {} #it will contain the best window for each topic, the standard deviation, the number of matches, the best and std of the values of the matches\n",
    "    all_topics_data = {} #it contains all the window sizes and all the values for each topic so that we can compute sum and std at the end and put the result in the previous dict\n",
    "    for topic in all_refs: #init the best windows\n",
    "        best_windows[topic] = {'best_window':0, 'n_matches':0, 'avg_value':0, 'std_value':0}\n",
    "        all_topics_data[topic]={}\n",
    "        for i in range(1,dim_max+1):\n",
    "            all_topics_data[topic][f'window {i}']=[]\n",
    "    for i,res in enumerate(results):\n",
    "        #if i==0:\n",
    "        #    print(res,res.keys())\n",
    "        if type == 'conversation':\n",
    "            sentence = res['whole sentence,speaker,turn'][0]\n",
    "        elif type == 'sentences':\n",
    "            sentence = res['whole sentence']\n",
    "        else:\n",
    "            print(\"Undefined element\")\n",
    "            return\n",
    "\n",
    "        #print(res.keys())\n",
    "        #speaker = res['whole sentence,speaker,turn'][1]\n",
    "        #turn = res['whole sentence,speaker,turn'][2]\n",
    "        #print(\"HERE\",int(list(res.keys())[-1][-1]))\n",
    "        window_dim = int(list(res.keys())[-2].split(' ')[-1]) #the last key of res contains data of the largest window that was used for the sentence.\n",
    "        #print(\"WINDOW DIM:\",window_dim)\n",
    "        #window_dim = len(sentence_splits)  TODO: CHECK IF IT IS CORRECT\n",
    "        #print(window_dim)\n",
    "        for j in range(1,window_dim+1):\n",
    "            #print(res[f\"window {j}\"])\n",
    "            values = res[f\"window {j}\"]['value']\n",
    "            th_indexes = [i for i in range(len(values)) if values[i] > new_th] #returns the indexes of the values that are greater than new_th\n",
    "            tmp_values = list(map(values.__getitem__,th_indexes)) #take all the values greater than new_th\n",
    "            tmp_ref = list(map(res[f\"window {j}\"]['ref'].__getitem__,th_indexes)) #take all the references that returns a value greater than new_th\n",
    "            #tmp_sentence = list(map(res[f\"window {j}\"]['sentence'].__getitem__,th_indexes))\n",
    "            #print(th_indexes)\n",
    "            for k,value in enumerate(tmp_values): #for each value that is greater than the threshold\n",
    "                ref = tmp_ref[k]\n",
    "                #print(ref)\n",
    "                topic = [key for key in all_refs.keys() for t_ref in all_refs[key] if ref == t_ref][0] #extract the topic associated with reference\n",
    "                best_windows[topic]['n_matches'] = best_windows[topic]['n_matches'] + 1 #increment the number of matches\n",
    "                all_topics_data[topic][f'window {j}'].append(value)\n",
    "    #print(all_topics_data['r_exulting'])\n",
    "\n",
    "    for topic in all_refs:  #now we can compute the best window for each topic by taking the highest avg-std value\n",
    "        tmp_best = 0\n",
    "        tmp_win = 0\n",
    "        tmp_avg = 0\n",
    "        tmp_std = 0\n",
    "        tmp_all_vals = []\n",
    "        #print(f\"The results of the topic {topic} are:\",all_topics_data[topic])\n",
    "        for i in range(1,dim_max+1):\n",
    "            if len(all_topics_data[topic][f'window {i}'])<=10: #we don't take into account the result if it has less than 10 matches\n",
    "                continue\n",
    "            tmp_res = np.mean(np.array(all_topics_data[topic][f'window {i}'])) - np.std(np.array(all_topics_data[topic][f'window {i}']))\n",
    "            if tmp_res > tmp_best:\n",
    "                tmp_all_vals = all_topics_data[topic][f'window {i}']\n",
    "                tmp_best = tmp_res\n",
    "                tmp_win = i\n",
    "                tmp_avg = np.mean(np.array(all_topics_data[topic][f'window {i}']))\n",
    "                tmp_std = np.std(np.array(all_topics_data[topic][f'window {i}']))\n",
    "        best_windows[topic]['best_window'] = tmp_win\n",
    "        best_windows[topic]['avg_value'] = tmp_avg\n",
    "        best_windows[topic]['std_value'] = tmp_std\n",
    "        #print(\"ALL VALS:\",tmp_all_vals)\n",
    "            #average_windows[topic]['avg_value'] = round(np.mean(np.array(all_topics_data[topic]['values'])),3)\n",
    "            #average_windows[topic]['std_value'] = round(np.std(np.array(all_topics_data[topic]['values'])), 3)\n",
    "\n",
    "    return best_windows\n",
    "\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "filename_old = 'all_similarities_Alice_base_old.pkl' #'all_similarities_standard_base_old.pkl' #\"all_processed_sentences_best.pkl\"\n",
    "filename =  'all_similarities_Alice_base.pkl'\n",
    "#filename_best = \"all_processed_sentences_best.pkl\"\n",
    "#filename_local = \"all_processed_sentences_local.pkl\"\n",
    "\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename))):    #load data already processed\n",
    "    results = pickle.load(open(os.path.join(path,filename),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path\")\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_old))):    #load data already processed\n",
    "    results_old = pickle.load(open(os.path.join(path,filename_old),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path\")\n",
    "\n",
    "#print(results[0])\n",
    "\n",
    "b_windows = best_windows(results+results_old, all_refs, new_th = 0.3, type = 'sentences')\n",
    "for i,el in enumerate(list(b_windows.keys())):\n",
    "    b_windows[el]['priority'] = np.random.randint(1000) #add a priority to the topic (now they are random, the order has yet to be decided)\n",
    "\n",
    "filename_res = \"best_windows_sentences_Alice_base_0_3.pkl\"\n",
    "print(\"writing to file... \\n \\n\")\n",
    "pickle.dump(b_windows , open(os.path.join(path,filename_res),'wb'))\n",
    "\n",
    "for el in list(b_windows.keys()):\n",
    "    print(f\"{el}: {b_windows[el]}\", \"\\n\", \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### MULTIPLE REFERENCES. USE THIS IF YOU WANT TO ITERATE FOR MULTIPLE REFERENCES, E.G. FOR IOU AND AP COMPUTATION\n",
    "\n",
    "reference_sentence_greet = {'Alice':[\"See you soon\",\"How are you? I haven’t seen you for a long time\",\"Hi! Are you here too?\",\"Have a good evening!\"],\n",
    "                         'Alice_random':[\"Hi! Are you here too?\"], 'Carmine_nuove':[\"Goodbye to everyone and thank you for listening.\", \"Good morning! So good to see you again!\", \"Hey! Hello! You've finally arrived.\", \"Hi everyone, I'm Carmine.\"],'Carmine_nuove_random':[\"Hi everyone, I'm Carmine.\"],\n",
    "                         'Antonio_base':['hello','how are you','goodbye','hey there'],'Antonio_large':['hello','how are you','goodbye','hey there'],'Antonio_random':['hello'], 'Lucrezia':[\"Hello!\",\"See you soon!\",\"Goodbye!\",\"Come back soon!\"], 'Lucrezia_random':[\"Hello!\"]}\n",
    "\n",
    "reference_sentence_idk = {'Alice':[\"I don’t know where is it\",\"I have no idea\",\"I don’t know if I like it\",\"I don’t remember what time is it\"],\n",
    "                         'Alice_random':[\"I don’t know if I like it\"], 'Carmine_nuove':[\"I really have no idea where John is.\",\"I don't know what I'm going to do tomorrow.\",\"I'm speechless, I don't know what to say.\", \"Don’t ask me, I'm not an expert.\"],'Carmine_nuove_random':[\"I'm speechless, I don't know what to say.\"], 'Antonio_base':[\"I don't know\",'maybe','who knows',\"I can't help you\"],'Antonio_large':[\"I don't know\",'maybe','who knows',\"I can't help you\"],'Antonio_random':[\"I don't know\"], 'Lucrezia':[\"I have no idea...\",\"I don't know what to say...\",\"There's nothing I can do...\",\"Do as you please!\"], 'Lucrezia_random':[\"Do as you please!\"]}\n",
    "\n",
    "reference_sentence_no = {'Alice':[\"No, I’m not doing all myself\",\"We can’t keep spending money on useless items\",\"I won’t walk all day\",\"No, I won’t go to the club\"],\n",
    "                         'Alice_random':[\"No, I’m not doing all myself\"], 'Carmine_nuove':[\"What you have done is not good.\",\"I don't want anything more.\",\"I can't believe what I saw.\",\"I don't have what you ask for.\"],'Carmine_nuove_random':[\"I can't believe what I saw.\"], 'Antonio_base':['no','never','I did not','not at all'],'Antonio_large':['no','never','I did not','not at all.'] ,'Antonio_random':['no'], 'Lucrezia':[\"It wasn't me!\",\"I don't know how it could've happened\",\"I don't like it.\",\"I don't feel like it!\"], 'Lucrezia_random':[\"I don't know how it could've happened\"]}\n",
    "\n",
    "reference_sentence_yes = {'Alice':[\"All right\",\"Yes\",\"Okay\",\"Yes I’m really happy\"], 'Alice_random':[\"Okay\"],\n",
    "                          'Carmine_nuove':[\"Yes thank you, I will gladly accept!\",\"It's exactly as you say.\",\n",
    "                          \"If you insist I will come to the party.\",\"You have answered my question correctly.\"],'Carmine_nuove_random':[\"Yes thank you, I will gladly accept!\"], 'Antonio_base':['yes','sure','of course','with pleasure'],'Antonio_large':['yes','sure','of course','with pleasure'],'Antonio_random':['yes'], 'Lucrezia':[\"Okay, let's go!\",\"I really like it.\",\"You're right\",\"Got it, clear.\"], 'Lucrezia_random':[\"Okay, let's go!\"]}\n",
    "\n",
    "reference_sentence_deictic = {'Alice':[\"We gotta get to the top of that mountain over there\",\"Would you hand me that pen?\",\n",
    "                              \"Take the flour in that cupboard\",\"Look at the sea!\"], 'Alice_random':[\"Take the flour in that cupboard\"], 'Carmine_nuove':[\"The building is exactly behind you.\",\"Look at what's going on there!\",\n",
    "                              \"Could you please get that book for me?\",\"Go there immediately and don't move.\"],'Carmine_nuove_random':[\"Go there immediately and don't move.\"], 'Antonio_base':['look at','there','that place','that object'],'Antonio_large':['look at','there','that place','that object'],'Antonio_random':['that object'], 'Lucrezia':[\"Who is that?\",\"Go straight until there.\", \"Can you take the object on the table?\",\"Up there is the Big Dipper constellation.\"], 'Lucrezia_random':[\"Up there is the Big Dipper constellation.\"]}\n",
    "\n",
    "reference_sentence_stop = {'Alice':[\"Stop, I don’t understand anything\",\"Let’s just calm down\",\"Let’s stop and take a breath\",\"Stop, I’m exhausted.\"],                                          'Alice_random':[\"Stop, I’m exhausted.\"], 'Carmine_nuove':[\"Wait, you're talking too fast.\",\"Stop where you are, the floor is slippery.\",                              \"Quiet, listen, there's a noise upstairs.\",\"I don't want to know about it.\"],'Carmine_nuove_random':[\"I don't want to know about it.\"],                              'Antonio_base':['stop','do not move','give me a break','stay away'],'Antonio_large':['stop','do not move','give me a break','stay                                    away'],'Antonio_random':['do not move'], 'Lucrezia':[\"Everyone stop!\",\"Watch out, the floor is wet!\",\"Don't move.\",\"I'm stopping you right                           now: I've already done it.\"], 'Lucrezia_random':[\"I'm stopping you right now: I've already done it.\"]}\n",
    "\n",
    "reference_sentence_run = {'Alice':[\"The train is leaving\", \"I have to run\",\"To lose weight I have to go for a run\",\"I ran all over the shops\"],\n",
    "                          'Alice_random':[\"I ran all over the shops\"], 'Carmine_nuove':[\"Do you want to go for a run?\", \"I just got back from a good run.\",\"I finished second in the cross-country race.\",\"I hope to win the 100-meter flat race.\"],'Carmine_nuove_random':[\"I finished second in the cross-country race.\"], 'Antonio_base':['shall we go',\"let's go\",'have a walk','have a run'],'Antonio_large':['shall we go',\"let's go\",'have a walk','have a run'],'Antonio_random':[\"let's go\"], 'Lucrezia':[\"Shall we go for a run?\",\"Are you ready to run?\",\"I just finished a nice treadmill run.\",\"You should get some exercise!\"], 'Lucrezia_random':[\"You should get some exercise!\"]}\n",
    "\n",
    "reference_sentence_apologize = {'Alice':[\"I’m so sorry\",\"I didn’t think you were hurt\",\"I’m sorry I made you wait\", \"I'm sorry, I can't now.\"],\n",
    "                         'Alice_random':[\"I'm sorry, I can't now.\"], 'Carmine_nuove':[\"It's all my fault.\",\"I'm sorry, I guess I misunderstood.\",\"I'm terribly sorry for the delay.\", \"Ehy, it's okay, don't fret.\"],'Carmine_nuove_random':[\"It's all my fault.\"], 'Antonio_base':['sorry','I beg your pardon','apologizes',\"don't be mad\"],'Antonio_large':['sorry','I beg your pardon','apologizes',\"don't be mad\"],'Antonio_random':['apologizes'], 'Lucrezia':[\"I won't do it anymore!\",\"I didn't mean to offend you.\",\"Don't take it personally!\", \"It's not my fault.\"], 'Lucrezia_random':[\"I didn't mean to offend you.\"]}\n",
    "\n",
    "reference_sentence_beg = {'Alice':[\"Will you please help me with the computer exam?\",\"Would you please take me to the party?\",\n",
    "                          \"Would you please buy me an icecream?\", \"Would you like to go to Paris?\"],\n",
    "                         'Alice_random':[\"Will you please help me with the computer exam?\"], 'Carmine_nuove':[\"Please, I absolutely need you now.\",\"Let's hope for the best.\", \"Please stop it!\", \"God bless you!\"],'Carmine_nuove_random':[\"God bless you!\"], 'Antonio_base':['please','do it','do me a favour','help me'],'Antonio_large':['please','do it','do me a favour','help me'],'Antonio_random':['please'], 'Lucrezia':[\"Will you come with me?\",\"Come on, please!\",\"Do it for me!\", \"Let's hope everything goes well!\"], 'Lucrezia_random':[\"Will you come with me?\"]}\n",
    "\n",
    "reference_sentence_time = {'Alice':[\"can you hurry up and get dressed?\",\"The bus leaves in ten minutes\",\"Would you hurry up?\",\"I stay there one hour\"],\n",
    "                         'Alice_random':[\"Would you hurry up?\"], 'Carmine_nuove':[\"You’re late, have you seen what time it is?\",\"Hurry up, they are expecting us.\",\n",
    "                           \"You only have 2 minutes left for your presentation.\",\"It's time to get back to work.\"],'Carmine_nuove_random':[\"It's time to get back to work.\"], 'Antonio_base':[\"it's late\",\"it's time to\",'time is running','we have to go'],'Antonio_large':[\"it's late\",\"it's time to\",'time is running','we have to go'],'Antonio_random':['time is running'], 'Lucrezia':[\"I've been waiting for you forever!\",\"So, are you ready? We need to go!\",\"Do you see what time it is?\",\"How much longer until we're done?\"], 'Lucrezia_random':[\"I've been waiting for you forever!\"]}\n",
    "\n",
    "reference_sentence_clap = {'Alice':[\"You were good to take such a high grade\",\"The competition was a success, congratulations really!\",\n",
    "                            \"The cake is delicious\",\"I’m so glad you could buy a house\"], 'Alice_random':[\"congratulations really!\"], 'Carmine_nuove':[\"Congratulations, really an interesting speech.\",\"Best wishes to the bride and groom!\",\"You did a really good job.\",\"Bravo! Encore!\"],'Carmine_nuove_random':[\"Best wishes to the bride and groom!\"], 'Antonio_base':['congratulations','well done','bravo!','good job!'],'Antonio_large':['congratulations','well done','bravo!','good job.'],'Antonio_random':['bravo!'], 'Lucrezia':[\"Well done!\",\"I really enjoyed it!\",\"I couldn't have done better myself!\",\"I would've never expected that from you!\"], 'Lucrezia_random':[\"I would've never expected that from you!\"]}\n",
    "\n",
    "reference_sentence_exulting = {'Alice':[\"What a great goal we made!\",\"I finally graduated!\",\"I win first prize!\",\"I finally got home\"],\n",
    "                               'Alice_random':[\"I finally graduated!\"], 'Carmine_nuove':[\"Yay! The concert begins!\",\"I finally made it!\",\"My team has won the game!\",\"I didn't expect you to come!\"],'Carmine_nuove_random':[\"I finally made it!\"], 'Antonio_base':['I did it!','hooray!','I won!','I am the best'],'Antonio_large':['I did it!','hooray!','I won!','I am the best'],'Antonio_random':['I did it!'], 'Lucrezia':[\"We won!\",\"Let's go celebrate!\",\"I did it!\",\"Woohoo, it works!\"], 'Lucrezia_random':[\"We won!\"]}\n",
    "\n",
    "topic_gesture_description = {\"Greeting\":(\"r_greet\", \"the gesture that you use when you greet people\"), \\\n",
    "                             \"Run\": (\"r_run\",\"moving arms aback and forth while maintaining an angle of 90° with the elbow to simulate a running movement\"),\n",
    "                            \"I apologize\":(\"r_apologize\", \"the gesture that you make when you ask someone to forgive you, for instance by tilting the upper body or by keeping arms up, palms facing up and elbows bent so that they form a 90° angle\"),\\\n",
    "                             \"I am exulting\":(\"r_exulting\", \"the gesture that you make when you are exulting (for example by putting both your arms up and by putting your keeping your hands in the shape of a fist)\"), \\\n",
    "                             \"I praise you\": (\"r_clap\",\"clapping with hands to congratulate with someone\"), \\\n",
    "                             \"No\":(\"r_no\", \"moving the head or a finger left and right when you want to say no\"), \\\n",
    "                             \"Yes\":(\"r_yes\",\"moving the head up and down when you are agreeing for something or you want to say yes\"), \\\n",
    "                             \"Indicate a place or object\":(\"r_deitic\",\"the gesture that you use for indicating a place or object (with head, finger, etc.)\"),\n",
    "                             \"Stop\":(\"r_stop\",\"gesture that you make with arms to tell someone that he as to slow down or stop to do something (for example with arms ahead)\"), \\\n",
    "                             \"I don't know\":(\"r_idk\", \"moving the shoulders up and down or move both the arms in front of you with palms facing up, used when you want to to show that you don't know something\"), \\\n",
    "                             \"I beg you\":(\"r_beg\",\"the gesture that you make when you are begging someone, for instance by keeping arms in a pray position\"), \\\n",
    "                             \"C'mon, it's late\":(\"r_time\", \"indicating the wrist with an index to say that it's late for something\")}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "################ NEW\n",
    "\n",
    "### COMPUTE RESULTS FOR FIXED WINDOW\n",
    "\n",
    "#TODO SET PRIORITY DEPENDING ON SIMILARITY MATCH\n",
    "#TODO DO COMPUTATIONS AGAIN AND DO NOT USE ALL_SIMILARITIES PRECIOUSLY COMPUTED\n",
    "\n",
    "# give priority to each topic. Use the best window of each topic to take data of the sentence. If, for a topic, we have overlapping windows, chose the one that returns the best value.\n",
    "# After we choose a topic given the value, the window size and the priority, all the words of that window can't be assigned to any other topic\n",
    "\n",
    "\n",
    "\n",
    "def process_results_priority_policy_best_windows(best_windows = {} ,sentences = [], new_th = 0.2, type = 'conversation'):\n",
    "\n",
    "    final_labels=[]\n",
    "    #topics_ordered = sorted(best_windows.items(), key=lambda x: x[1]['priority'], reverse=True) #order the topics on priority values\n",
    "    window_max_dim = 10\n",
    "    counter = 0\n",
    "\n",
    "    for k,sentence in enumerate(sentences):\n",
    "        sentence_labels = {'sentence':'', 'topics&ref':[],  'matching_words':[], 'values':[], 'processing_time': 0,\n",
    "                           'sentence_index':k, 'word_indexes':[]} #it contains a sentence labelled\n",
    "        sentence_labels['sentence'] = sentence\n",
    "        sentence_splits = sentence.split()\n",
    "        final_word_index = len(sentence_splits) - 1 #index of the last word (starting from 0)\n",
    "        current_word_index = 0\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        while current_word_index <= final_word_index:\n",
    "            #topics_best_res = {}\n",
    "            top_res = 0\n",
    "            top_words = ''\n",
    "            top_ref = ''\n",
    "            best_topic = ''\n",
    "            top_word_indexes = []\n",
    "            top_window = 0\n",
    "            for key in all_refs.keys():    #for all types of references\n",
    "                window_dim = best_windows[key]['best_window']\n",
    "                if window_dim == 0:\n",
    "                    continue\n",
    "                if current_word_index + window_dim > final_word_index: #then we can't  compute result with this topic\n",
    "                    continue\n",
    "                for ref_sentence in all_refs[key]: #for all references\n",
    "                    sub_sentence = ' '.join(sentence_splits[current_word_index : current_word_index + window_dim])\n",
    "                    tmp_res = model.predict([sub_sentence,ref_sentence])\n",
    "                    #print(window_dim,key,tmp_res, top_res)\n",
    "                    if tmp_res > top_res:\n",
    "                        top_words = sub_sentence\n",
    "                        top_ref = ref_sentence\n",
    "                        top_res = tmp_res\n",
    "                        best_topic = key\n",
    "                        top_window = window_dim\n",
    "                        top_word_indexes = [i for i in range(current_word_index,current_word_index + window_dim)]\n",
    "                #topics_best_res[key] = top_res\n",
    "            #max_topic = max(topics_best_res, key=topics_best_res.get) #find the topic with highest match score\n",
    "            #max_value = topics_best_res[max_topic]\n",
    "            if top_res == 0:  #then we reached the end of the sentence\n",
    "                break\n",
    "            elif top_res >= new_th: #then we have a match\n",
    "                #print(\"I'm here\", ((best_topic, top_words,top_res, top_window)))\n",
    "                sentence_labels['topics&ref'].append((best_topic,top_ref)) #save the result\n",
    "                sentence_labels['matching_words'].append(top_words)\n",
    "                sentence_labels['values'].append(top_res)\n",
    "                sentence_labels['word_indexes'].append(top_word_indexes)\n",
    "                current_word_index +=  top_window + 1\n",
    "                continue\n",
    "            else:\n",
    "                current_word_index = current_word_index + 1\n",
    "                continue\n",
    "        end_time = time.perf_counter()\n",
    "        processing_time = round(end_time-start_time, 4)\n",
    "        print(\"ELAPSED TIME:\", processing_time)\n",
    "        counter += 1\n",
    "        print(sentence_labels)\n",
    "        print(\"SENTENCE INDEX: \", counter)\n",
    "        sentence_labels[\"processing_time\"] = processing_time\n",
    "        final_labels.append(sentence_labels)\n",
    "    return final_labels\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "#filename = 'all_similarities_600.pkl'\n",
    "#filename_best = \"new_all_processed_sentences_best.pkl\"\n",
    "#filename_local = \"all_processed_sentences_local.pkl\"\n",
    "#filename_windows = \"best_windows_200_sentences.pkl\"\n",
    "#filename_windows = 'best_windows_sentences_Antonio_random_large_0_5.pkl'\n",
    "#filename_sentences = \"sentences_gpt_300_old.pkl\"\n",
    "\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "names = [\"Antonio\",\"Antonio_random\",\"Alice\",\"Alice_random\",\"Carmine_nuove\",\"Carmine_nuove_random\",\"Lucrezia\",\"Lucrezia_random\"]\n",
    "th = [0.3, 0.6, 0.9] #0.9\n",
    "th_string = [\"0_3\", \"0_6\", \"0_9\"] #, \"0_9\" , \"0_5\"\n",
    "models = [\"base\",\"large\"]\n",
    "types = [\"\",\"old\"]\n",
    "\n",
    "\n",
    "for mod in models:\n",
    "    model = CrossEncoder(f'cross-encoder/stsb-roberta-large')  #stsb-roberta-large\n",
    "    #( model.config.model_type)\n",
    "    for name in names:\n",
    "        for i,t in enumerate(th):\n",
    "            filename_windows = f\"best_windows_sentences_{name}_{mod}_0_3.pkl\"\n",
    "            b_windows = pickle.load(open(os.path.join(path,filename_windows),'rb'))\n",
    "            for type in types:\n",
    "                if type == \"\":\n",
    "                    filename_sentences = \"sentences_gpt_300.pkl\"\n",
    "                    filename_labels = f\"results_priority_policy_best_windows_{name}_{mod}_{th_string[i]}.pkl\"\n",
    "                else:\n",
    "                    filename_sentences = \"sentences_gpt_300_old.pkl\"\n",
    "                    filename_labels = f\"results_priority_policy_best_windows_{name}_{mod}_{type}_{th_string[i]}.pkl\"\n",
    "\n",
    "                if os.path.exists((os.path.join(path,filename_labels))):\n",
    "                    print(f\"FILE {filename_labels} already exists, SKIPPED\")\n",
    "                    continue\n",
    "                sentences = pickle.load(open(os.path.join(path,filename_sentences),'rb'))\n",
    "                if name == \"Antonio\" and mod == \"base\":\n",
    "                    n_tmp = \"Antonio_base\"\n",
    "                elif name == \"Antonio\" and mod == \"large\":\n",
    "                    n_tmp = \"Antonio_large\"\n",
    "                else:\n",
    "                    n_tmp = name\n",
    "\n",
    "                all_refs = {\"r_greet\":reference_sentence_greet[n_tmp], \"r_clap\":reference_sentence_clap[n_tmp],  \"r_idk\":reference_sentence_idk[n_tmp], \"r_no\":reference_sentence_no[n_tmp],\"r_yes\":reference_sentence_yes[n_tmp],\"r_stop\":reference_sentence_stop[n_tmp], \"r_beg\":reference_sentence_beg[n_tmp],\"r_time\":reference_sentence_time[n_tmp], \"r_apologize\":reference_sentence_apologize[n_tmp], \"r_deitic\":reference_sentence_deictic[n_tmp],\"r_exulting\":reference_sentence_exulting[n_tmp],\"r_run\":reference_sentence_run[n_tmp]}\n",
    "\n",
    "                labels = process_results_priority_policy_best_windows(best_windows=b_windows,sentences=sentences, new_th=t, type = 'sentences')\n",
    "                print(\"writing to file:\",filename_labels)\n",
    "                pickle.dump(labels, open(os.path.join(path,filename_labels),'wb'))\n",
    "\n",
    "'''\n",
    "if(os.path.exists(os.path.join(path,filename_windows))):    #load data already processed\n",
    "    b_windows = pickle.load(open(os.path.join(path,filename_windows),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path windows\")\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_sentences))):    #load sentences\n",
    "    sentences = pickle.load(open(os.path.join(path,filename_sentences),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path sentences\")\n",
    "\n",
    "print(b_windows)\n",
    "labels = process_results_priority_policy_best_windows(best_windows=b_windows,sentences=sentences, new_th=0, type = 'sentences')\n",
    "filename_labels =  \"results_priority_policy_best_windows_Antonio_random_large_old_0_0.pkl\"\n",
    "print(\"writing to file...\")\n",
    "pickle.dump(labels  , open(os.path.join(path,filename_labels),'wb'))\n",
    "\n",
    "for label in labels:\n",
    "    print(label, \"\\n \\n \\n \\n\")\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### MOVING WINDOW LABELLING\n",
    "\n",
    "# this function process the results obtained previously that contain all the possible matches between the words inside each sentence and all the references (grouped by topic, or gesture)\n",
    "# by using the sentence similarity. The function must have the following characteristics:\n",
    "# 1) only the results greater than a certain threhsold new_th are accepted\n",
    "# 2) in case we have a match between words inside a window and more than one references related to one topic, we take only the best one related to that topi (already true in\n",
    "# all_processed_sentences_best.pkl file)\n",
    "# 3) in case we have more topics that have a match with words in a window, we take the topic related to the best match\n",
    "# 4) the functions has to start with a window_dim equal to 1, then compares the best match of a topic with window one, with the matches found by incrementing the window size: it takes\n",
    "# the best one\n",
    "# 5) once we found the best window that matches to best topic, we try to see if, by incrementing n_times the window size, the result for that topic changes more than a given threshold.\n",
    "# If it changes more than a given threshold at least one time, that best topic is no more considered valid\n",
    "# 6) if the topic is no more considered valid, it restarts from window_dim equal to 1 but this time it starts to search considering only from second best topic to below.\n",
    "# 7) repeats steps 5 and 6 until it find a match that works.\n",
    "# 8) if it find a match that works, we start a new search by moving the window by 1 and comparing all the results obtained with the current best. If a better one is found, substitute the\n",
    "# the best window with the one found, otherwise continue to move until the window no more contain the words of the best window.\n",
    "# 9) finally, if for a word we don't have a match, the algorithm moves to the next word\n",
    "# TODO add a full stop control\n",
    "\n",
    "def process_results_best_policy(results, all_refs, new_th=0 ,window_max_dim=10, type = 'conversation'): #the maximum number of words that the window can contain\n",
    "\n",
    "    #takes all elements of the sentence that have a value greater than threshold\n",
    "    def take_elements_th(el, window_dim, new_th):\n",
    "        #print(el)\n",
    "        all_values = el[f\"window {window_dim}\"]['value']  #all the values (results of the matches between topics and parts of the sentence\n",
    "        th_indexes = [i for i in range(len(all_values)) if all_values[i] > new_th] #returns the indexes of the values that are greater than new_th\n",
    "        values = list(map(all_values.__getitem__,th_indexes)) #take all the values greater than new_th\n",
    "        refs = list(map(el[f\"window {window_dim}\"]['ref'].__getitem__,th_indexes)) #take all the references that return a value greater than new_th\n",
    "        matching_words = list(map(el[f\"window {window_dim}\"]['sentence'].__getitem__,th_indexes)) #take all the sentence parts that return a value greater than new_th\n",
    "        start_word_indexes = list(map(el[f\"window {window_dim}\"]['start_word_indexes'].__getitem__,th_indexes))\n",
    "\n",
    "        return values, refs, matching_words, start_word_indexes\n",
    "\n",
    "    #n_tries indicates how many times we want to check that the threshold in maintained trough time\n",
    "    # t_res contains the result referred to the best topic found in a part of the sentence\n",
    "    # s_res contains all the results obtained by applying the sentence model over a sentence (only results greater tan a certain threshold are taken)\n",
    "    # sentence contains the sentence that we are processing\n",
    "    # max_dim is the maximum size of the window that we can apply over that sentence\n",
    "    def is_coherent(n_tries, th, s_res, t_res, sentence, max_dim):\n",
    "        s_splits = sentence.split()\n",
    "        current_topic = t_res['topic'] #the topic found\n",
    "        current_topic_value = t_res['best_match'] #the value referred to the topic found\n",
    "        end_index = t_res['new_last_word_idx'] + 1 #the index of the last word of the topic found\n",
    "        matching_words = t_res['matching_words'] # a sentence can have more matching_words inside, we have to pick the right ones\n",
    "        start_index = t_res['old_last_word_idx'] + 1\n",
    "        if t_res['new_last_word_idx'] == len(s_splits) - 1: #in this case we reached the end and we have nothing else to check (we assume that is coherent)\n",
    "            return True\n",
    "        #print(\"IS COHERENT:\",matching_words, s_splits[start_index:end_index])\n",
    "        #print(\"Sentence:\",sentence)\n",
    "        #print(\"t_res\", t_res['topic'], t_res['best_match'], t_res['matching_words'],\"start\", start_index,\"end\", end_index,\"len\", len(matching_words),\"ind\", t_res['new_last_word_idx'])\n",
    "        #print(\"COHERENCE\",matching_words, s_splits[start_index:end_index])\n",
    "        assert(matching_words == ' '.join(s_splits[start_index:end_index])) #check if the words we are picking are the same of the matching\n",
    "        topic_ok = False\n",
    "\n",
    "        for t in range(1,n_tries+1):\n",
    "            if end_index + t > len(s_splits) - 1 or len(matching_words.split()) + t > max_dim: #check if, by increasing the window size, we reach the end of the sentence or if we can\n",
    "                return True                                                                    #try to search results in a greater window\n",
    "            else:\n",
    "                window_dim = len(matching_words.split()) + t\n",
    "                s_results = s_res[f\"window {window_dim}\"]\n",
    "                processed_words = ' '.join(s_splits[start_index:(start_index + window_dim)])\n",
    "                #print(\"IS COHERENT 2:\",t_res['old_match_idx'])\n",
    "                start_search_index = t_res['old_match_idx'][f\"window {window_dim}\"] #since we are assuming that the match is still not valid, start to search values from old match_indexes\n",
    "                #print(\"IS COHERENT 3:\",processed_words, s_results['matching_words'][start_search_index:])\n",
    "                if not processed_words in s_results['matching_words'][start_search_index:]: #then the topic doesn't exists anymore, so fur sure we have a drop in probabilities\n",
    "                    return False\n",
    "                else:\n",
    "                    match_index = s_results['matching_words'][start_search_index:].index(processed_words)\n",
    "                    n_possible_topics = 0\n",
    "                    while s_results['matching_words'][start_search_index:][match_index:][n_possible_topics] == processed_words: #count how many topics match with processed words\n",
    "                        n_possible_topics = n_possible_topics + 1\n",
    "                        if n_possible_topics >= len(s_results['matching_words'][start_search_index:][match_index:]):\n",
    "                            break\n",
    "                    all_match_values = s_results['values'][start_search_index:][match_index : match_index + n_possible_topics]\n",
    "                    for value in all_match_values:\n",
    "                        tmp_ref = s_results['refs'][start_search_index:][s_results['values'][start_search_index:].index(value)] #extract the reference associated with the value\n",
    "                        tmp_topic = [key for key in all_refs.keys() for ref in all_refs[key] if tmp_ref == ref][0] #extract the topic associated with reference\n",
    "                        if tmp_topic == current_topic:\n",
    "                            if current_topic_value - value < th: #then the topic does not change so mach by incrementing the window size, check for another n_try\n",
    "                                topic_ok = True\n",
    "                                break\n",
    "                        else: #check the other topic\n",
    "                            continue\n",
    "                    if topic_ok:\n",
    "                        continue\n",
    "                    else:\n",
    "                        return False\n",
    "        return True #if we reached this point than everything is ok. If the function returns False, than we have to avoid the topic\n",
    "\n",
    "    #this function checks if the topic found is for sure the best one: since we didn't try to search results that starts after a given word, we are not sure for that\n",
    "    #the paramters are the same as the previous function except for n_tries\n",
    "    #the function returns a dictionary which tells if the current topic is the best one; if it is the best one that dictionary contain that topic otherwise\n",
    "    #it will contain the best topic (i.e. the topic that returns greater results). The dictionary contains also the skipped words in case we the best topic changes by sliding the window\n",
    "    def is_real_best(s_res, t_res, sentence, max_dim):\n",
    "        s_splits = sentence.split()\n",
    "        max_th_drop = 0.2 #for coherence check\n",
    "        current_topic = t_res['topic'] #the topic found\n",
    "        current_topic_value = t_res['best_match'] #the value referred to the topic found\n",
    "        end_index = t_res['new_last_word_idx'] + 1  #the index of the last word of the topic found\n",
    "        matching_words = t_res['matching_words'] # a sentence can have more matching_words inside, we have to pick the right ones\n",
    "        #matching_words_indexes = t_res['matching_words_indexes']\n",
    "        start_index = t_res['old_last_word_idx'] + 1\n",
    "        skipped_words = ''\n",
    "        end = False\n",
    "        #tmp_new_match_idx = t_res['old_match_idx'].copy()\n",
    "        if t_res['new_last_word_idx'] == len(s_splits) - 1: #in this case we reached the end and we have nothing else to check (we assume that is the best topic)\n",
    "            return {\"is_best\": True, \"topic_result\": t_res, \"skipped_words\":skipped_words}\n",
    "        assert(matching_words == ' '.join(s_splits[start_index:end_index])) #check if the words we are picking are the same of the matching\n",
    "        tmp_best_res = t_res.copy()\n",
    "        #processed_words = ' '.join(s_splits[start_index:(start_index + 1 + window_dim)])\n",
    "        if len(matching_words.split()) == 1: #in this case we have nothing to check since in the next iteration we will search for other words that will not contains the current one\n",
    "            return {\"is_best\": True, \"topic_result\": t_res, \"skipped_words\":skipped_words}\n",
    "        else:\n",
    "            topics_to_avoid = {'topic & dim': []}\n",
    "            tmp_topics_to_avoid =  topics_to_avoid.copy()\n",
    "            for t in range(1,len(matching_words.split())-1):\n",
    "                '''\n",
    "                w_d = 1 #window dimension ############################### this block is used to start to search results from the right index\n",
    "                while t_res['old_last_word_idx'] + w_d <= len(s_splits) - 1 and w_d <= max_dim:    #until we reach the end of the sentence\n",
    "                    s_results = s_res[f\"window {w_d}\"]\n",
    "                    if len(s_results['start_word_indexes']) != 0:\n",
    "                        tmp = [i for i in range(len(s_results['start_word_indexes'])) if s_results['start_word_indexes'][i] > t_res['old_last_word_idx'] + w_d - 1]\n",
    "                        if len(tmp) > 0: #it means that we have at least one value that will possibly match\n",
    "                            tmp_idx_start_match_idx[f\"window {w_d}\"] = tmp[0]\n",
    "                        else:\n",
    "                            tmp_idx_start_match_idx[f\"window {w_d}\"] = len(s_results['matching_words']) #we can avoid to search again\n",
    "                        w_d = w_d + 1\n",
    "                '''\n",
    "                while True:  #try to search if there exists a topic that is coherent (does not change by th if we increase the window) and that is better than the current one\n",
    "                    next_result = find_best_topic(s_res, max_dim, t_res['old_match_idx'], start_index + t, sentence, topics_to_avoid = topics_to_avoid)\n",
    "                    #print(next_result)\n",
    "                    if next_result['new_last_word_idx'] != next_result['old_last_word_idx']: ## then we have a new match\n",
    "                        #then this topic associated to these words is not coherent, check the next word\n",
    "                        if not is_coherent(n_tries = 1, th = max_th_drop, s_res = s_res, t_res = next_result, sentence = sentence, max_dim = max_dim): #skip this topic and try to search the next\n",
    "                            topics_to_avoid['topic & dim'].append((next_result['topic'],next_result['best_window'])) #######\n",
    "                            continue\n",
    "                        if next_result['best_match'] > current_topic_value:\n",
    "                            tmp_best_res = next_result.copy()\n",
    "                            skipped_words = s_splits[start_index : start_index + t]\n",
    "                            topics_to_avoid['topic & dim'] = []\n",
    "                        break #in any case we stop the topic search\n",
    "                    if topics_to_avoid != tmp_topics_to_avoid: #then we have a new topic to avoid and we can search if there are som other topics better than the current one\n",
    "                        tmp_topics_to_avoid = topics_to_avoid.copy()\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                    #if next_result['end']: #we reached the end and we didn't find any result\n",
    "                    #    break\n",
    "\n",
    "        return {\"is_best\": False, \"topic_result\": tmp_best_res, \"skipped_words\":skipped_words}\n",
    "\n",
    "    #this function finds the best topic starting from a given word. The best topic is the topic that, for a certain window_dim that contains some of the words of the sentence,\n",
    "    #returns the best results. If for a given word we can't find any match, the function returns some default values, otherwise it retuns the best topic, the associated\n",
    "    #reference and value, the words of the sentence that return the best topic, the window_dim that contains those words, the old and new indexes that tell us where to start\n",
    "    #at the next iteration for both the sentence and the results related to that sentence. If this function is called and we fund that the returned result is for real the best one (for\n",
    "    #some define policy) then we update all the indexes, otherwise we maintain the old ones and we restart teh search by not considering the topics that do not respect the policy.\n",
    "    def find_best_topic(s_res, max_dim, idx_last_matches, idx_last_word, #starting from idx_last_word+1, it will find the window that returns the best topic value\n",
    "                        sentence, topics_to_avoid ={}): #topics_to_avoid indicates which topics we can't search for (e.g. if we found that a topic does not respect the                                                 #policy). and the referred window_dim on which we can't search for that topic\n",
    "        s_splits = sentence.split()\n",
    "        best_match = 0 #initialize the results in case we do not find anything\n",
    "        best_window = 1\n",
    "        ref = ''\n",
    "        topic = ''\n",
    "        matching_words = ''\n",
    "        matching_words_indexes = None\n",
    "        tmp_best_value_match = 0\n",
    "\n",
    "        tmp_idx_last_matches = idx_last_matches.copy()\n",
    "        #print(idx_last_word)\n",
    "        #print(idx_last_matches)\n",
    "        tmp_idx_last_word = idx_last_word\n",
    "        window_dim = 1\n",
    "        end = False #this becomes true when we reached the end of the sentence, i.e. if we are in the last word\n",
    "\n",
    "        if idx_last_word + window_dim == len(s_splits):\n",
    "            end = True\n",
    "\n",
    "        #print(s_res)\n",
    "        #print(max_dim)\n",
    "        while idx_last_word + window_dim <= len(s_splits) - 1:    #until we reach the end of the sentence\n",
    "            #for window in range(1,max_dim + 1):\n",
    "            s_results = s_res[f\"window {window_dim}\"]\n",
    "            #print(idx_last_matches)\n",
    "            start_search_index = idx_last_matches[f\"window {window_dim}\"] #we take the data inside s_res from this index since the previous ones were already used for search purposes\n",
    "            processed_words = ' '.join(s_splits[(idx_last_word+1):(idx_last_word + 1 + window_dim)]) #we start with window = 1\n",
    "            processed_words_indexes = [i for i in range((idx_last_word+1),(idx_last_word + 1 + window_dim))]\n",
    "            #print(processed_words, \"    idx last match\", idx_last_matches[f\"window {window_dim}\"], \"    len results\",len(s_results['matching_words']))\n",
    "            #print(\"window_dim:\",window_dim)\n",
    "            #print(\"processed words:\",processed_words, \"  window_dim:\", window_dim, \"  dim_max:\",max_dim, \"  start_search_index:\",start_search_index)\n",
    "            #print(\"RESULTS:\", s_results['matching_words'][start_search_index:], \"window_dim:\", window_dim, processed_words)\n",
    "            if idx_last_matches[f\"window {window_dim}\"] > (len(s_results['matching_words']) - 1): #it means that we have no more results for that window_dim, check with another window\n",
    "                #print(\"continuing...\", \"   window_dim\",window_dim, \"  idx\", idx_last_matches[f\"window {window_dim}\"])\n",
    "                if max_dim > window_dim:\n",
    "                    #print(\"HERE\",max_dim,window_dim)\n",
    "                    window_dim = window_dim + 1\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            #print(processed_words, s_results['matching_words'][start_search_index:])\n",
    "            if processed_words in s_results['matching_words'][start_search_index:]: #then we have at least one match\n",
    "                index = s_results['matching_words'][start_search_index:].index(processed_words) # takes the index of the first match between a topic\n",
    "                                                                                                 # and processed words starting from the last index where we had a match previously\n",
    "                #print(\"index\",index)\n",
    "                n_possible_topics = 0\n",
    "                #print(\"HERE:\", s_results['matching_words'][start_search_index:][index:], processed_words)\n",
    "                while s_results['matching_words'][start_search_index:][index:][n_possible_topics] == processed_words: #count how many topics match with processed words\n",
    "                    n_possible_topics = n_possible_topics + 1\n",
    "                    #print(n_possible_topics)\n",
    "                    if n_possible_topics >= len(s_results['matching_words'][start_search_index:][index:]): #avoid taking elements that are outside the array\n",
    "                        break\n",
    "                # these values are associated with the topics that match the # processed words\n",
    "                all_match_values = s_results['values'][start_search_index:][index : index + n_possible_topics]\n",
    "                ordered_values = sorted(all_match_values, reverse=True) #order the values that match from the highest to the lowest\n",
    "                #print(ordered_values,tmp_best_value_match)\n",
    "\n",
    "                #tmp_idx_last_matches[f\"window {window_dim}\"] = start_search_index + index + len(ordered_values) - 1\n",
    "                #print(\"inside fun indx matches:\",idx_last_matches)\n",
    "                #print(\"topics to avoid\",topics_to_avoid,\"len ordered values\", len(ordered_values))\n",
    "                #if len(ordered_values) == len(topics_to_avoid): #then we increment the window_size and try with other topics\n",
    "                #    skip_dims.append(window_dim) #we skip until this dimension next iteration\n",
    "                #    print(\"topics before breaking:\",topics_to_avoid)\n",
    "                #    print(\"breaking... Skipping_dim:\",skip_dims)\n",
    "                #    break\n",
    "                for i in range(len(ordered_values)):\n",
    "                    tmp_match = ordered_values[i]\n",
    "                    #print(tmp_match)\n",
    "                    #print(\"match:\",tmp_best_match,\"  window_dim:\",window_dim, \"  start_search_index\", start_search_index)\n",
    "                    tmp_ref = s_results['refs'][start_search_index:][s_results['values'][start_search_index:].index(tmp_match)] #reference associated to best result\n",
    "                    #print(tmp_ref)\n",
    "                    tmp_topic = [key for key in all_refs.keys() for ref in all_refs[key] if tmp_ref == ref][0] #extract the topic associated with reference\n",
    "                    #print(\"FINAL CHECK:\",(tmp_topic,window_dim), \"FINAL CHECK 2:\",topics_to_avoid['topic & dim'])\n",
    "                    if (tmp_topic,window_dim) in topics_to_avoid['topic & dim']: #try with the next ordered value because this one referred to window_dim does not respect he policy\n",
    "                        #print(\"SKIPPED TOPIC\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        if tmp_match > tmp_best_value_match: #then save the current best\n",
    "                            #print(\"OKOKOK\")\n",
    "                            tmp_idx_last_word = idx_last_word + window_dim #Index of the last processed word.\n",
    "                            tmp_best_value_match = tmp_match\n",
    "                            matching_words = processed_words\n",
    "                            matching_words_indexes = processed_words_indexes\n",
    "                            best_window = window_dim\n",
    "                            best_match = tmp_best_value_match\n",
    "                            ref = tmp_ref\n",
    "                            topic = tmp_topic\n",
    "                            break # stop the search for this window_dim, try with a bigger window\n",
    "\n",
    "            if max_dim > window_dim:\n",
    "                window_dim = window_dim + 1\n",
    "            else: #we dindn't find any result even with the maximum window size, so we didn't find any match for a word and we can try with the next one (in the main)\n",
    "                break\n",
    "\n",
    "        # save the new indexes from which to start the search for each window. These are temporary since it may happen that the best\n",
    "        # topic found does not respect the policy (in this case indexes must not be updated).\n",
    "        # We search for the first match that has a \"last_word_index\" greater than tmp_idx_last_word ([tmp_idx_last_word + 1][0). Since start index starts from\n",
    "        # idx_last_word + 1, here we have to compensate by subtracting 1\n",
    "        window_dim = 1\n",
    "        while idx_last_word + window_dim <= len(s_splits) - 1 and window_dim <= max_dim:    #until we reach the end of the sentence\n",
    "            s_results = s_res[f\"window {window_dim}\"]\n",
    "            if len(s_results['start_word_indexes']) != 0:\n",
    "                #print(\"IDX LAST WORD:\",tmp_idx_last_word)\n",
    "                tmp = [i for i in range(len(s_results['start_word_indexes'])) if s_results['start_word_indexes'][i] > tmp_idx_last_word]#[0] # - 1\n",
    "                #print(f\"TMP INDEXES DIM :{window_dim}, RES:\", tmp_idx_last_matches[f\"window {window_dim}\"])\n",
    "                if len(tmp) > 0: #it means that we have at least one value that will possibly match\n",
    "                    tmp_idx_last_matches[f\"window {window_dim}\"] = tmp[0]\n",
    "                else:\n",
    "                    tmp_idx_last_matches[f\"window {window_dim}\"] = len(s_results['matching_words']) #we can avoid to search again\n",
    "            window_dim = window_dim + 1\n",
    "\n",
    "        #print(best_match,tmp_idx_last_word,idx_last_word )\n",
    "        return {'best_match':best_match, 'best_window':best_window, 'reference':ref, 'topic':topic, 'matching_words':matching_words,\n",
    "                'matching_words_indexes': matching_words_indexes, 'new_match_idx':tmp_idx_last_matches, 'old_match_idx':idx_last_matches,\n",
    "                'new_last_word_idx':tmp_idx_last_word, 'old_last_word_idx':idx_last_word, 'end': end, 'topics_to_avoid': topics_to_avoid}\n",
    "\n",
    "    all_labels = [] #it will contain all the labels for all the sentences\n",
    "    max_th_drop = 0.2  #if we find that the best topic x does not change by this amount for n_tries times, then this is the best topic. Otherwise we can try by taking the second best topic and by repeating the check\n",
    "    #all_res = []\n",
    "    for ind,el in enumerate(results):\n",
    "        sentence_labels = {'sentence':'', 'topics&ref':[],  'matching_words':[], 'values':[], 'priority_levels':[],\n",
    "                           'sentence_index':ind, 'word_indexes':[], \"skipped_words\":[], 'processing_t_1':0, 'processing_t_2':0} #it contains a sentence labelled\n",
    "        #sentence_labels = {'sentence':'', 'topics&ref':[],  'matching_words':[], 'values':[], \"skipped_words\":[]} #it contains a sentence labelled\n",
    "        if type == 'conversation':\n",
    "            sentence = el['whole sentence,speaker,turn'][0]\n",
    "            speaker = el['whole sentence,speaker,turn'][1]\n",
    "            turn = el['whole sentence,speaker,turn'][2]\n",
    "        elif type == 'sentences':\n",
    "            sentence = el['whole sentence']\n",
    "        else:\n",
    "            print('Undefined type')\n",
    "            return\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        sentence_labels['sentence'] = sentence\n",
    "        sentence_splits = sentence.split()\n",
    "        final_word_index = len(sentence_splits) - 1 #index of the last word (starting from 0)\n",
    "        last_processed_word_idx = -1\n",
    "        processed_words = sentence_splits[0]\n",
    "        sentence_results = {}\n",
    "        if len(sentence_splits) < window_max_dim:\n",
    "            window_max_d = len(sentence_splits)\n",
    "        else:\n",
    "            window_max_d = window_max_dim\n",
    "\n",
    "        last_match_indexes = {}\n",
    "        #for w in sentence_results.keys():\n",
    "        #    print(sentence_results[w],\"\\n \\n \\n\")\n",
    "        for i in range(1,window_max_d + 1): #take all the data related to the matches greater than a threshold for a given sentence\n",
    "            values, refs, matching_words, start_word_indexes = take_elements_th(el, i, new_th)\n",
    "            sentence_results[f\"window {i}\"] = {'matching_words':[], 'refs':[], 'values':[]}\n",
    "            sentence_results[f\"window {i}\"]['values'] = values\n",
    "            sentence_results[f\"window {i}\"]['matching_words'] = matching_words\n",
    "            sentence_results[f\"window {i}\"]['refs'] = refs\n",
    "            sentence_results[f\"window {i}\"]['start_word_indexes'] = start_word_indexes\n",
    "            last_match_indexes[f\"window {i}\"] = 0 #this dict will contain, for each window_dim, the index of the first element from which we will start the search in that window\n",
    "                                                  #every time we use data, we update this indexes to avoid searching data that is no more useful\n",
    "\n",
    "        topics_to_avoid = {'topic & dim': []}\n",
    "\n",
    "        counter = 0\n",
    "        '''\n",
    "        print(sentence,\"\\n \\n \\n\")\n",
    "        for w in sentence_results.keys():\n",
    "            print(sentence_results[w],\"\\n \\n \\n\")\n",
    "        '''\n",
    "        while True and counter<10:\n",
    "            #print(\"counter:\",counter)\n",
    "            #print(\"topics_before:\",topics_to_avoid)\n",
    "            #print(\"last match idx before:\",last_match_indexes)\n",
    "            result_dict = find_best_topic(sentence_results, window_max_d, last_match_indexes, last_processed_word_idx, sentence, topics_to_avoid)\n",
    "            #print(\"last_idx:\",result_dict['old_last_word_idx'],'new_idx:',result_dict['new_last_word_idx'], result_dict['topic'])\n",
    "            #print(last_processed_word_idx)\n",
    "            #print(result_dict)\n",
    "            if result_dict['new_last_word_idx'] != result_dict['old_last_word_idx']: #then we found a match that respects the policy starting from a word\n",
    "                #print(result_dict, \"\\n \\n \\n\")\n",
    "                #print(is_coherent(n_tries = 1, th = max_th_drop, s_res = sentence_results, t_res = result_dict, sentence = sentence, max_dim = window_max_d))\n",
    "                if is_coherent(n_tries = 1, th = max_th_drop, s_res = sentence_results, t_res = result_dict, sentence = sentence, max_dim = window_max_d):\n",
    "                    #print(\"OK:\",result_dict,\"\\n \\n \\n\")\n",
    "                    #print(\"LAST WORD IDX\",last_processed_word_idx)\n",
    "                    check = is_real_best(s_res = sentence_results, t_res = result_dict, sentence = sentence, max_dim = window_max_d)\n",
    "                    if check[\"is_best\"]: #then the current topic is for real the best one\n",
    "                        sentence_labels['topics&ref'].append((result_dict['topic'],result_dict['reference']))\n",
    "                        sentence_labels['matching_words'].append(result_dict['matching_words'])\n",
    "                        sentence_labels['word_indexes'].append(result_dict['matching_words_indexes'])\n",
    "                        sentence_labels['values'].append(result_dict['best_match'])\n",
    "                        last_processed_word_idx = result_dict['new_last_word_idx']\n",
    "                        last_match_indexes = result_dict['new_match_idx'].copy()\n",
    "                    else:\n",
    "                        new_topic = check[\"topic_result\"]\n",
    "                        sentence_labels['topics&ref'].append((new_topic['topic'],new_topic['reference']))\n",
    "                        sentence_labels['matching_words'].append(new_topic['matching_words'])\n",
    "                        sentence_labels['word_indexes'].append(new_topic['matching_words_indexes'])\n",
    "                        sentence_labels['values'].append(new_topic['best_match'])\n",
    "                        sentence_labels['skipped_words'].append(check['skipped_words']) #TODO manage the skipped words\n",
    "                        last_processed_word_idx = new_topic['new_last_word_idx']\n",
    "                        last_match_indexes = new_topic['new_match_idx'].copy()\n",
    "                        #print(\"HERERHRHERHE\"\n",
    "                    topics_to_avoid['topic & dim'] = [] #in botch cases we found the best result\n",
    "                    #print(\"LAST WORD IDX\",last_processed_word_idx)\n",
    "                else:\n",
    "                    #print(\"PROBLEM:\",result_dict,\"\\n \")\n",
    "                    #print(\"last match idx after:\",last_match_indexes)\n",
    "                    topics_to_avoid['topic & dim'].append((result_dict['topic'],result_dict['best_window']))\n",
    "                    #print(\"NEW TOPICS TO AVOID:\",topics_to_avoid, \"\\n \\n \\n\")\n",
    "                    #print(\"topics_to_append:\",result_dict['topic'])\n",
    "                    #print(\"topics_before:\",topics_to_avoid)\n",
    "\n",
    "                    #print(\"topics_after:\",topics_to_avoid)\n",
    "\n",
    "            #in this case we didn't find any match, go to the next word\n",
    "            else:\n",
    "                topics_to_avoid['topic & dim'] = []\n",
    "                last_processed_word_idx = last_processed_word_idx + 1\n",
    "\n",
    "            if result_dict['end'] == True: #we reached the end\n",
    "                #print(\"ending...\")\n",
    "                break\n",
    "            counter = counter + 1\n",
    "        end = time.perf_counter()\n",
    "        processing_t_2 = end-start\n",
    "        sentence_labels['processing_t_1'] = el[\"processing_time\"]\n",
    "        sentence_labels['processing_t_2'] = round(processing_t_2,4)\n",
    "        print(\"processing time t1\",sentence_labels['processing_t_1'],'processing time t2',processing_t_2, \"SENTENCE_IDNEX\",ind)\n",
    "\n",
    "        all_labels.append(sentence_labels)\n",
    "\n",
    "    return all_labels\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "#filename_old = 'all_similarities_Alice_base_old.pkl' #'all_similarities_standard_base_old.pkl' #\"all_processed_sentences_best.pkl\"\n",
    "#filename = 'all_similarities_Alice_random_base_old.pkl' #'all_similarities_standard_base_old.pkl' #\"all_processed_sentences_best.pkl\"\n",
    "#filename = \"new_all_processed_sentences_best.pkl\"\n",
    "#filename = 'all_similarities_600.pkl'\n",
    "name = 'Antonio_random' # Alice Carmine_nuove Carmine_nuove_random Antonio Antonio_random Lucrezia Lucrezia_random\n",
    "th = [0.3, 0.5, 0.6, 0.9]\n",
    "th_string = [\"0_3\", \"0_5\", \"0_6\", \"0_9\"]\n",
    "models = [\"base\",\"large\"]\n",
    "types = [\"\",\"old\"]\n",
    "for i,t in enumerate(th):\n",
    "    for model in models:\n",
    "        for type in types:\n",
    "            if type == \"\":\n",
    "                filename = f'all_similarities_{name}_{model}.pkl'\n",
    "            else:\n",
    "                filename = f'all_similarities_{name}_{model}_{type}.pkl'\n",
    "            results = pickle.load(open(os.path.join(path,filename),'rb'))\n",
    "            labels = process_results_best_policy(results, all_refs, new_th=t, type = 'sentences')\n",
    "            if type == \"\":\n",
    "                filename_labels = f\"results_moving_windows_{name}_{model}_{th_string[i]}.pkl\"\n",
    "            else:\n",
    "                filename_labels = f\"results_moving_windows_{name}_{model}_{type}_{th_string[i]}.pkl\"\n",
    "            print(\"writing to file...\")\n",
    "            pickle.dump(labels  , open(os.path.join(path,filename_labels),'wb'))\n",
    "'''\n",
    "if(os.path.exists(os.path.join(path,filename))):    #load data already processed\n",
    "    results = pickle.load(open(os.path.join(path,filename),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path\")\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#filename_labels = \"results_moving_window_policy.pkl\"\n",
    "\n",
    "\n",
    "#for el in labels:\n",
    "#    print(el,\"\\n \\n \\n\")\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### RANDOM LABELING\n",
    "\n",
    "def process_results_random(new_th=0.2):\n",
    "\n",
    "    def gen_avg(expected_avg=5, n=10, min=1, max=10): #generates a sequence of n random numbers that have minimum min, maximum max, and average expected_avg\n",
    "        values = [random.randint(min, max)  for x in range(n)]\n",
    "        average = sum(values)/n\n",
    "        new_values = [round(x - average + expected_avg) for x in values]\n",
    "        #new_average = sum(new_values)/n\n",
    "        return new_values\n",
    "\n",
    "    def count_gt_instances(): #counts the labelled words in the corpus, all the words in topic, all the labels and for each label counts how many times they appear and how many words they label\n",
    "        topic_count =  {topic_name: {'topic_instances':0, 'topic_words':0} for topic_name in topic_names}\n",
    "        total_words = 0 #total words in corpus\n",
    "        total_labels = 0 #total label instances\n",
    "        total_labelled_words = 0 #counts how many words were labelled\n",
    "        for gt_result in gt_labels:\n",
    "            sentence = gt_result['sentence']\n",
    "            total_words += len(sentence.split())\n",
    "            for gt_label in gt_result['labels,word_indexes']:\n",
    "                gt_topic = topic_gesture_description[[el for el in gt_label[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "                topic_count[gt_topic]['topic_instances'] += 1\n",
    "                topic_count[gt_topic]['topic_words'] += len(gt_label[1])\n",
    "                total_labels += 1\n",
    "                total_labelled_words += len(gt_label[1])\n",
    "        print(\"TOPIC COUNT:\",topic_count)\n",
    "        print(\"total_words\",total_words)\n",
    "        print(\"total_labels\",total_labels)\n",
    "        print(\"total_labelled_words\",total_labelled_words)\n",
    "\n",
    "        return topic_count, total_words, total_labels, total_labelled_words\n",
    "\n",
    "\n",
    "    def label_probabilities(topic_count, total_words, total_labels, total_labelled_words): #computes the probabilities for each topic and assigns to each word of corpus a topic\n",
    "                                                                                           #TODO it does not consider full stops when it labels so it may happen that the window falls in the middle of two sentences.\n",
    "        probs_dictionary = {topic_name: {'average_window_size':0, 'instance_probability':0} for topic_name in topic_names}\n",
    "        probs_instance_list = [] #compute the instance probability depending on how many times a topic appears in a corpus\n",
    "        topic_names_list= [] #only for debugging, to be sure that topic probabilities appear in the correct order\n",
    "        probs_window = {} #compute the window probability depending on the average_window_size\n",
    "\n",
    "\n",
    "        for topic_name in topic_names: #compute probabilities related to each topic\n",
    "            if topic_count[topic_name]['topic_instances'] != 0:\n",
    "                probs_dictionary[topic_name]['average_window_size'] = topic_count[topic_name]['topic_words'] / topic_count[topic_name]['topic_instances']\n",
    "                probs_dictionary[topic_name]['instance_probability'] = topic_count[topic_name]['topic_instances'] / total_labels #prob to have a label of some topic among all the labels\n",
    "                topic_names_list.append(topic_name)\n",
    "                probs_instance_list.append(probs_dictionary[topic_name]['instance_probability'])\n",
    "                probs_window[topic_name] = (gen_avg(expected_avg=probs_dictionary[topic_name]['average_window_size'],\n",
    "                                                n=total_labels, min = 1, max = window_max_dim))\n",
    "\n",
    "        labels_list = np.random.choice(topic_names_list,total_labels,p=probs_instance_list) #the list that shows the order on which the labels will appear accordingly to instance probs\n",
    "        labelled_words = []\n",
    "        prob_to_label_word = total_labels / total_words\n",
    "        i = 0\n",
    "        label_index = 0 #to select a label in labels_list\n",
    "        label_indexes = {topic_name: 0 for topic_name in topic_names} #it takes trace of the chosen window for each topic\n",
    "\n",
    "        while i < total_words:\n",
    "            is_labelled = random.random() < prob_to_label_word\n",
    "            if is_labelled and label_index < len(labels_list):\n",
    "                topic = labels_list[label_index]\n",
    "                topic_window = probs_window[topic][label_indexes[topic]] #choose the window size for the topic\n",
    "                for j in range(topic_window):\n",
    "                    labelled_words.append(topic)\n",
    "                label_index += 1\n",
    "                #print(\"label_index\", label_index, \"Word\",i,\"total_words\", total_words)\n",
    "                label_indexes[topic] += 1\n",
    "                i += topic_window\n",
    "            else:\n",
    "                labelled_words.append('NONE')\n",
    "                i += 1\n",
    "        return probs_dictionary, labelled_words\n",
    "\n",
    "\n",
    "    def label_sentences(labelled_words):\n",
    "\n",
    "        all_labelled_sentences = []\n",
    "        label_index = 0\n",
    "        for k, gt_result in enumerate(gt_labels):\n",
    "            sentence_labels = {'sentence':'', 'topics&ref':[],  'matching_words':[], 'values':[],\n",
    "                               'sentence_index':k, 'word_indexes':[],'processing_time':[]} #it contains a sentence labelled\n",
    "            sentence = gt_result['sentence']\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            sentence_labels['sentence'] = sentence\n",
    "            sentence_splits = sentence.split()\n",
    "            final_word_index = len(sentence_splits) - 1 #index of the last word (starting from 0)\n",
    "            current_word_index = 0\n",
    "\n",
    "            while current_word_index <= final_word_index:\n",
    "                chosen_topic = labelled_words[label_index]\n",
    "                if chosen_topic == \"NONE\":\n",
    "                    current_word_index += 1\n",
    "                    label_index += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    tmp_topic = chosen_topic\n",
    "                    tmp_matching_words = []\n",
    "                    tmp_word_indexes = []\n",
    "                    #print(\"Chosen topic:\",chosen_topic)\n",
    "                    while tmp_topic == chosen_topic:\n",
    "                        tmp_matching_words.append(sentence_splits[current_word_index])\n",
    "                        tmp_word_indexes.append(current_word_index)\n",
    "                        current_word_index += 1\n",
    "                        label_index += 1\n",
    "                        chosen_topic = labelled_words[label_index]\n",
    "                        if current_word_index > final_word_index:\n",
    "                            break\n",
    "                    ref_index = random.randint(0, len(all_refs[tmp_topic])-1) #choose a reference randomly for the given topic\n",
    "                    #print(ref_index, len(all_refs[tmp_topic]), tmp_topic)\n",
    "                    sentence_labels['matching_words'].append(' '.join(tmp_matching_words))\n",
    "                    sentence_labels['values'].append(random.randint(new_th*10, 10)/10) #generage a random value between new_th and 1\n",
    "                    sentence_labels['topics&ref'].append((tmp_topic,all_refs[tmp_topic][ref_index]))\n",
    "                    sentence_labels['word_indexes'].append(tmp_word_indexes)\n",
    "            end = time.perf_counter()\n",
    "            processing_time = end-start\n",
    "            sentence_labels['proceessing_time'] = round(processing_time,4)\n",
    "            print(\"processing time\",sentence_labels[\"proceessing_time\"],\"SENTENCE_INDEX\",k)\n",
    "            all_labelled_sentences.append(sentence_labels)\n",
    "\n",
    "        return all_labelled_sentences\n",
    "\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    window_max_dim = 10\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    topic_count, total_words, total_labels, total_labelled_words = count_gt_instances()\n",
    "    #print(total_labels, total_labelled_words, total_words)\n",
    "    topic_probs, labels = label_probabilities(topic_count, total_words, total_labels, total_labelled_words)\n",
    "    labelled_sentences = label_sentences(labels)\n",
    "\n",
    "    return labelled_sentences\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "filename_labelled_sentences = \"labelled_sentences_final_old.pkl\" #it contains the ground truth labels\n",
    "#filename = 'all_similarities.pkl'\n",
    "filename_random = \"results_random_final_0_9.pkl\"\n",
    "\n",
    "gt_labels = []\n",
    "if(os.path.exists(os.path.join(path,filename_labelled_sentences))):    #load data already processed\n",
    "    all_real_labels = pickle.load(open(os.path.join(path,filename_labelled_sentences),'rb'))\n",
    "    for label in all_real_labels:\n",
    "        gt_labels.append(label)\n",
    "else:\n",
    "    print(\"invalid real labels path\")\n",
    "\n",
    "random_labels = process_results_random(new_th=0.9)\n",
    "'''\n",
    "if(os.path.exists(os.path.join(path,filename_random))):    #load data already processed\n",
    "    print(\"File already exists...\")\n",
    "    random_labels = []\n",
    "    all_random_labels = pickle.load(open(os.path.join(path,filename_random),'rb'))\n",
    "    for label in all_random_labels:\n",
    "        print(label['sentence'])\n",
    "        random_labels.append(label)\n",
    "    #print(random_labels[2])\n",
    "    print(len(random_labels))\n",
    "else:\n",
    "    random_labels = process_results_random(new_th=0.9)\n",
    "    print(random_labels)\n",
    "    print(\"Saving file...\")\n",
    "    pickle.dump(random_labels , open(os.path.join(path,filename_random),'wb'))\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### HALF CORPUS LABELLING\n",
    "#TODO It is still not completed, it is not necessary. This function gives 50% probability to each word to have a label with a given win_size\n",
    "\n",
    "def process_results_half_corpus(sentences = all_sentences, new_th=0.2):\n",
    "\n",
    "    def count_gt_instances(): #counts the labelled words in the corpus, all the words in topic, all the labels and for each label counts how many times they appear and how many words they label\n",
    "        topic_count =  {topic_name: {'topic_instances':0, 'topic_words':0} for topic_name in topic_names}\n",
    "        total_words = 0 #total words in corpus\n",
    "        total_labels = 0 #total label instances\n",
    "        total_labelled_words = 0 #counts how many words were labelled\n",
    "        for gt_result in gt_labels:\n",
    "            sentence = gt_result['sentence']\n",
    "            total_words += len(sentence.split())\n",
    "            for gt_label in gt_result['labels,word_indexes']:\n",
    "                gt_topic = topic_gesture_description[[el for el in gt_label[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "                topic_count[gt_topic]['topic_instances'] += 1\n",
    "                topic_count[gt_topic]['topic_words'] += len(gt_label[1])\n",
    "                total_labels += 1\n",
    "                total_labelled_words += len(gt_label[1])\n",
    "\n",
    "        return topic_count, total_words, total_labels, total_labelled_words\n",
    "\n",
    "    def label_sentences(topic_count):\n",
    "        topics_average_window = {topic_name: 0 for topic_name in topic_names}\n",
    "        for topic_name in topic_names: #compute probabilities related to each topic\n",
    "            if topic_count[topic_name]['topic_instances'] != 0:\n",
    "                topics_average_window[topic_name] = round(topic_count[topic_name]['topic_words'] / topic_count[topic_name]['topic_instances'])\n",
    "\n",
    "        all_labelled_sentences = []\n",
    "        for k, gt_result in enumerate(gt_labels):\n",
    "            sentence_labels = {'sentence':'', 'topics&ref':[],  'matching_words':[], 'values':[],\n",
    "                               'sentence_index':k, 'word_indexes':[]} #it contains a sentence labelled\n",
    "            sentence = gt_result['sentence']\n",
    "            sentence_labels['sentence'] = sentence\n",
    "            sentence_splits = sentence.split()\n",
    "            for topic in topic_names:\n",
    "\n",
    "\n",
    "\n",
    "        all_labelled_sentences = []\n",
    "        label_index = 0\n",
    "\n",
    "\n",
    "\n",
    "            sentence_labels['sentence'] = sentence\n",
    "            sentence_splits = sentence.split()\n",
    "            final_word_index = len(sentence_splits) - 1 #index of the last word (starting from 0)\n",
    "            current_word_index = 0\n",
    "\n",
    "            while current_word_index <= final_word_index:\n",
    "                chosen_topic = labelled_words[label_index]\n",
    "                if chosen_topic == \"NONE\":\n",
    "                    current_word_index += 1\n",
    "                    label_index += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    tmp_topic = chosen_topic\n",
    "                    tmp_matching_words = []\n",
    "                    tmp_word_indexes = []\n",
    "                    #print(\"Chosen topic:\",chosen_topic)\n",
    "                    while tmp_topic == chosen_topic:\n",
    "                        tmp_matching_words.append(sentence_splits[current_word_index])\n",
    "                        tmp_word_indexes.append(current_word_index)\n",
    "                        current_word_index += 1\n",
    "                        label_index += 1\n",
    "                        chosen_topic = labelled_words[label_index]\n",
    "                        if current_word_index > final_word_index:\n",
    "                            break\n",
    "                    ref_index = random.randint(1, len(all_refs[tmp_topic])-1) #choose a reference randomly for the given topic\n",
    "                    sentence_labels['matching_words'].append(' '.join(tmp_matching_words))\n",
    "                    sentence_labels['values'].append(random.randint(new_th*10, 10)/10) #generage a random value between new_th and 1\n",
    "                    sentence_labels['topics&ref'].append((tmp_topic,all_refs[tmp_topic][ref_index]))\n",
    "                    sentence_labels['word_indexes'].append(tmp_word_indexes)\n",
    "            all_labelled_sentences.append(sentence_labels)\n",
    "\n",
    "        return all_labelled_sentences\n",
    "\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    window_max_dim = 10\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    topic_count, total_words, total_labels, total_labelled_words = count_gt_instances()\n",
    "    #print(total_labels, total_labelled_words, total_words)\n",
    "    topic_probs, labels = label_probabilities(topic_count, total_words, total_labels, total_labelled_words)\n",
    "    labelled_sentences = label_sentences(labels)\n",
    "\n",
    "    return labelled_sentences\n",
    "\n",
    "\n",
    "filename = \"sentences.pkl\"\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "all_sentences = pickle.load(open(os.path.join(path,filename),'rb'))\n",
    "#print(all_sentences)\n",
    "'''\n",
    "conv_subs=[]\n",
    "for key in list(all_sentences.keys()):  #take sentences\n",
    "    speaker = (key,)\n",
    "    tuples = [speaker + element for element in all_sentences[key]]\n",
    "    conv_subs.append(tuples)\n",
    "all_sentences = conv_subs[1][:100] + conv_subs[0][:100] #process first 200 sentences\n",
    "\n",
    "print(all_sentences)\n",
    "'''\n",
    "\n",
    "gt_labels = []\n",
    "if(os.path.exists(os.path.join(path,filename_labelled_sentences))):    #load data already processed\n",
    "    all_real_labels = pickle.load(open(os.path.join(path,filename_labelled_sentences),'rb'))\n",
    "else:\n",
    "    print(\"invalid real labels path\")\n",
    "\n",
    "filename_half_corpus = \"processed_sentences_half_corpus.pkl\"\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_half_corpus))):    #load data already processed\n",
    "    print(\"File already exists...\")\n",
    "    half_corpus = pickle.load(open(os.path.join(path,filename_random),'rb'))\n",
    "    print(half_corpus[198])\n",
    "else:\n",
    "    half_corpus_labels = process_results_half_corpus(sentences = all_sentences, new_th=0.2)\n",
    "    print(half_corpus_labels[197])\n",
    "    print(\"Saving file...\")\n",
    "    pickle.dump(half_corpus_labels , open(os.path.join(path,filename_half_corpus),'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### RANDOMIZE SENTENCES ORDER FOR LABELLING\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "filename_sentences = \"sentences_gpt_300_old.pkl\"\n",
    "filename_sentences_randomized = \"sentences_gpt_randomized_old.pkl\"\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_sentences))):    #load data already processed\n",
    "    all_sentences = pickle.load(open(os.path.join(path,filename_sentences),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path\")\n",
    "\n",
    "n_sentences = len(all_sentences)\n",
    "print(all_sentences)\n",
    "#print([i for i in range(n_sentences)])\n",
    "n_shuffles = 5 #how many times we want to shuffle the whole set of sentences (if sentences are 500, then we will have a set of 500 * 3 sentences randomly ordered)\n",
    "new_sentences_list = []\n",
    "for i in range(n_shuffles):\n",
    "    indexes = [i for i in range(n_sentences)]\n",
    "    shuffled_indexes = random.shuffle(indexes)\n",
    "    new_sentences_list.extend([all_sentences[i] for i in indexes])\n",
    "pickle.dump(new_sentences_list,  open(os.path.join(path,filename_sentences_randomized),'wb'))\n",
    "print(len(new_sentences_list))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "root = tk.Tk()\n",
    "root.mainloop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import os\n",
    "import pickle\n",
    "import ctypes\n",
    "import textwrap\n",
    "from tkinter.font import Font\n",
    "import math\n",
    "#import screeninfo\n",
    "\n",
    "\n",
    "class WordLabelingGUI:\n",
    "    def __init__(self, sentence, labels, path, sentence_index, local_path):\n",
    "\n",
    "        self.path = path\n",
    "        self.local_path = local_path\n",
    "        self.sentence_index = sentence_index\n",
    "        self.sentence = sentence\n",
    "        self.labels = labels\n",
    "        self.completed_labels = []\n",
    "        self.completed_labels_local = []\n",
    "        self.word_buttons = []\n",
    "        self.label_buttons = []\n",
    "        self.n_words = len(self.sentence.split())\n",
    "        self.n_labels = len(labels)\n",
    "        self.n_words_inline = 10\n",
    "        self.n_labels_inline = 7\n",
    "        self.height_increment = 60\n",
    "        self.height_increment_rows = 30\n",
    "        self.actual_height = self.height_increment_rows\n",
    "        n_word_rows = math.ceil(self.n_words/self.n_words_inline)\n",
    "        n_label_rows = math.ceil(self.n_labels/self.n_labels_inline)\n",
    "\n",
    "        #self.word_button_spaces_2 = [1/self.n_words_inline if i < n_word_rows-1 else 1/(self.n_labels - self.n_labels_inline * i) for i in range(n_word_rows)]\n",
    "        self.root = tk.Tk()\n",
    "        #tmp_objective_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), fg = \"black\", text=\"Sentence that you have to label to label:  \" )\n",
    "        #objective_width = tmp_objective_start.winfo_reqwidth()\n",
    "        #tmp_objective_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), fg = \"darkblue\", text = sentence, wraplength = self.window_width-5-objective_width)\n",
    "        #tmp_objective_height = tmp_objective_end.winfo_reqheight()\n",
    "        self.window_height = int((n_word_rows + n_label_rows + 2.5) * self.height_increment_rows + 8.5 * self.height_increment)\n",
    "        self.window_width = int(self.window_height * 1.5)\n",
    "        self.word_button_spaces = [self.window_width / (self.n_words_inline + 1) if i < n_word_rows-1 else\n",
    "                                   self.window_width / ((self.n_words - self.n_words_inline * i) + 1)  for i in range(n_word_rows)]\n",
    "        self.label_button_spaces = [self.window_width / (self.n_labels_inline + 1) if i < n_label_rows-1 else\n",
    "                                   self.window_width / ((self.n_labels - self.n_labels_inline * i) + 1)  for i in range(n_label_rows)]\n",
    "        #scaling_factor = self.root.tk.call(\"tk\", \"scaling\")\n",
    "        #self.root.tk.call(\"tk\", \"scaling\", scaling_factor)\n",
    "        self.root.resizable(False, False)\n",
    "        scaling = self.root.tk.call('tk', 'scaling')\n",
    "        #screen_width = int(self.root.winfo_screenwidth() * scaling)\n",
    "        #screen_height = int(self.root.winfo_screenheight() * scaling)\n",
    "        #print(screen_height,screen_width)\n",
    "        screen_width = 1920\n",
    "        screen_height = 1080\n",
    "        #screen_info = self.root.winfo_screenmmwidth(screen=1), self.root.winfo_screenmmheight(screen=1)\n",
    "        self.root.update_idletasks()\n",
    "        #screen_width  = self.root.winfo_screenwidth()\n",
    "        #screen_height = self.root.winfo_screenheight()\n",
    "        #print(screen_width, screen_height)\n",
    "        self.center_x = int(screen_width/2) #- self.window_width/2)\n",
    "        self.center_y = int(screen_height/2)# - self.window_height/2)\n",
    "        if self.window_height < 400:\n",
    "            self.window_height = 400\n",
    "            self.window_width = int(self.window_height * 1.5)\n",
    "        self.root.geometry(f'{self.window_width}x{self.window_height}+{self.center_x}+{self.center_y-35}')\n",
    "        #print(self.root.winfo_width())\n",
    "        self.word_label = tk.Label(self.root, font=(\"Helvetica\",16,\"bold\"), text=\"Select words that you want to label:\")\n",
    "        self.word_label.place(relwidth=1, y = self.actual_height)\n",
    "        self.all_sentence_labels = []\n",
    "        self.final_result = {\"sentence\":self.sentence, \"labels,word_indexes\":[], \"sentence_index\":self.sentence_index}\n",
    "        null_image = tk.PhotoImage(width=0, height=0)\n",
    "        self.words_font = Font(family=\"Helvetica\", size=13, weight=\"bold\")\n",
    "        self.height_exceeded = False\n",
    "\n",
    "        y = self.actual_height\n",
    "        index = -1\n",
    "        #self.textbox_words_height = y + (n_word_rows+2) * self.height_increment_rows\n",
    "        #self.textbox_label_height = int(self.textbox_words_height + 1.5 * self.height_increment_rows)\n",
    "\n",
    "        for i,word in enumerate(self.sentence.split()):\n",
    "            button_name = (word,i)\n",
    "            button = tk.Button(self.root, text=word, command=lambda b=button_name: self.choose_word(b), font=(\"Arial\", 10, \"bold\"), fg = 'black', borderwidth=3)\n",
    "            button_copy = tk.Button(self.root, text=word, image=null_image, command=lambda b=button_name: self.choose_word(b), compound=\"center\", font=(\"Arial\", 10, \"bold\"), fg = 'black', borderwidth=3)\n",
    "            #button_create = tk.Tk()\n",
    "            #button_create.withdraw()\n",
    "            button_width = button_copy.winfo_reqwidth()\n",
    "            #button_create.destroy()\n",
    "            #print(\"button_width:\",button_width)\n",
    "            if i % self.n_words_inline == 0:\n",
    "                if i == 0:\n",
    "                    self.actual_height += self.height_increment_rows/2\n",
    "                index +=  1\n",
    "                self.actual_height += self.height_increment_rows\n",
    "                y = self.actual_height\n",
    "            button.place(x = self.word_button_spaces[index] * (i+1-index*self.n_words_inline) - button_width/2, y=y)\n",
    "            self.word_buttons.append(button)\n",
    "            #self.root.update()\n",
    "            #self.start_word_button += button.winfo_x()\n",
    "            #print(button.winfo_y(),button.winfo_width())\n",
    "\n",
    "        self.actual_height += self.height_increment\n",
    "        self.label_label = tk.Label(self.root, font=(\"Helvetica\",16,\"bold\"), text=\"Select a label:\")\n",
    "        self.label_label.place(relwidth=1, y = self.actual_height)\n",
    "        #self.actual_height += self.height_increment\n",
    "        y = self.actual_height\n",
    "        index = -1\n",
    "\n",
    "        for i,label in enumerate(self.labels):\n",
    "            button = tk.Button(self.root, text=label, command=lambda l=label: self.choose_label(l), font=(\"Arial\", 10, \"bold\"), fg = 'blue', borderwidth=3)\n",
    "            button_copy = tk.Button(self.root, text=label, image=null_image, command=lambda l=label: self.choose_label(l), compound=\"center\", font=(\"Arial\", 10,\"bold\"), fg = 'blue', borderwidth=3)\n",
    "            #button_create = tk.Tk()\n",
    "            #button_create.withdraw()\n",
    "            button_width = button_copy.winfo_reqwidth()\n",
    "            #button_create.destroy()\n",
    "            #print(\"button_width:\",button_width)\n",
    "            if i % self.n_labels_inline == 0:\n",
    "                if i == 0:\n",
    "                    self.actual_height += self.height_increment_rows/2\n",
    "                self.actual_height += self.height_increment_rows\n",
    "                index +=  1\n",
    "                y = self.actual_height\n",
    "            button.place(x = self.label_button_spaces[index] * (i+1-index*self.n_labels_inline) - button_width/2, y=y)\n",
    "            #self.start_label_button += 100\n",
    "            self.label_buttons.append(button)\n",
    "            #button.pack(side='left',ipadx=30,ipady=30)\n",
    "\n",
    "\n",
    "\n",
    "        self.actual_height += self.height_increment\n",
    "        final_button_space = self.window_width / 5\n",
    "\n",
    "\n",
    "        self.reset_button = tk.Button(self.root, text=\"Reset word \\n selection\", command=self.reset, font=(\"Arial\", 12, \"bold\"), fg = 'red', borderwidth=3)\n",
    "        button_copy =  tk.Button(self.root, image=null_image, compound=\"center\", text=\"Reset word \\n selection\", command=self.restart, padx=20, pady=10,font=(\"Arial\", 12, \"bold\"), fg = 'red', borderwidth=3)\n",
    "        button_width = button_copy.winfo_reqwidth()\n",
    "        button_height = button_copy.winfo_reqheight()\n",
    "        self.reset_button.place(x = final_button_space - button_width/2, y = self.actual_height, width=button_width,height=button_height)\n",
    "\n",
    "\n",
    "        self.submit_button = tk.Button(self.root, text=\"Submit\", command=self.submit_labels, font=(\"Arial\", 12, \"bold\"), fg = 'red', borderwidth=3)\n",
    "        self.submit_button.place(x = final_button_space * 2 - button_copy.winfo_reqwidth()/2, y = self.actual_height, width=button_width,height=button_height)\n",
    "\n",
    "        self.restart_button = tk.Button(self.root, text=\"Restart\", command=self.restart, font=(\"Arial\", 12, \"bold\"), fg = 'red', borderwidth=3)\n",
    "        self.restart_button.place(x = final_button_space * 3 - button_width/2, y = self.actual_height, width=button_width,height=button_height)\n",
    "\n",
    "        self.finish_button = tk.Button(self.root, text=\"Finish\", command=self.finish, font=(\"Arial\", 12, \"bold\"), fg = 'red', borderwidth=3)\n",
    "        self.finish_button.place(x = final_button_space * 4 - button_width/2, y = self.actual_height, width=button_width,height=button_height)\n",
    "\n",
    "        self.selected_words = []\n",
    "        self.selected_indexes = []\n",
    "        self.selected_label = \"\"\n",
    "\n",
    "        self.objective_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), fg = \"black\", text=\"Sentence that you have to label:  \" )\n",
    "        objective_width = self.objective_start.winfo_reqwidth()\n",
    "        self.objective_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), fg = \"darkblue\", text = sentence, wraplength = self.window_width-5-objective_width)\n",
    "        objective_height = self.objective_end.winfo_reqheight()\n",
    "        self.actual_height = self.actual_height +self.height_increment * 1.5 #self.height_increment_rows/2\n",
    "        self.textbox_words_height = int(self.actual_height + objective_height + self.height_increment_rows/2)\n",
    "        self.textbox_label_height = int(self.textbox_words_height + 1.0 * self.height_increment_rows)\n",
    "        self.textbox_final_height = int(self.textbox_label_height + 1.0 * self.height_increment_rows)\n",
    "        self.last_word_index=-1 #index used to indicate the word from which we can start the selection\n",
    "        self.word_labelled_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"Here there will be shown the selected words\")\n",
    "        self.word_labelled_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "        self.label_chosen_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"Here there will be shown the selected label\")\n",
    "        self.label_chosen_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "        self.final_labels_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"The words that you already labelled are:\")\n",
    "        self.final_width = self.final_labels_start.winfo_reqwidth()\n",
    "        self.final_labels_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "        self.objective_start.place(x=5, y = self.actual_height)\n",
    "        self.objective_end.place(x=6+objective_width, y=self.actual_height)\n",
    "        self.word_labelled_start.place(x=5, y = self.textbox_words_height)\n",
    "        self.label_chosen_start.place(x=5, y = self.textbox_label_height)\n",
    "        self.final_labels_start.place(x=5, y = self.textbox_final_height)\n",
    "\n",
    "\n",
    "    def choose_word(self, button_name):\n",
    "        #self.root.update()\n",
    "        button_index = button_name[1]\n",
    "        word = button_name[0]\n",
    "        #print(self.selected_words, self.selected_words[-1][1])\n",
    "        if len(self.selected_indexes)>0:\n",
    "            if button_index - self.selected_indexes[-1] > 1:    #if the word that the user choses in not subsequent to the one that he choses before,\n",
    "                                                                 # put all buttons in the normal state and clear the selected_buttons list\n",
    "                for button in self.word_buttons[self.last_word_index+1:]:\n",
    "                    button.config(state='normal')\n",
    "                    self.word_labelled_start.destroy()\n",
    "                    self.word_labelled_end.destroy()\n",
    "                    self.word_labelled_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"You didn't select successive words! Please repeat the selection...\")\n",
    "                    self.word_labelled_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "                    self.word_labelled_start.place(x=5, y = self.textbox_words_height)\n",
    "                self.selected_words=[]\n",
    "                self.selected_indexes=[]\n",
    "                return\n",
    "            if len(self.selected_indexes) > 10:\n",
    "                for button in self.word_buttons[self.last_word_index+1:]:\n",
    "                    button.config(state='normal')\n",
    "                    self.word_labelled_start.destroy()\n",
    "                    self.word_labelled_end.destroy()\n",
    "                    self.word_labelled_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"You can't label more than 10 words at a time! Please repeat the selection...\")\n",
    "                    self.word_labelled_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "                    self.word_labelled_start.place(x=5, y = self.textbox_words_height)\n",
    "                self.selected_words=[]\n",
    "                self.selected_indexes=[]\n",
    "                return\n",
    "\n",
    "        self.word_labelled_start.destroy()\n",
    "        self.word_labelled_end.destroy()\n",
    "        self.selected_words.append(word)\n",
    "        self.selected_indexes.append(button_index)\n",
    "        self.word_labelled_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"Selected words:   \")\n",
    "        self.word_labelled_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), fg=\"darkblue\", text=\" \".join(self.selected_words))\n",
    "        self.word_labelled_start.place(x=5, y = self.textbox_words_height)\n",
    "        word_labelled_width = self.word_labelled_start.winfo_reqwidth()\n",
    "        self.word_labelled_end.place(x=6+word_labelled_width, y = self.textbox_words_height)\n",
    "        for i,button in enumerate(self.word_buttons):\n",
    "            if i<=button_index:\n",
    "            #if button['text'] in self.selected_words:\n",
    "                button.config(state='disabled')\n",
    "\n",
    "    def reset(self):\n",
    "        for button in self.word_buttons[self.last_word_index+1:]:\n",
    "            button.config(state='normal')\n",
    "            self.word_labelled_start.destroy()\n",
    "            self.word_labelled_end.destroy()\n",
    "            self.word_labelled_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"Resetting word selection...\")\n",
    "            self.word_labelled_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "            self.word_labelled_start.place(x=5, y = self.textbox_words_height)\n",
    "        self.selected_words=[]\n",
    "        self.selected_indexes=[]\n",
    "\n",
    "    def restart(self):\n",
    "        self.word_labelled_start.destroy()\n",
    "        self.word_labelled_end.destroy()\n",
    "        self.final_labels_start.destroy()\n",
    "        self.final_labels_end.destroy()\n",
    "        self.label_chosen_start.destroy()\n",
    "        self.label_chosen_end.destroy()\n",
    "        self.all_sentence_labels = []\n",
    "        self.final_result[\"labels,word_indexes\"] = []\n",
    "        self.selected_words=[]\n",
    "        self.selected_indexes=[]\n",
    "        self.last_word_index=-1\n",
    "        self.selected_label = \"\"\n",
    "        self.final_labels_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"All the labelled words were removed\")\n",
    "        self.final_labels_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "        self.final_labels_start.place(x=5, y = self.textbox_final_height)\n",
    "        self.word_labelled_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"Restart the selection of words\")\n",
    "        self.word_labelled_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "        self.word_labelled_start.place(x=5, y = self.textbox_words_height)\n",
    "        self.label_chosen_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"Restart the selection of the label\")\n",
    "        self.label_chosen_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "        self.label_chosen_start.place(x=5, y = self.textbox_label_height)\n",
    "        for button in self.word_buttons:\n",
    "            button.config(state='normal')\n",
    "        for button in self.label_buttons:\n",
    "            button.config(state='normal')\n",
    "\n",
    "    def finish(self):\n",
    "        if(os.path.exists(self.path)):    #load data already processed\n",
    "            self.completed_labels = pickle.load(open(self.path,'rb'))\n",
    "        if(os.path.exists(self.local_path)):    #load data already processed\n",
    "            self.completed_labels_local = pickle.load(open(self.local_path,'rb'))\n",
    "            #print(self.completed_labels)\n",
    "        self.final_result[\"labels,word_indexes\"] = self.all_sentence_labels\n",
    "        print(self.final_result)\n",
    "        self.completed_labels.append(self.final_result)\n",
    "        self.completed_labels_local.append(self.final_result)\n",
    "        #pickle.dump(self.completed_labels, open(self.path,'wb'))\n",
    "        #pickle.dump(self.completed_labels_local, open(self.local_path,'wb'))\n",
    "        self.root.destroy()\n",
    "\n",
    "\n",
    "    def choose_label(self, label):\n",
    "        self.selected_label = label\n",
    "        self.label_chosen_start.destroy()\n",
    "        self.label_chosen_end.destroy()\n",
    "        self.label_chosen_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"Label:  \")\n",
    "        label_width = self.label_chosen_start.winfo_reqwidth()\n",
    "        self.label_chosen_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), fg = \"darkblue\", text=label)\n",
    "        self.label_chosen_start.place(x=5, y = self.textbox_label_height)\n",
    "        self.label_chosen_end.place(x=6+label_width, y = self.textbox_label_height)\n",
    "        for button in self.label_buttons:\n",
    "            if button['text'] == self.selected_label:\n",
    "                button.config(state='disabled')\n",
    "            else:\n",
    "                button.config(state='normal')\n",
    "\n",
    "    def submit_labels(self):\n",
    "        if len(self.selected_words) > 0 and self.selected_label != \"\":\n",
    "            self.final_labels_start.destroy()\n",
    "            self.final_labels_end.destroy()\n",
    "            self.word_labelled_start.destroy()\n",
    "            self.word_labelled_end.destroy()\n",
    "            self.label_chosen_start.destroy()\n",
    "            self.label_chosen_end.destroy()\n",
    "            all_words = ' '.join([word for word in self.selected_words])\n",
    "            #print(all_words)\n",
    "            self.last_word_index = self.selected_indexes[-1]\n",
    "            label_dict = {all_words:self.selected_label}\n",
    "            self.all_sentence_labels.append((label_dict, self.selected_indexes))\n",
    "            #print(self.all_sentence_labels)\n",
    "            self.final_labels_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"The words that you already labelled are:\")\n",
    "            self.final_labels_start.place(x=5, y = self.textbox_final_height)\n",
    "            if self.height_exceeded == False:\n",
    "                self.tmp_sentence_labels = [el[0] for el in self.all_sentence_labels]\n",
    "            else:\n",
    "                self.tmp_sentence_labels.append(self.all_sentence_labels[-1][0])\n",
    "            self.final_labels_end = tk.Label(self.root, text=' '.join(f\"{self.tmp_sentence_labels}\"),\n",
    "                                             wraplength=self.window_width - 5 - self.final_width, justify=\"left\", fg = \"darkblue\", font=(\"Helvetica\",13,\"bold\"))\n",
    "            final_labels_height = self.final_labels_end.winfo_reqheight()\n",
    "            if self.window_height - (self.textbox_final_height + final_labels_height + self.height_increment_rows) < 0:\n",
    "                self.height_exceeded = True\n",
    "                self.tmp_sentence_labels.pop(0)\n",
    "                self.final_labels_end = tk.Label(self.root, text= \"...\" + ' '.join(f\"{self.tmp_sentence_labels}\"),\n",
    "                                             wraplength=self.window_width - 5 - self.final_width, justify=\"left\", fg = \"darkblue\", font=(\"Helvetica\",13,\"bold\"))\n",
    "            self.final_labels_end.place(x=6+self.final_width, y = self.textbox_final_height)\n",
    "            self.word_labelled_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"Here there be shown the words that you are selecting for the labelling\")\n",
    "            self.word_labelled_end = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "            self.label_chosen_start = tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"Here there be shown the label selected for the words\")\n",
    "            self.label_chosen_end= tk.Label(self.root, font=(\"Helvetica\",13,\"bold\"), text=\"\")\n",
    "            self.word_labelled_start.place(x=5, y = self.textbox_words_height)\n",
    "            self.label_chosen_start.place(x=5, y = self.textbox_label_height)\n",
    "            #label_dict = {word: self.selected_label for word in self.selected_words}\n",
    "            #print(label_dict)\n",
    "            self.selected_words = []\n",
    "            self.selected_indexes = []\n",
    "            self.selected_label = \"\"\n",
    "            for button in self.label_buttons:\n",
    "                button.config(state='normal')\n",
    "            #for button in self.word_buttons:\n",
    "            #    if button['text'] not in self.selected_words:\n",
    "            #        button.config(state='normal')\n",
    "\n",
    "def continueGUI(n_experiment):\n",
    "\n",
    "    def on_yes_click():\n",
    "        answer.set(\"Yes\")\n",
    "        root.destroy()\n",
    "\n",
    "    def on_no_click():\n",
    "        answer.set(\"No\")\n",
    "        root.destroy()\n",
    "\n",
    "    def finish_click():\n",
    "        root.destroy()\n",
    "\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(f\"SENTENCE NUMBER:  {n_experiment}  of  30\")\n",
    "    #root.geometry(\"300x100\")\n",
    "    window_width = 400\n",
    "    window_height = 100\n",
    "    screen_width = 1920\n",
    "    screen_height = 1080\n",
    "    root.update_idletasks()\n",
    "    center_x = int(2560 + screen_width/2 - window_width/2)\n",
    "    center_y = int(screen_height/2 - window_height/2)\n",
    "    root.geometry(f'{window_width}x{window_height}+{center_x}+{center_y-35}')\n",
    "\n",
    "    if n_experiment == 30:\n",
    "        question_label = tk.Label(root, text=\"You finished the experiment!\", fg = \"black\", font=(\"Helvetica\",13,\"bold\"))\n",
    "        question_label.pack(pady=10)\n",
    "        button_frame = tk.Frame(root)\n",
    "        button_frame.pack()\n",
    "        finish_button = tk.Button(button_frame, text=\"OK\", width=10, command=finish_click, font=(\"Arial\", 12, \"bold\"), fg = 'red',  borderwidth=3)\n",
    "        finish_button.pack(side=\"left\", padx=5)\n",
    "        root.mainloop()\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "    question_label = tk.Label(root, text=\"Do you want to continue?\", fg = \"black\", font=(\"Helvetica\",14,\"bold\"))\n",
    "    question_label.pack(pady=10)\n",
    "\n",
    "    button_frame = tk.Frame(root)\n",
    "    button_frame.pack()\n",
    "\n",
    "    answer = tk.StringVar()\n",
    "\n",
    "    yes_button = tk.Button(button_frame, text=\"Yes\", width=10, command=on_yes_click, font=(\"Arial\", 12, \"bold\"), fg = 'red',  borderwidth=3)\n",
    "    yes_button.pack(side=\"left\", padx=5)\n",
    "\n",
    "    no_button = tk.Button(button_frame, text=\"No\", width=10, command=on_no_click, font=(\"Arial\", 12, \"bold\"), fg = 'red',  borderwidth=3)\n",
    "    no_button.pack(side=\"left\", padx=5)\n",
    "\n",
    "    root.mainloop()\n",
    "    print(\"Answer:\", answer.get())\n",
    "    return answer.get()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "    #filename_labelled_sentences = \"labelled_sentences_final.pkl\" #sentences already labelled\n",
    "    filename_labelled_sentences = \"labelled_sentences_final.pkl\" # \"labelled_sentences_final.pkl\"\n",
    "    #filename_best = \"new_all_processed_sentences_best.pkl\"\n",
    "    filename_sentences_to_label =  \"sentences_gpt_randomized.pkl\" #\"sentences_gpt_randomized.pkl\"\n",
    "    #ctypes.windll.shcore.SetProcessDpiAwareness(2) # option for high resolution screens\n",
    "    #ctypes.windll.user32.SetProcessDPIAware()\n",
    "\n",
    "    if(os.path.exists(os.path.join(path,filename_sentences_to_label))):    #load data already processed\n",
    "        all_sentences_to_label = pickle.load(open(os.path.join(path,filename_sentences_to_label),'rb'))\n",
    "    else:\n",
    "        print(\"Invalid path\")\n",
    "\n",
    "    #n_people = 30 #number of people that will participate to the experiment\n",
    "    n_sentences = len(all_sentences_to_label)\n",
    "    n_sentences_for_each_person = math.floor(30)\n",
    "    experiment_number = 20   #####  13\n",
    "    labeller_name = \"Test\"\n",
    "    start_index = experiment_number * n_sentences_for_each_person\n",
    "    end_index = (experiment_number + 1) * n_sentences_for_each_person\n",
    "    sentences_to_label = all_sentences_to_label[start_index:end_index]\n",
    "\n",
    "    #sentences_to_label = []\n",
    "    sentences_already_labelled = []\n",
    "    if(os.path.exists(os.path.join(path,filename_labelled_sentences))):    #load data already processed\n",
    "        all_labels =  pickle.load(open(os.path.join(path,filename_labelled_sentences),'rb'))\n",
    "        #print(all_labels)\n",
    "        for label in all_labels:\n",
    "            #print(label)\n",
    "            sentences_already_labelled.append(label['sentence_index'])\n",
    "    #print(sentences_already_labelled[3:])\n",
    "    #print(all_labels)\n",
    "    #for el in sentences_to_label:\n",
    "        #print(el)\n",
    "        #print(sentence_labels.keys())\n",
    "        #sentence = el['whole sentence,speaker,turn'][0]\n",
    "        #sentences_to_label.append(sentence) ####sentence\n",
    "        #print(\"here\")\n",
    "        #speaker = el['whole sentence,speaker,turn'][1]\n",
    "        #turn = el['whole sentence,speaker,turn'][2]\n",
    "    labels = topic_gesture_description.keys()\n",
    "    #finish_flag = False\n",
    "\n",
    "    for ind,sentence in enumerate(sentences_to_label):\n",
    "        #print(sentence,ind)\n",
    "        index = n_sentences_for_each_person * experiment_number + ind\n",
    "        print(index)\n",
    "        if index in sentences_already_labelled:\n",
    "            continue\n",
    "        gui = WordLabelingGUI(sentence, labels, os.path.join(path,filename_labelled_sentences),index,\n",
    "                              os.path.join(path,labeller_name+\"_\"+filename_labelled_sentences))\n",
    "        #print(\"here\")\n",
    "        gui.root.mainloop()\n",
    "        gui_2 = continueGUI(ind + 1)\n",
    "        if gui_2 == True:\n",
    "            break\n",
    "        if gui_2 == \"Yes\":\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    print(\"Experiment finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### VISUALIZE SOME LABELS\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "filename_labelled_sentences = \"labelled_sentences_final.pkl\" #it contains the ground truth labels\n",
    "labeller_name = \"Claudio_Del_Gaizo\"\n",
    "filename_local_labelled_sentences = labeller_name + \"_\" + filename_labelled_sentences\n",
    "if(os.path.exists(os.path.join(path,filename_labelled_sentences))):    #load data already processed\n",
    "    all_real_labels = pickle.load(open(os.path.join(path,filename_labelled_sentences),'rb'))\n",
    "else:\n",
    "    print(\"invalid real labels path\")\n",
    "#for i in range(len(all_real_labels)):\n",
    "#    print(all_real_labels[i]['sentence'], \"\\n \\n\")\n",
    "#print(all_real_labels)\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_local_labelled_sentences))):    #load data already processed\n",
    "    local_real_labels = pickle.load(open(os.path.join(path,filename_local_labelled_sentences),'rb'))\n",
    "else:\n",
    "    print(\"invalid real labels path\")\n",
    "\n",
    "print(all_real_labels)\n",
    "#pickle.dump(all_real_labels[:240], open(os.path.join(path,filename_labelled_sentences),'wb'))\n",
    "#pickle.dump(all_real_labels[:420],open(os.path.join(path,filename_labelled_sentences),'wb'))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### EVALUATION\n",
    "\n",
    "import os, pickle\n",
    "\n",
    "\n",
    "def precision_recall_curve(y_true, pred_scores, thresholds):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = [1 if score >= threshold else 0 for score in pred_scores]\n",
    "\n",
    "        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=1, zero_division=0)\n",
    "        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "def mean_IOU(gt_labels, pd_labels, confidence = 0.5):\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    topics_iou = {topic_gesture_description[key][0]: {'tot_intersection':0, 'tot_union':0, 'iou':0} for key in gt_labels_names}\n",
    "    total_positive_topics = {topic_name: 0 for topic_name in topic_names}\n",
    "    #topics_gt_labels = {topic_gesture_description[key][0]: [] for key in gt_labels_names}\n",
    "    #for gt_result,pd_result in zip(gt_labels,pd_labels): #for each sentence\n",
    "    #    assert(gt_result['sentence_index']==pd_result['sentence_index'])\n",
    "    for gt_result in gt_labels:\n",
    "        break_flag = False\n",
    "        for pd_result in pd_labels:\n",
    "            if pd_result['sentence'] != gt_result['sentence']: #check that the results are referred to the same sentence before continuing\n",
    "                continue\n",
    "            else:\n",
    "                #print(pd_result['sentence'], gt_result['sentence'])\n",
    "                for topic in topic_names:\n",
    "                    all_gt_indexes = []\n",
    "                    all_pd_indexes = []\n",
    "                    for j,gt_match in enumerate(gt_result['labels,word_indexes']):\n",
    "                        gt_topic = topic_gesture_description[[el for el in gt_match[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "                        if gt_topic != topic:\n",
    "                            continue\n",
    "                        else:\n",
    "                            all_gt_indexes.append(gt_match[1])\n",
    "                    for k,pd_match in enumerate(zip(pd_result['topics&ref'],pd_result['word_indexes'])):\n",
    "                        pd_topic = pd_match[0][0]\n",
    "                        if pd_topic != topic or pd_result['values'][k] < confidence: #the result must respect the confidence threshold\n",
    "                            continue\n",
    "                        else:\n",
    "                            all_pd_indexes.append(pd_match[1])\n",
    "                    union = len(set([j for sub in all_gt_indexes + all_pd_indexes for j in sub])) #total union in the sentence\n",
    "                    alL_intersections = [set(j).intersection(k) for j in all_pd_indexes for k in all_gt_indexes if len(set(j).intersection(k))!=0] #all the possible intersections\n",
    "                    intersection = len(set([i for a in alL_intersections for i in a]))  #total intersection area in the sentence\n",
    "                    topics_iou[topic]['tot_intersection'] += intersection\n",
    "                    topics_iou[topic]['tot_union'] += union\n",
    "                    if union == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        topics_iou[topic]['iou'] = topics_iou[topic]['tot_intersection'] / topics_iou[topic]['tot_union']\n",
    "                break\n",
    "\n",
    "    total_considered_topics = 0\n",
    "    for gt_result in gt_labels: #count how many gt labels we have for each topic. Will be considered only the topics that have at least one gt label\n",
    "        for j,gt_label in enumerate(gt_result['labels,word_indexes']):\n",
    "            gt_topic = topic_gesture_description[[el for el in gt_label[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "            total_positive_topics[gt_topic] += 1\n",
    "    #for topic in topic_names:\n",
    "    #    if total_positive_topics[topic] > 0:\n",
    "    #        total_considered_topics += 1\n",
    "\n",
    "    iou_sum = 0\n",
    "    for topic in topic_names: #compute mean_iou\n",
    "        if total_positive_topics[topic] > 0:\n",
    "            total_considered_topics += 1\n",
    "            #print(\"TOPIC NAME:\", topic, \"TOPIC_IOU:\",topics_iou[topic]['iou'])\n",
    "            iou_sum += topics_iou[topic]['iou']\n",
    "    mean_IOU = iou_sum / total_considered_topics\n",
    "    #print(\"MEAN IOU:\", mean_IOU)\n",
    "\n",
    "    return topics_iou, mean_IOU\n",
    "\n",
    "\n",
    "def mean_average_precision(gt_labels, pd_labels, th_fix = 0.5):\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    total_positive_topics = {topic_name: 0 for topic_name in topic_names}\n",
    "    all_results = {topic_name: {th/20: {\"Confidences\":[],\"TP\":[], \"FP\":[], \"Acc_TP\":[], \"Acc_FP\":[], \"Precision\":[], \"Recall\":[]} for th in range(1,21,1)}\n",
    "                   for topic_name in topic_names }\n",
    "    average_precision = {topic_name:{th/20:0 for th in range(1,21,1)} for topic_name in topic_names}\n",
    "    final_average_precision = {topic_name:0 for topic_name in topic_names}\n",
    "    total_considered_topics = 0\n",
    "\n",
    "    for gt_result in gt_labels:\n",
    "        for j,gt_label in enumerate(gt_result['labels,word_indexes']):\n",
    "            gt_topic = topic_gesture_description[[el for el in gt_label[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "            total_positive_topics[gt_topic] += 1\n",
    "    for topic in topic_names:\n",
    "        if total_positive_topics[topic] > 0:\n",
    "            total_considered_topics += 1\n",
    "    #print(total_considered_topics)\n",
    "    for t in range(1,21,1): #compute all the info needed to compute average precision for all the possible thresholds of IOU\n",
    "        th = t/20\n",
    "        for topic in topic_names: #for each possible topic\n",
    "            if total_positive_topics[topic] == 0: #it does not make sense the computation of Average Precision (we can't compute the Recall)\n",
    "                continue\n",
    "            row = all_results[topic][th] #we will create rows in the results referred to the chosen topic and th. The final table will contain\n",
    "                                         #all the info needed to compute average precision for a topic with a given IOU threshold\n",
    "            acc_TP = 0\n",
    "            acc_FP = 0\n",
    "            n_comparisons = 0\n",
    "            #####for skip, (gt_result,pd_result) in enumerate(zip(gt_labels,pd_labels)): #for each sentence\n",
    "            #print(\"LEN:\", len(gt_labels))\n",
    "            for num,gt_result in enumerate(gt_labels):\n",
    "                #if t==1 and topic == 'r_greet':\n",
    "                    #print(num)\n",
    "                #break_flag = False\n",
    "                for pd_result in pd_labels:\n",
    "                    if pd_result['sentence'] != gt_result['sentence']: #check that the results are referred to the same sentence before continuing\n",
    "                        continue\n",
    "                    else:\n",
    "                        #break_flag = True\n",
    "                        #if t == 1 and topic == 'r_greet':\n",
    "                        n_comparisons += 1\n",
    "                        #print(\"prediction:\",pd_result['sentence'])\n",
    "                        #print(\"gt:\",gt_result['sentence'])\n",
    "                        #n_comparisons += 1\n",
    "                        #print(n_comparisons)\n",
    "                        ######assert(gt_result['sentence_index']==pd_result['sentence_index'])\n",
    "                        #all_gt_indexes = []\n",
    "                        #all_pd_indexes = []\n",
    "                        detected_gt = {\"IOU\":[], \"Confidence\":[], 'gt_indexes':[]}\n",
    "                        for k,pd_match in enumerate(zip(pd_result['topics&ref'],pd_result['word_indexes'])):\n",
    "                            pd_topic = pd_match[0][0]  #extract the pd_topic from the pd_label   #do the same for the predicted label\n",
    "                            if pd_topic != topic:\n",
    "                                #print(\"SKIPPING PREDICTED:\", k, pd_result)\n",
    "                                continue\n",
    "                            else: #right topic, check the matches with ground truths\n",
    "                                #print(\"NOT SKIPPED PD TOPIC\",skip,pd_result)\n",
    "                                n_intersections = 0\n",
    "                                tmp_IOU = 0\n",
    "                                tmp_conf = 0\n",
    "                                for j,gt_match in enumerate(gt_result['labels,word_indexes']):  #for each prediction, we take only the GT that has highest IOU\n",
    "                                                                                                #the other can be False Negatives or True positives\n",
    "                                    gt_topic = topic_gesture_description[[el for el in gt_match[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "                                    if gt_topic != topic: #only if gt_topic and pd_topic are the same we can do the comparison\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        intersection = len(set(pd_match[1]).intersection(gt_match[1]))\n",
    "                                        if intersection != 0: #we take append to a list that takes all the possible IOUs. Only the best one IOU is TP, for the other topics we have to check if they have predictions with the others GT\n",
    "                                            union = len(set(pd_match[1] + gt_match[1]))\n",
    "                                            IOU = intersection/union\n",
    "                                            n_intersections += 1\n",
    "                                            if IOU > tmp_IOU: #this allow to take only the best IOU, the others (with other GT) are false negatives\n",
    "                                                tmp_conf = pd_result['values'][k]\n",
    "                                                tmp_IOU = IOU\n",
    "                                                gt_index = j\n",
    "                                        else:\n",
    "                                            continue\n",
    "                                if n_intersections == 0: #then the prediction has no intersections, so we have a false positive\n",
    "                                    row[\"Confidences\"].append(pd_result['values'][k])\n",
    "                                    row[\"FP\"].append(1)\n",
    "                                    row[\"TP\"].append(0)\n",
    "                                else: #we have to do another check: for each GT, we have to check which is the predicted topic that has highest IOU\n",
    "                                    detected_gt[\"IOU\"].append(tmp_IOU) #IOU\n",
    "                                    detected_gt[\"Confidence\"].append(tmp_conf) #confidence\n",
    "                                    detected_gt[\"gt_indexes\"].append(gt_index)\n",
    "\n",
    "                        if len(detected_gt[\"IOU\"]) == 0: #then we have no TP, only FP which is already considered and appended as rows\n",
    "                            break\n",
    "                        else:\n",
    "                            unique_indexes = set(detected_gt[\"gt_indexes\"])\n",
    "                            for idx in unique_indexes:\n",
    "                                gt_idx = [i for i, d in enumerate(detected_gt[\"gt_indexes\"]) if d == idx] #extract predictions related to same GT\n",
    "                                IOUs = [detected_gt[\"IOU\"][i] for i in gt_idx]\n",
    "                                max_IOU_idx = detected_gt[\"IOU\"].index(max(IOUs))\n",
    "                                if max(IOUs) < th:\n",
    "                                    for i in gt_idx:\n",
    "                                        row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                        row[\"FP\"].append(1)\n",
    "                                        row[\"TP\"].append(0)\n",
    "                                else:\n",
    "                                    for i in gt_idx:\n",
    "                                        if i == max_IOU_idx: #True Positive\n",
    "                                            row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                            row[\"FP\"].append(0)\n",
    "                                            row[\"TP\"].append(1)\n",
    "                                        else:\n",
    "                                            row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                            row[\"FP\"].append(1)\n",
    "                                            row[\"TP\"].append(0)\n",
    "                        '''\n",
    "                        if len(detected_gt[\"IOU\"]) == 0:\n",
    "                            #continue\n",
    "                            break\n",
    "                        else:\n",
    "                            best_prediction_value = max(detected_gt[\"IOU\"])\n",
    "                            if best_prediction_value < th: #then all the predictions are false positives\n",
    "                                for i in range(len(detected_gt[\"IOU\"])):\n",
    "                                    row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                    row[\"FP\"].append(1)\n",
    "                                    row[\"TP\"].append(0)\n",
    "                            else:\n",
    "                                for i in range(len(detected_gt[\"IOU\"])):\n",
    "                                    if best_prediction_value == detected_gt[\"IOU\"][i]: #True Positive\n",
    "                                        row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                        row[\"FP\"].append(0)\n",
    "                                        row[\"TP\"].append(1)\n",
    "                                    else:\n",
    "                                        row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                        row[\"FP\"].append(1)\n",
    "                                        row[\"TP\"].append(0)\n",
    "                            break\n",
    "                        '''\n",
    "            #### Sort all the elements by confidence\n",
    "            sort_indexes = [i for i, x in sorted(enumerate(row[\"Confidences\"]), key=lambda x: x[1], reverse=True)]\n",
    "            #print(sort_indexes, row.keys())\n",
    "            for key in row.keys():\n",
    "                if key == \"Recall\" or key == \"Precision\" or key == \"Acc_FP\" or key == \"Acc_TP\":\n",
    "                    continue\n",
    "                row[key] = [row[key][i] for i in sort_indexes]\n",
    "            for i in range(len(row[\"Confidences\"])):\n",
    "                if row[\"TP\"][i] == 1:\n",
    "                    acc_TP += 1\n",
    "                else:\n",
    "                    acc_FP += 1\n",
    "                row[\"Acc_TP\"].append(acc_TP)\n",
    "                row[\"Acc_FP\"].append(acc_FP)\n",
    "                row[\"Precision\"].append(acc_TP / (acc_TP + acc_FP))\n",
    "                row[\"Recall\"].append(acc_TP / total_positive_topics[topic])\n",
    "\n",
    "    ######### COCO EVALUATOR\n",
    "    mean_average_precision = 0\n",
    "    #for iou in range(50,100,5):\n",
    "    #iou = 30\n",
    "    #th = iou/100\n",
    "    th = th_fix\n",
    "    for topic in topic_names:\n",
    "        if total_positive_topics[topic] > 0:\n",
    "            topic_results = all_results[topic][th]\n",
    "            #fig = plt.figure()\n",
    "            #plt.plot(topic_results[\"Recall\"],topic_results[\"Precision\"])\n",
    "            #plt.pause(1)\n",
    "            if len(topic_results[\"Recall\"])<2:\n",
    "                continue\n",
    "            AP = sklearn.metrics.auc(topic_results[\"Recall\"], topic_results[\"Precision\"]) * 100\n",
    "            #print(\"AP:\", AP)\n",
    "            average_precision[topic][th] = AP\n",
    "            final_average_precision[topic] += AP\n",
    "        else:\n",
    "            continue\n",
    "    for topic in topic_names:\n",
    "        #final_average_precision[topic] = final_average_precision[topic] #/ 10 #now we have\n",
    "        mean_average_precision += final_average_precision[topic]\n",
    "    mean_average_precision = mean_average_precision/total_considered_topics\n",
    "    print(\"n_comparisons:\", n_comparisons)\n",
    "    #print(round(mean_average_precision,2),\"%\")\n",
    "\n",
    "    return average_precision, mean_average_precision\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "    names = ['Alice','Antonio','Carmine_nuove', 'Lucrezia']#,'Carmine_nuove', 'Lucrezia']\n",
    "    names_random = ['Alice_random','Antonio_random','Carmine_nuove_random', 'Lucrezia_random']#\n",
    "    model_types = [\"base\",\"large\"]\n",
    "    analysis_types = [\"results_moving_windows\",\"results_priority_policy_best_windows\"]\n",
    "    analysis_random = [\"results_random_final_0_3.pkl\",\"results_random_final_old_0_3.pkl\"]\n",
    "    gt_new = \"labelled_sentences_final.pkl\"\n",
    "    gt_old = \"labelled_sentences_final_old.pkl\"\n",
    "    processed_results = ''\n",
    "    all_labels = []\n",
    "    #filename_best = \"new_all_processed_sentences_best.pkl\" #it contains all the possible results; a results is the best words match for each topic for each possible group of words inside the sentence, for all the sentences\n",
    "    #filename_best = \"all_similarities_600.pkl\"\n",
    "    #filename_random = \"processed_sentences_random.pkl\" #it contains all the random labels\n",
    "    #filename_labelled_sentences = \"labelled_sentences_final.pkl\" #it contains the ground truth labels\n",
    "    #filename_labelled_sentences_old = \"labelled_sentences_final_old.pkl\" #it contains the ground truth labels\n",
    "    #filename_labelled_sentences = \"Marco_Limone_labelled_sentences_final.pkl\"\n",
    "    #filename_fixed_window_labels = \"results_priority_policy_best_windows.pkl\" #it contains the labelled sentences obtained by applying the average window approach\n",
    "    #filename_moving_window_labels = \"results_moving_window_policy.pkl\" #it contains the labelled sentences obtained by applying the moving window approach\n",
    "    AP_sum = []\n",
    "    '''\n",
    "    array_a = []\n",
    "    array_b = []\n",
    "    tmp_1 = []\n",
    "    tmp_2 =[]\n",
    "    tmp_val_1 = 0\n",
    "    tmp_val_2 = 0\n",
    "    real_labels = pickle.load(open(os.path.join(path,gt_new),'rb')) if os.path.exists(os.path.join(path,gt_new)) else print(\"unable to load GT\")\n",
    "    real_labels_old = pickle.load(open(os.path.join(path,gt_old),'rb')) if os.path.exists(os.path.join(path,gt_old)) else print(\"unable to load GT old\")\n",
    "    for name in names:\n",
    "        filename = analysis_types[0]+'_'+name+'_'+model_types[0]+\"_old_0_3.pkl\"\n",
    "        if(os.path.exists(os.path.join(path,filename))):\n",
    "            tmp_1 = (pickle.load(open(os.path.join(path,filename),'rb')))\n",
    "        else:\n",
    "            print(\"unable to find the file:  \",filename)\n",
    "        #filename = analysis_types[1]+'_'+name+'_'+model_types[0]+\"_old_0_3.pkl\"\n",
    "        filename = analysis_random[1]\n",
    "        if(os.path.exists(os.path.join(path,filename))):\n",
    "            tmp_2 = (pickle.load(open(os.path.join(path,filename),'rb')))\n",
    "        else:\n",
    "            print(\"unable to find the file:  \",filename)\n",
    "        _, tmp_val_1 = mean_average_precision(real_labels_old, tmp_1, th_fix=0.5)\n",
    "        _, tmp_val_2 = mean_average_precision(real_labels_old, tmp_2, th_fix=0.5)\n",
    "        array_a.append(tmp_val_1)\n",
    "        array_b.append(tmp_val_2)\n",
    "    res = scipy.stats.ttest_ind(array_a,array_b)\n",
    "    print(res)\n",
    "    '''\n",
    "\n",
    "    for name in names_random:\n",
    "        filename = analysis_types[0]+'_'+name+'_'+model_types[0]+\"_old_0_3.pkl\"\n",
    "        #filename = analysis_random[0]\n",
    "        if(os.path.exists(os.path.join(path,filename))):\n",
    "            all_labels.append(pickle.load(open(os.path.join(path,filename),'rb')))\n",
    "        else:\n",
    "            print(\"unable to find the file:  \",filename)\n",
    "\n",
    "    real_labels = pickle.load(open(os.path.join(path,gt_new),'rb')) if os.path.exists(os.path.join(path,gt_new)) else print(\"unable to load GT\")\n",
    "    real_labels_old = pickle.load(open(os.path.join(path,gt_old),'rb')) if os.path.exists(os.path.join(path,gt_old)) else print(\"unable to load GT old\")\n",
    "\n",
    "    all_AP = {name:[] for name in names}\n",
    "    all_IOU = {name:[] for name in names}\n",
    "    all_mAP = {name:0 for name in names}\n",
    "    all_mIOU = {name:0 for name in names}\n",
    "    exec_time = []\n",
    "    for i,labels in enumerate(all_labels):\n",
    "        #print(labels[0]['processing_t_1']+)\n",
    "        for label in labels:\n",
    "            exec_time.append(label['processing_t_1'] + label['processing_t_2'])\n",
    "            #exec_time.append(label['processing_time'])\n",
    "        all_AP[names[i]], all_mAP[names[i]] = mean_average_precision(real_labels_old, labels, th_fix=0.5)\n",
    "        all_IOU[names[i]], all_mIOU[names[i]] = mean_IOU(real_labels_old, labels,confidence=0.5)\n",
    "        #all_AP_ml, mAP_ml = mean_average_precision(all_real_labels, moving_window_labels)\n",
    "        #all_IOU_ml, mIOU_ml = mean_IOU(all_real_labels, moving_window_labels)\n",
    "    #print(\"mAP random labels:\", round(mAP_rl,2),\"   mIOU random labels:\",round(mIOU_rl*100,2))\n",
    "    #for name in names:\n",
    "\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    #print(topic_names)\n",
    "    AP_mean = {topic:[] for topic in topic_names}\n",
    "    mAP_mean = []\n",
    "    IOU_mean = {topic:[] for topic in topic_names}\n",
    "    mIOU_mean = []\n",
    "\n",
    "\n",
    "    for name in names:\n",
    "        #print(all_AP[name])\n",
    "        print(all_mAP[name])\n",
    "        print(all_mIOU[name])\n",
    "        mAP_mean.append(all_mAP[name])\n",
    "        mIOU_mean.append(all_mIOU[name]*100)\n",
    "        for topic in topic_names:\n",
    "            AP_mean[topic].append(all_AP[name][topic][0.5])\n",
    "            IOU_mean[topic].append(all_IOU[name][topic]['iou']*100)\n",
    "\n",
    "    print('mAP_mean:',round(np.mean(mAP_mean),2),'mAP_std:',round(np.std(mAP_mean),2))\n",
    "    print('mIOU_mean:',round(np.mean(mIOU_mean),2),'mAP_std:',round(np.std(mIOU_mean),2))\n",
    "    for topic in topic_names:\n",
    "        print(f'AP_mean {topic}:',round(np.mean(AP_mean[topic]),2),f'AP_std {topic}:',round(np.std(AP_mean[topic]),2))\n",
    "        print(f'IOU_mean {topic}:',round(np.mean(IOU_mean[topic]),2),f'mAP_std {topic}:',round(np.std(IOU_mean[topic]),2))\n",
    "    print(\"MEAN TIME:\", round(np.mean(exec_time),2), \"STD TIME:\", round(np.std(exec_time),2))\n",
    "\n",
    "\n",
    "    #print(np.mean(mAP_mean),np.AP_mean)\n",
    "    #print(IOU_mean, mIOU_mean)\n",
    "    #for topic in topic_names:\n",
    "    #    print()\n",
    "        #print(f\" mAP {name}\",round(all_mAP[name],2))\n",
    "        #print(f\" mIOU {name}\",round(all_mIOU[name]*100,2))\n",
    "    #AP_MEAN = np.mean(np.array(AP_mean))\n",
    "    #print(AP_MEAN[0.5])\n",
    "        #for topic_name in topic_names:\n",
    "        #    print(f\" AP {name} {topic_name}\",round(all_AP[name][topic_name][0.3],2))\n",
    "        #    print(f\" IOU {name} {topic_name}\",round(all_IOU[name][topic_name]['iou']*100,2))\n",
    "\n",
    "    #print(\"mAP fixed window labels:\", round(mAP_fl,2),\"     mIOU fixed window labels:\",round(mIOU_fl*100,2))\n",
    "    #print(\"mAP moving window labels:\", round(mAP_ml,2),\"    mIOU moving window labels:\",round(mIOU_ml*100,2))\n",
    "    '''\n",
    "    if(os.path.exists(os.path.join(path,filename_random))):    #load data already processed\n",
    "        all_random_labels = pickle.load(open(os.path.join(path,filename_random),'rb'))\n",
    "    else:\n",
    "        print(\"Invalid path for random labels\")\n",
    "    '''\n",
    "    '''\n",
    "    if(os.path.exists(os.path.join(path,filename_best))):    #load data already processed\n",
    "        result = pickle.load(open(os.path.join(path,filename_best),'rb'))\n",
    "    else:\n",
    "        print(\"Invalid path for all the results\")\n",
    "\n",
    "    if(os.path.exists(os.path.join(path,filename_labelled_sentences))):    #load data already processed\n",
    "        all_real_labels = pickle.load(open(os.path.join(path,filename_labelled_sentences),'rb'))\n",
    "        print(\"NEW\",len(all_real_labels))\n",
    "    else:\n",
    "        print(\"invalid real labels path\")\n",
    "\n",
    "    if(os.path.exists(os.path.join(path,filename_labelled_sentences_old))):    #load data already processed\n",
    "        all_real_labels_old = pickle.load(open(os.path.join(path,filename_labelled_sentences_old),'rb'))\n",
    "        print(\"OLD\",len(all_real_labels_old))\n",
    "    else:\n",
    "        print(\"invalid real labels path\")\n",
    "\n",
    "    if(os.path.exists(os.path.join(path,filename_fixed_window_labels))):    #load data already processed\n",
    "        fixed_window_labels = pickle.load(open(os.path.join(path,filename_fixed_window_labels),'rb'))\n",
    "    else:\n",
    "        print(\"invalid fixed window labels path\")\n",
    "\n",
    "    if(os.path.exists(os.path.join(path,filename_moving_window_labels))):    #load data already processed\n",
    "        moving_window_labels = pickle.load(open(os.path.join(path,filename_moving_window_labels),'rb'))\n",
    "    else:\n",
    "        print(\"invalid moving window labels path\")\n",
    "    '''\n",
    "    #print(len(all_real_labels))\n",
    "\n",
    "    #print(avg_labels[0],\"\\n\",real_labels[0])\n",
    "    #all_AP_rl, mAP_rl = mean_average_precision(all_real_labels, all_random_labels)\n",
    "    #all_IOU_rl, mIOU_rl = mean_IOU(all_real_labels, all_random_labels)\n",
    "    '''\n",
    "    all_real_labels = all_real_labels + all_real_labels_old\n",
    "    print(len(all_real_labels))\n",
    "    all_AP_fl, mAP_fl = mean_average_precision(all_real_labels, fixed_window_labels)\n",
    "    all_IOU_fl, mIOU_fl = mean_IOU(all_real_labels, fixed_window_labels)\n",
    "    all_AP_ml, mAP_ml = mean_average_precision(all_real_labels, moving_window_labels)\n",
    "    all_IOU_ml, mIOU_ml = mean_IOU(all_real_labels, moving_window_labels)\n",
    "    #print(\"mAP random labels:\", round(mAP_rl,2),\"   mIOU random labels:\",round(mIOU_rl*100,2))\n",
    "    print(\"mAP fixed window labels:\", round(mAP_fl,2),\"     mIOU fixed window labels:\",round(mIOU_fl*100,2))\n",
    "    print(\"mAP moving window labels:\", round(mAP_ml,2),\"    mIOU moving window labels:\",round(mIOU_ml*100,2))\n",
    "    '''\n",
    "    #print(all_real_labels[0],moving_window_labels[0])\n",
    "    #print(mAP)\n",
    "    #print(mIOU*100)\n",
    "    #print(\"m_IOU\",all_IOU)\n",
    "    #print(\"mAP\", all_AP)\n",
    "    #print(sentences_already_labelled[3:])\n",
    "    #print(avg_labels[0],\"\\n\",all_real_labels[199])\n",
    "    #for el in result:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### EVALUATION BETWEEN GT AND GT\n",
    "\n",
    "import os, pickle\n",
    "\n",
    "\n",
    "def precision_recall_curve(y_true, pred_scores, thresholds):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = [1 if score >= threshold else 0 for score in pred_scores]\n",
    "\n",
    "        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=1, zero_division=0)\n",
    "        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "def mean_IOU(gt_labels_1, gt_labels_2, confidence = 0.5):\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    topics_iou = {topic_gesture_description[key][0]: {'tot_intersection':0, 'tot_union':0, 'iou':0} for key in gt_labels_names}\n",
    "    total_positive_topics = {topic_name: 0 for topic_name in topic_names}\n",
    "    #topics_gt_labels = {topic_gesture_description[key][0]: [] for key in gt_labels_names}\n",
    "    #for gt_result,pd_result in zip(gt_labels,pd_labels): #for each sentence\n",
    "    #    assert(gt_result['sentence_index']==pd_result['sentence_index'])\n",
    "    for gt_1, gt_result in enumerate(gt_labels_1):\n",
    "        break_flag = False\n",
    "        for gt_2 in range(gt_1+1,len(gt_labels_2)):\n",
    "            if gt_1 == len(gt_labels_1):\n",
    "                break\n",
    "            if gt_labels_2[gt_2]['sentence'] != gt_result['sentence']: #check that the results are referred to the same sentence before continuing\n",
    "                continue\n",
    "            else:\n",
    "                #print(pd_result['sentence'], gt_result['sentence'])\n",
    "                for topic in topic_names:\n",
    "                    all_gt_indexes = []\n",
    "                    all_pd_indexes = []\n",
    "\n",
    "                    for k,gt_match_1 in enumerate(gt_result['labels,word_indexes']):\n",
    "                        gt_topic_1 = topic_gesture_description[[el for el in gt_match_1[0].items()][0][1]][0]\n",
    "                        if gt_topic_1 != topic: #or pd_result['values'][k] < confidence: #the result must respect the confidence threshold\n",
    "                            continue\n",
    "                        else:\n",
    "                            all_pd_indexes.append(gt_match_1[1])\n",
    "\n",
    "                    for j,gt_match_2 in enumerate(gt_labels_2[gt_2]['labels,word_indexes']):\n",
    "                        gt_topic_2 = topic_gesture_description[[el for el in gt_match_2[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "                        if gt_topic_2 != topic:\n",
    "                            continue\n",
    "                        else:\n",
    "                            all_gt_indexes.append(gt_match_2[1])\n",
    "\n",
    "\n",
    "                    union = len(set([j for sub in all_gt_indexes + all_pd_indexes for j in sub])) #total union in the sentence\n",
    "                    alL_intersections = [set(j).intersection(k) for j in all_pd_indexes for k in all_gt_indexes if len(set(j).intersection(k))!=0] #all the possible intersections\n",
    "                    intersection = len(set([i for a in alL_intersections for i in a]))  #total intersection area in the sentence\n",
    "                    topics_iou[topic]['tot_intersection'] += intersection\n",
    "                    topics_iou[topic]['tot_union'] += union\n",
    "                    if union == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        topics_iou[topic]['iou'] = topics_iou[topic]['tot_intersection'] / topics_iou[topic]['tot_union']\n",
    "                    #print(gt_result)\n",
    "                    #print(gt_labels_2[gt_2])\n",
    "                    #print(\"intersection:\",intersection, \"union:\",union, \"topic\", topic, \"tot_intersection\",topics_iou[topic]['tot_intersection'])\n",
    "                break\n",
    "\n",
    "    total_considered_topics = 0\n",
    "    for gt_result in gt_labels_1: #count how many gt labels we have for each topic. Will be considered only the topics that have at least one gt label\n",
    "        for j,gt_label in enumerate(gt_result['labels,word_indexes']):\n",
    "            gt_topic = topic_gesture_description[[el for el in gt_label[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "            total_positive_topics[gt_topic] += 1\n",
    "    #for topic in topic_names:\n",
    "    #    if total_positive_topics[topic] > 0:\n",
    "    #        total_considered_topics += 1\n",
    "\n",
    "    iou_sum = 0\n",
    "    for topic in topic_names: #compute mean_iou\n",
    "        if total_positive_topics[topic] > 0:\n",
    "            total_considered_topics += 1\n",
    "            #print(\"TOPIC NAME:\", topic, \"TOPIC_IOU:\",topics_iou[topic]['iou'])\n",
    "            iou_sum += topics_iou[topic]['iou']\n",
    "    mean_IOU = iou_sum / total_considered_topics\n",
    "    #print(\"MEAN IOU:\", mean_IOU)\n",
    "\n",
    "    return topics_iou, mean_IOU\n",
    "\n",
    "\n",
    "def mean_average_precision(gt_labels_1, gt_labels_2, th_fix = 0.5):\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    total_positive_topics = {topic_name: 0 for topic_name in topic_names}\n",
    "    all_results = {topic_name: {th/20: {\"Confidences\":[],\"TP\":[], \"FP\":[], \"Acc_TP\":[], \"Acc_FP\":[], \"Precision\":[], \"Recall\":[]} for th in range(1,21,1)}\n",
    "                   for topic_name in topic_names }\n",
    "    average_precision = {topic_name:{th/20:0 for th in range(1,21,1)} for topic_name in topic_names}\n",
    "    final_average_precision = {topic_name:0 for topic_name in topic_names}\n",
    "    total_considered_topics = 0\n",
    "\n",
    "    for gt_result in gt_labels_1:\n",
    "        for j,gt_label in enumerate(gt_result['labels,word_indexes']):\n",
    "            gt_topic = topic_gesture_description[[el for el in gt_label[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "            total_positive_topics[gt_topic] += 1\n",
    "    for topic in topic_names:\n",
    "        if total_positive_topics[topic] > 0:\n",
    "            total_considered_topics += 1\n",
    "    #print(total_considered_topics)\n",
    "    for t in range(1,21,1): #compute all the info needed to compute average precision for all the possible thresholds of IOU\n",
    "        th = t/20\n",
    "        for topic in topic_names: #for each possible topic\n",
    "            if total_positive_topics[topic] == 0: #it does not make sense the computation of Average Precision (we can't compute the Recall)\n",
    "                continue\n",
    "            row = all_results[topic][th] #we will create rows in the results referred to the chosen topic and th. The final table will contain\n",
    "                                         #all the info needed to compute average precision for a topic with a given IOU threshold\n",
    "            acc_TP = 0\n",
    "            acc_FP = 0\n",
    "            n_comparisons = 0\n",
    "            #####for skip, (gt_result,pd_result) in enumerate(zip(gt_labels,pd_labels)): #for each sentence\n",
    "            #print(\"LEN:\", len(gt_labels))\n",
    "            for num,gt_result in enumerate(gt_labels_1):\n",
    "                #if t==1 and topic == 'r_greet':\n",
    "                    #print(num)\n",
    "                #break_flag = False\n",
    "                for gt_2 in range(num+1,len(gt_labels_2)):\n",
    "                    if num == len(gt_labels_2):\n",
    "                        break\n",
    "                    if gt_labels_2[gt_2]['sentence'] != gt_result['sentence']: #check that the results are referred to the same sentence before continuing\n",
    "                        continue\n",
    "                    else:\n",
    "                        #print(gt_labels_2[gt_2])\n",
    "                        #print(gt_result)\n",
    "                        #break_flag = True\n",
    "                        #if t == 1 and topic == 'r_greet':\n",
    "                        n_comparisons += 1\n",
    "                        #print(\"prediction:\",pd_result['sentence'])\n",
    "                        #print(\"gt:\",gt_result['sentence'])\n",
    "                        #n_comparisons += 1\n",
    "                        #print(n_comparisons)\n",
    "                        ######assert(gt_result['sentence_index']==pd_result['sentence_index'])\n",
    "                        #all_gt_indexes = []\n",
    "                        #all_pd_indexes = []\n",
    "                        detected_gt = {\"IOU\":[], \"Confidence\":[], 'gt_indexes':[]}\n",
    "                        for k,gt_match_2 in enumerate(gt_labels_2[gt_2]['labels,word_indexes']):\n",
    "                            #print(gt_labels_2[gt_2]['labels,word_indexes'])\n",
    "                            gt_topic_2 = topic_gesture_description[[el for el in gt_match_2[0].items()][0][1]][0]  #extract the pd_topic from the pd_label   #do the same for the predicted label\n",
    "                            if gt_topic_2 != topic:\n",
    "                                #print(\"SKIPPING PREDICTED:\", k, pd_result)\n",
    "                                continue\n",
    "                            else: #right topic, check the matches with ground truths\n",
    "                                #print(\"NOT SKIPPED PD TOPIC\",skip,pd_result)\n",
    "                                n_intersections = 0\n",
    "                                tmp_IOU = 0\n",
    "                                tmp_conf = 0\n",
    "                                for j,gt_match_1 in enumerate(gt_result['labels,word_indexes']):  #for each prediction, we take only the GT that has highest IOU\n",
    "                                                                                                #the other can be False Negatives or True positives\n",
    "                                    gt_topic_1 = topic_gesture_description[[el for el in gt_match_1[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "                                    if gt_topic_1 != topic: #only if gt_topic and pd_topic are the same we can do the comparison\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        intersection = len(set(gt_match_2[1]).intersection(gt_match_1[1]))\n",
    "                                        if intersection != 0: #we take append to a list that takes all the possible IOUs. Only the best one IOU is TP, for the other topics we have to check if they have predictions with the others GT\n",
    "                                            union = len(set(gt_match_2[1] + gt_match_1[1]))\n",
    "                                            IOU = intersection/union\n",
    "                                            n_intersections += 1\n",
    "                                            if IOU > tmp_IOU: #this allow to take only the best IOU, the others (with other GT) are false negatives\n",
    "                                                tmp_conf = 1 #pd_result['values'][k]\n",
    "                                                tmp_IOU = IOU\n",
    "                                                gt_index = j\n",
    "                                        else:\n",
    "                                            continue\n",
    "                                if n_intersections == 0: #then the prediction has no intersections, so we have a false positive\n",
    "                                    row[\"Confidences\"].append(1)\n",
    "                                    row[\"FP\"].append(1)\n",
    "                                    row[\"TP\"].append(0)\n",
    "                                else: #we have to do another check: for each GT, we have to check which is the predicted topic that has highest IOU\n",
    "                                    detected_gt[\"IOU\"].append(tmp_IOU) #IOU\n",
    "                                    detected_gt[\"Confidence\"].append(tmp_conf) #confidence\n",
    "                                    detected_gt[\"gt_indexes\"].append(gt_index)\n",
    "                        #print(detected_gt)\n",
    "                        if len(detected_gt[\"IOU\"]) == 0: #then we have no TP, only FP which is already considered and appended as rows\n",
    "                            break\n",
    "                        else:\n",
    "                            unique_indexes = set(detected_gt[\"gt_indexes\"])\n",
    "                            for idx in unique_indexes:\n",
    "                                gt_idx = [i for i, d in enumerate(detected_gt[\"gt_indexes\"]) if d == idx] #extract predictions related to same GT\n",
    "                                IOUs = [detected_gt[\"IOU\"][i] for i in gt_idx]\n",
    "                                max_IOU_idx = detected_gt[\"IOU\"].index(max(IOUs))\n",
    "                                if max(IOUs) < th:\n",
    "                                    for i in gt_idx:\n",
    "                                        row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                        row[\"FP\"].append(1)\n",
    "                                        row[\"TP\"].append(0)\n",
    "                                else:\n",
    "                                    for i in gt_idx:\n",
    "                                        if i == max_IOU_idx: #True Positive\n",
    "                                            row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                            row[\"FP\"].append(0)\n",
    "                                            row[\"TP\"].append(1)\n",
    "                                        else:\n",
    "                                            row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                            row[\"FP\"].append(1)\n",
    "                                            row[\"TP\"].append(0)\n",
    "                        '''\n",
    "                        if len(detected_gt[\"IOU\"]) == 0:\n",
    "                            #continue\n",
    "                            break\n",
    "                        else:\n",
    "                            best_prediction_value = max(detected_gt[\"IOU\"])\n",
    "                            if best_prediction_value < th: #then all the predictions are false positives\n",
    "                                for i in range(len(detected_gt[\"IOU\"])):\n",
    "                                    row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                    row[\"FP\"].append(1)\n",
    "                                    row[\"TP\"].append(0)\n",
    "                            else:\n",
    "                                for i in range(len(detected_gt[\"IOU\"])):\n",
    "                                    if best_prediction_value == detected_gt[\"IOU\"][i]: #True Positive\n",
    "                                        row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                        row[\"FP\"].append(0)\n",
    "                                        row[\"TP\"].append(1)\n",
    "                                    else:\n",
    "                                        row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                        row[\"FP\"].append(1)\n",
    "                                        row[\"TP\"].append(0)\n",
    "                            break\n",
    "                        '''\n",
    "            #### Sort all the elements by confidence\n",
    "            sort_indexes = [i for i, x in sorted(enumerate(row[\"Confidences\"]), key=lambda x: x[1], reverse=True)]\n",
    "            #print(sort_indexes, row.keys())\n",
    "            for key in row.keys():\n",
    "                if key == \"Recall\" or key == \"Precision\" or key == \"Acc_FP\" or key == \"Acc_TP\":\n",
    "                    continue\n",
    "                row[key] = [row[key][i] for i in sort_indexes]\n",
    "            for i in range(len(row[\"Confidences\"])):\n",
    "                if row[\"TP\"][i] == 1:\n",
    "                    acc_TP += 1\n",
    "                else:\n",
    "                    acc_FP += 1\n",
    "                row[\"Acc_TP\"].append(acc_TP)\n",
    "                row[\"Acc_FP\"].append(acc_FP)\n",
    "                row[\"Precision\"].append(acc_TP / (acc_TP + acc_FP))\n",
    "                row[\"Recall\"].append(acc_TP / total_positive_topics[topic])\n",
    "\n",
    "    ######### COCO EVALUATOR\n",
    "    mean_average_precision = 0\n",
    "    #for iou in range(50,100,5):\n",
    "    #iou = 30\n",
    "    #th = iou/100\n",
    "    th = th_fix\n",
    "    for topic in topic_names:\n",
    "        if total_positive_topics[topic] > 0:\n",
    "            topic_results = all_results[topic][th]\n",
    "            #fig = plt.figure()\n",
    "            #plt.plot(topic_results[\"Recall\"],topic_results[\"Precision\"])\n",
    "            #plt.pause(1)\n",
    "            if len(topic_results[\"Recall\"])<2:\n",
    "                continue\n",
    "            AP = sklearn.metrics.auc(topic_results[\"Recall\"], topic_results[\"Precision\"]) * 100\n",
    "            #print(\"AP:\", AP)\n",
    "            average_precision[topic][th] = AP\n",
    "            final_average_precision[topic] += AP\n",
    "        else:\n",
    "            continue\n",
    "    for topic in topic_names:\n",
    "        #final_average_precision[topic] = final_average_precision[topic] #/ 10 #now we have\n",
    "        mean_average_precision += final_average_precision[topic]\n",
    "    mean_average_precision = mean_average_precision/total_considered_topics\n",
    "    print(\"n_comparisons:\", n_comparisons)\n",
    "    #print(round(mean_average_precision,2),\"%\")\n",
    "\n",
    "    return average_precision, mean_average_precision\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "    names = ['Alice','Antonio','Carmine_nuove', 'Lucrezia']#,'Carmine_nuove', 'Lucrezia']\n",
    "    names_random = ['Alice_random','Antonio_random','Carmine_nuove_random', 'Lucrezia_random']#\n",
    "    model_types = [\"base\",\"large\"]\n",
    "    analysis_types = [\"results_moving_windows\",\"results_priority_policy_best_windows\"]\n",
    "    analysis_random = [\"results_random_final_0_3.pkl\",\"results_random_final_old_0_3.pkl\"]\n",
    "    gt_new = \"labelled_sentences_final.pkl\"\n",
    "    gt_old = \"labelled_sentences_final_old.pkl\"\n",
    "    processed_results = ''\n",
    "    all_labels = []\n",
    "    #filename_best = \"new_all_processed_sentences_best.pkl\" #it contains all the possible results; a results is the best words match for each topic for each possible group of words inside the sentence, for all the sentences\n",
    "    #filename_best = \"all_similarities_600.pkl\"\n",
    "    #filename_random = \"processed_sentences_random.pkl\" #it contains all the random labels\n",
    "    #filename_labelled_sentences = \"labelled_sentences_final.pkl\" #it contains the ground truth labels\n",
    "    #filename_labelled_sentences_old = \"labelled_sentences_final_old.pkl\" #it contains the ground truth labels\n",
    "    #filename_labelled_sentences = \"Marco_Limone_labelled_sentences_final.pkl\"\n",
    "    #filename_fixed_window_labels = \"results_priority_policy_best_windows.pkl\" #it contains the labelled sentences obtained by applying the average window approach\n",
    "    #filename_moving_window_labels = \"results_moving_window_policy.pkl\" #it contains the labelled sentences obtained by applying the moving window approach\n",
    "    AP_sum = []\n",
    "    '''\n",
    "    array_a = []\n",
    "    array_b = []\n",
    "    tmp_1 = []\n",
    "    tmp_2 =[]\n",
    "    tmp_val_1 = 0\n",
    "    tmp_val_2 = 0\n",
    "    real_labels = pickle.load(open(os.path.join(path,gt_new),'rb')) if os.path.exists(os.path.join(path,gt_new)) else print(\"unable to load GT\")\n",
    "    real_labels_old = pickle.load(open(os.path.join(path,gt_old),'rb')) if os.path.exists(os.path.join(path,gt_old)) else print(\"unable to load GT old\")\n",
    "    for name in names:\n",
    "        filename = analysis_types[0]+'_'+name+'_'+model_types[0]+\"_old_0_3.pkl\"\n",
    "        if(os.path.exists(os.path.join(path,filename))):\n",
    "            tmp_1 = (pickle.load(open(os.path.join(path,filename),'rb')))\n",
    "        else:\n",
    "            print(\"unable to find the file:  \",filename)\n",
    "        #filename = analysis_types[1]+'_'+name+'_'+model_types[0]+\"_old_0_3.pkl\"\n",
    "        filename = analysis_random[1]\n",
    "        if(os.path.exists(os.path.join(path,filename))):\n",
    "            tmp_2 = (pickle.load(open(os.path.join(path,filename),'rb')))\n",
    "        else:\n",
    "            print(\"unable to find the file:  \",filename)\n",
    "        _, tmp_val_1 = mean_average_precision(real_labels_old, tmp_1, th_fix=0.5)\n",
    "        _, tmp_val_2 = mean_average_precision(real_labels_old, tmp_2, th_fix=0.5)\n",
    "        array_a.append(tmp_val_1)\n",
    "        array_b.append(tmp_val_2)\n",
    "    res = scipy.stats.ttest_ind(array_a,array_b)\n",
    "    print(res)\n",
    "    '''\n",
    "    '''\n",
    "    for name in names_random:\n",
    "        filename = analysis_types[0]+'_'+name+'_'+model_types[0]+\"_old_0_3.pkl\"\n",
    "        #filename = analysis_random[0]\n",
    "        if(os.path.exists(os.path.join(path,filename))):\n",
    "            all_labels.append(pickle.load(open(os.path.join(path,filename),'rb')))\n",
    "        else:\n",
    "            print(\"unable to find the file:  \",filename)\n",
    "    '''\n",
    "    real_labels = pickle.load(open(os.path.join(path,gt_new),'rb')) if os.path.exists(os.path.join(path,gt_new)) else print(\"unable to load GT\")\n",
    "    real_labels_old = pickle.load(open(os.path.join(path,gt_old),'rb')) if os.path.exists(os.path.join(path,gt_old)) else print(\"unable to load GT old\")\n",
    "    IOU, mIOU = mean_IOU(real_labels_old + real_labels, real_labels_old + real_labels,confidence=0.5)\n",
    "    AP, mAP = mean_average_precision(real_labels_old + real_labels, real_labels_old + real_labels, th_fix=0.5)\n",
    "\n",
    "    #all_AP = {name:[] for name in names}\n",
    "    #all_IOU = {name:[] for name in names}\n",
    "    #all_mAP = {name:0 for name in names}\n",
    "    #all_mIOU = {name:0 for name in names}\n",
    "    #exec_time = []\n",
    "    #for i,labels in enumerate(all_labels):\n",
    "        #print(labels[0]['processing_t_1']+)\n",
    "        #for label in labels:\n",
    "            #exec_time.append(label['processing_t_1'] + label['processing_t_2'])\n",
    "            #exec_time.append(label['processing_time'])\n",
    "\n",
    "        #all_AP_ml, mAP_ml = mean_average_precision(all_real_labels, moving_window_labels)\n",
    "        #all_IOU_ml, mIOU_ml = mean_IOU(all_real_labels, moving_window_labels)\n",
    "    #print(\"mAP random labels:\", round(mAP_rl,2),\"   mIOU random labels:\",round(mIOU_rl*100,2))\n",
    "    #for name in names:\n",
    "\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    #print(topic_names)\n",
    "    AP_mean = {topic:[] for topic in topic_names}\n",
    "    mAP_mean = []\n",
    "    IOU_mean = {topic:[] for topic in topic_names}\n",
    "    mIOU_mean = []\n",
    "\n",
    "\n",
    "    #for name in names:\n",
    "        #print(all_AP[name])\n",
    "    #print(mAP)\n",
    "    #print(mIOU*100)\n",
    "    #mAP_mean.append(all_mAP[name])\n",
    "    #mIOU_mean.append(all_mIOU[name]*100)\n",
    "    #for topic in topic_names:\n",
    "    #    AP_mean[topic].append(AP[topic][0.5])\n",
    "    #    IOU_mean[topic].append(IOU[topic]['iou']*100)\n",
    "\n",
    "    print('mAP:',round(mAP,2)) #,'mAP_std:',round(mAP,2))\n",
    "    print('mIOU:',round(mIOU*100,2))\n",
    "    #print('mIOU_mean:',round(np.mean(mIOU_mean),2),'mAP_std:',round(np.std(mIOU_mean),2))\n",
    "    for topic in topic_names:\n",
    "        print(f'AP {topic}:',round(AP[topic][0.5],2))#,f'AP_std {topic}:',round(np.std(AP_mean[topic]),2))\n",
    "        print(f'IOU {topic}:',round(IOU[topic]['iou']*100,2), \"tot intersection:\",round(IOU[topic]['tot_intersection'],2), \"tot union\",round(IOU[topic]['tot_union'],2))\n",
    "        #print(f'IOU_mean {topic}:',round(np.mean(IOU_mean[topic]),2),f'mAP_std {topic}:',round(np.std(IOU_mean[topic]),2))\n",
    "    #print(\"MEAN TIME:\", round(np.mean(exec_time),2), \"STD TIME:\", round(np.std(exec_time),2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### JUST FOR TESTS\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "result_name = \"all_similarities_Antonio_base_old.pkl\"\n",
    "result = pickle.load(open(os.path.join(path,result_name),'rb'))\n",
    "print(result[1], list(result[2].keys())[1:-1])\n",
    "tmp = result[0]\n",
    "tmp_ind = result[0]['window 1']['start_word_indexes'].index(1)\n",
    "tmp_ind_2 = result[0]['window 1']['start_word_indexes'].index(2)\n",
    "print(result[0]['window 1']['value'][tmp_ind:tmp_ind_2])\n",
    "a=[1,2,3,1,2,1,1,3,4,3,2]\n",
    "f=[-1,0,1,2,3,4,5,6,7,8,9]\n",
    "b=[i for i in range(5)]\n",
    "print(b)\n",
    "if 3 not in a:\n",
    "    print('here')\n",
    "#print(a.index(4))\n",
    "b = set(a)\n",
    "for x in b:\n",
    "    c = [i for i, d in enumerate(a) if d == x]\n",
    "    print(\"b\",b,c,[f[i] for i in c])\n",
    "\n",
    "a = [[1,2,3],[4,5],[7,8,9,10], [11,12,13,14,15]]\n",
    "b = [[2,3], [6,7,8], [9,10], [12,13], [16]]\n",
    "\n",
    "alL_intersections = [set(j).intersection(k) for j in a for k in b if len(set(j).intersection(k))!=0] #all the possible interesections\n",
    "intersection = len(set([i for a in alL_intersections for i in a]))  #total intersection area in the sentence\n",
    "print(alL_intersections, intersection)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### COMPUTE SOME STATISTICS ON PROCESSED SENTENCES\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "#filename_best = \"new_all_processed_sentences_best.pkl\" #it contains all the possible results; a results is the best words match for each topic for each possible group of words inside the sentence, for all the sentences\n",
    "#filename_best = \"all_similarities.pkl\"\n",
    "#filename_random = \"processed_sentences_random_300.pkl\" #it contains all the random labels\n",
    "#filename_labelled_sentences = \"labelled_sentences_final.pkl\" #it contains the ground truth labels\n",
    "filename_fixed_window_labels = \"results_priority_policy_best_windows.pkl\" #it contains the labelled sentences obtained by applying the average window approach\n",
    "filename_moving_window_labels = \"results_moving_window_policy.pkl\" #it contains the labelled sentences obtained by applying the moving window approach\n",
    "#filename_random_2 = \"processed_sentences_random_300_2.pkl\" #it contains all the random labels\n",
    "#filename_labelled_sentences = \"labelled_sentences_final.pkl\" #it contains the ground truth labels\n",
    "filename_fixed_window_labels_old = \"results_priority_policy_best_windows_old.pkl\" #it contains the labelled sentences obtained by applying the average window approach\n",
    "filename_moving_window_labels_old = \"results_moving_window_policy_old.pkl\" #it contains the labelled sentences obtained by applying the moving window approach\n",
    "\n",
    "\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_moving_window_labels))):    #load data already processed\n",
    "    moving_window_labels = pickle.load(open(os.path.join(path,filename_moving_window_labels),'rb'))\n",
    "else:\n",
    "    print(\"invalid moving window labels path\")\n",
    "\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_fixed_window_labels))):    #load data already processed\n",
    "    fixed_window_labels = pickle.load(open(os.path.join(path,filename_fixed_window_labels),'rb'))\n",
    "else:\n",
    "    print(\"invalid fixed window labels path\")\n",
    "\n",
    "'''\n",
    "if(os.path.exists(os.path.join(path,filename_random))):    #load data already processed\n",
    "    all_random_labels = pickle.load(open(os.path.join(path,filename_random),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path for random labels\")\n",
    "'''\n",
    "\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_moving_window_labels_old))):    #load data already processed\n",
    "    moving_window_labels_old = pickle.load(open(os.path.join(path,filename_moving_window_labels_old),'rb'))\n",
    "else:\n",
    "    print(\"invalid moving window labels path\")\n",
    "\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_fixed_window_labels_old))):    #load data already processed\n",
    "    fixed_window_labels_old = pickle.load(open(os.path.join(path,filename_fixed_window_labels_old),'rb'))\n",
    "else:\n",
    "    print(\"invalid fixed window labels path\")\n",
    "\n",
    "'''\n",
    "if(os.path.exists(os.path.join(path,filename_random_2))):    #load data already processed\n",
    "    all_random_labels_2 = pickle.load(open(os.path.join(path,filename_random_2),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path for random labels\")\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def count_gt_instances(pd_results): #counts the labelled words in the corpus, all the words in topic, all the labels and for each label counts how many times they appear and how many words they label\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    topic_count =  {topic_name: {'topic_instances':0, 'topic_words':0} for topic_name in topic_names}\n",
    "    total_words = 0 #total words in corpus\n",
    "    total_labels = 0 #total label instances\n",
    "    total_labelled_words = 0 #counts how many words were labelled\n",
    "    for pd_result in pd_results:\n",
    "        sentence = pd_result['sentence']\n",
    "        total_words += len(sentence.split())\n",
    "        for pd_label in zip(pd_result['topics&ref'],pd_result['word_indexes']):\n",
    "            pd_topic = pd_label[0][0] #topic_gesture_description[[el for el in gt_label[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "            topic_count[pd_topic]['topic_instances'] += 1\n",
    "            topic_count[pd_topic]['topic_words'] += len(pd_label[1])\n",
    "            total_labels += 1\n",
    "            total_labelled_words += len(pd_label[1])\n",
    "\n",
    "    return topic_count, total_words, total_labels, total_labelled_words\n",
    "\n",
    "topic_count_f, total_words_f, total_labels_f, total_labelled_words_f = count_gt_instances(fixed_window_labels)\n",
    "topic_count_m, total_words_m, total_labels_m, total_labelled_words_m = count_gt_instances(moving_window_labels)\n",
    "#topic_count_r, total_words_r, total_labels_r, total_labelled_words_r = count_gt_instances(all_random_labels)\n",
    "topic_count_f2, total_words_f2, total_labels_f2, total_labelled_words_f2 = count_gt_instances(fixed_window_labels_old)\n",
    "topic_count_m2, total_words_m2, total_labels_m2, total_labelled_words_m2 = count_gt_instances(moving_window_labels_old)\n",
    "#topic_count_r2, total_words_r2, total_labels_r2, total_labelled_words_r2 = count_gt_instances(all_random_labels_2)\n",
    "print(topic_count_f, \"\\n\\n\", topic_count_m,\"\\n\\n\", total_labels_f, \"\\n\\n\", total_labels_m, \"\\n\\n\",total_labelled_words_m, \"\\n\\n\\n\")\n",
    "print(topic_count_f2, \"\\n\\n\", topic_count_m2, \"\\n\\n\", total_labels_f2, \"\\n\\n\", total_labels_m2, \"\\n\\n\", total_labelled_words_m2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### EVALUATION - MODIFIED VERSION THAT CONSIDERS AS TRUE ALL THE MATCHES\n",
    "\n",
    "import os, pickle, copy\n",
    "\n",
    "def take_elements_topic(res, topic):\n",
    "    refs = all_refs[topic]\n",
    "    #print(refs)\n",
    "    res_copy = copy.deepcopy(res) #res.copy()\n",
    "    #print(res_copy)\n",
    "    #for res in result:\n",
    "    max_window_dim = int(list(res.keys())[-2].split(' ')[-1])\n",
    "    #print(max_window_dim)\n",
    "    for window in range(1,max_window_dim+1):\n",
    "        all_res_refs = res[f\"window {window}\"]['ref']\n",
    "        #print(all_res_refs)\n",
    "        topic_indexes = [i for i in range(len(all_res_refs)) for r in refs if all_res_refs[i] == r] #returns the indexes of the matches that are related to a topic\n",
    "        res_copy[f\"window {window}\"]['value'] = list(map(res_copy[f\"window {window}\"]['value'].__getitem__,topic_indexes))\n",
    "        res_copy[f\"window {window}\"]['ref'] = list(map(res_copy[f\"window {window}\"]['ref'].__getitem__,topic_indexes))\n",
    "        res_copy[f\"window {window}\"]['sentence'] = list(map(res_copy[f\"window {window}\"]['sentence'].__getitem__,topic_indexes))\n",
    "        res_copy[f\"window {window}\"]['start_word_indexes'] = list(map(res_copy[f\"window {window}\"]['start_word_indexes'].__getitem__,topic_indexes))\n",
    "    #print(res)\n",
    "    return res_copy\n",
    "\n",
    "def take_best_match(best_match, pd_result, values, window, th):\n",
    "    dic_window = f'window {window}'\n",
    "    #print(pd_result)\n",
    "    #print(dic_window)\n",
    "    #print(values)\n",
    "    th_indexes = [i for i in range(len(values)) if values[i] > th] #returns the indexes of the values that are greater than th\n",
    "    th_values = list(map(values.__getitem__,th_indexes)) #take all the values greater than th\n",
    "    if len(th_values)==0:\n",
    "        return best_match\n",
    "    #th_ref = list(map(pd_result[dic_window]['ref'].__getitem__,th_indexes)) #take all the references that returns a value greater than th\n",
    "    #th_matching_words = list(map(pd_result[dic_window]['sentence'].__getitem__,th_indexes)) #take all the sentence parts that return a value greater than new_th\n",
    "    #th_topics = [key for key in all_refs.keys() for ref in all_refs[key] if tmp_ref == ref][0]\n",
    "    #th_start_indexes =  list(map(pd_result[dic_window]['start_word_indexes'].__getitem__,th_indexes))\n",
    "    max_val_index = pd_result[dic_window]['value'].index(max(th_values))\n",
    "    max_val = pd_result[dic_window]['value'][max_val_index]\n",
    "    #print(\"WINDOW:\",window,\"MAX VAL INDEX\", max_val_index)\n",
    "    #print(\"MAX VAL\",max_val, \" best_match['value']\", best_match['value'])\n",
    "    #print(\"PD RESULT\", pd_result[dic_window])\n",
    "    if max_val > best_match['value']:\n",
    "        best_match['value'] = max_val\n",
    "        best_match['ref'] = pd_result[dic_window]['ref'][max_val_index]\n",
    "        best_match['matching_words'] = pd_result[dic_window]['sentence'][max_val_index]\n",
    "        best_match['start_word_indexes'] = pd_result[dic_window]['start_word_indexes'][max_val_index]\n",
    "        best_match['window'] = window\n",
    "    return best_match\n",
    "\n",
    "def extract_values(start_index,pd_result,window):  ## extract all the values related to the references of some topic (if a topic has 4 refs then I will have 4 vals)\n",
    "    #print(window)\n",
    "    window = f'window {window}'\n",
    "    if start_index not in pd_result[window]['start_word_indexes']:\n",
    "        return False\n",
    "    else:\n",
    "        values = []\n",
    "        #print(start_index,type(start_index))\n",
    "        valid_index = pd_result[window]['start_word_indexes'].index(start_index)\n",
    "        i = valid_index\n",
    "        while pd_result[window]['start_word_indexes'][i] == start_index:\n",
    "            values.append(pd_result[window]['value'][i])\n",
    "            i += 1\n",
    "            if i >= len(pd_result[window]['start_word_indexes']):\n",
    "                break\n",
    "\n",
    "    '''\n",
    "    elif start_index in pd_result[window]['start_word_indexes'] and start_index + 1  not in pd_result[window]['start_word_indexes']:\n",
    "        valid_index = pd_result[window]['start_word_indexes'].index(start_index)\n",
    "        values = pd_result[window]['value'][valid_index:]\n",
    "    else:\n",
    "        valid_index = pd_result[window]['start_word_indexes'].index(start_index)\n",
    "        end_index = pd_result[window]['start_word_indexes'].index(start_index + 1)\n",
    "        values = pd_result[window]['value'][valid_index:end_index]\n",
    "    '''\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def precision_recall_curve(y_true, pred_scores, thresholds):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = [1 if score >= threshold else 0 for score in pred_scores]\n",
    "\n",
    "        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=1, zero_division=0)\n",
    "        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "def mean_IOU(gt_labels, pd_labels, best_windows, confidence = 0.5, type = 'moving_window'):\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    topics_iou = {topic_gesture_description[key][0]: {'tot_intersection':0, 'tot_union':0, 'iou':0} for key in gt_labels_names}\n",
    "    total_positive_topics = {topic_name: 0 for topic_name in topic_names}\n",
    "    #topics_gt_labels = {topic_gesture_description[key][0]: [] for key in gt_labels_names}\n",
    "    #for gt_result,pd_result in zip(gt_labels,pd_labels): #for each sentence\n",
    "    #    assert(gt_result['sentence_index']==pd_result['sentence_index'])\n",
    "    for topic in topic_names:\n",
    "        for gt_result in gt_labels:\n",
    "            for pd_result in pd_labels:\n",
    "                if pd_result['whole sentence'] != gt_result['sentence']: #check that the results are referred to the same sentence before continuing\n",
    "                    continue\n",
    "                else:\n",
    "                    #print(pd_result['sentence'], gt_result['sentence'])\n",
    "                    all_gt_indexes = []\n",
    "                    all_pd_indexes = []\n",
    "                    start_index = 0 #we can take the results from the word in this index to avoid window overlapping\n",
    "                    pd_result_copy = take_elements_topic(pd_result, topic) ##all the results referred to some topic\n",
    "                    new_pd_result = {'topics&ref':[],'word_indexes':[],'values':[]}\n",
    "                    #n_comparisons += 1 #count how many GT-PD comparisons we made for debugging\n",
    "\n",
    "                    while start_index < len(pd_result['whole sentence'].split()):\n",
    "                        best_match = {'value':0,'ref':'','matching_words':'','start_word_indexes':0,'window':1} #it contains the data related to the topic with highest confidence\n",
    "                        if type == 'moving_window':\n",
    "                            for window in list(pd_result_copy.keys())[1:-1]:\n",
    "                                window = int(window.split(' ')[-1])\n",
    "                                values = extract_values(start_index,pd_result_copy,window)\n",
    "                                if values is False:\n",
    "                                    break\n",
    "                                else:\n",
    "                                    best_match = take_best_match(best_match,pd_result_copy,values,window,confidence)\n",
    "\n",
    "                        elif type == 'fixed_window':\n",
    "                            window = best_windows[topic]['best_window']\n",
    "                            if window > len(pd_result_copy['whole sentence'].split()):\n",
    "                                break\n",
    "                            values = extract_values(start_index,pd_result_copy,window)\n",
    "                            if values is False:\n",
    "                                break\n",
    "                            else:\n",
    "                                best_match = take_best_match(best_match,pd_result_copy,values,window,confidence)\n",
    "                        else:\n",
    "                            print(\"wrong type\")\n",
    "                            return\n",
    "                        swi = best_match['start_word_indexes']\n",
    "                        new_pd_result['topics&ref'].append((topic, best_match['ref']))\n",
    "                        new_pd_result['word_indexes'].append([i for i in range(swi,swi + best_match['window'])])\n",
    "                        new_pd_result['values'].append(best_match['value'])\n",
    "                        start_index += best_match['window']\n",
    "                    pd_result_copy.clear()\n",
    "                    for j,gt_match in enumerate(gt_result['labels,word_indexes']):\n",
    "                        gt_topic = topic_gesture_description[[el for el in gt_match[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "                        if gt_topic != topic:\n",
    "                            continue\n",
    "                        else:\n",
    "                            all_gt_indexes.append(gt_match[1])\n",
    "                    for k,pd_match in enumerate(zip(new_pd_result['topics&ref'],new_pd_result['word_indexes'])):\n",
    "                        pd_topic = pd_match[0][0]\n",
    "                        if pd_topic != topic or new_pd_result['values'][k] < confidence: #the result must respect the confidence threshold\n",
    "                            continue\n",
    "                        else:\n",
    "                            all_pd_indexes.append(pd_match[1])\n",
    "\n",
    "                    union = len(set([j for sub in all_gt_indexes + all_pd_indexes for j in sub])) #total union in the sentence\n",
    "                    alL_intersections = [set(j).intersection(k) for j in all_pd_indexes for k in all_gt_indexes if len(set(j).intersection(k))!=0] #all the possible intersections\n",
    "                    intersection = len(set([i for a in alL_intersections for i in a]))  #total intersection area in the sentence\n",
    "                    topics_iou[topic]['tot_intersection'] += intersection\n",
    "                    topics_iou[topic]['tot_union'] += union\n",
    "                    if union == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        topics_iou[topic]['iou'] = topics_iou[topic]['tot_intersection'] / topics_iou[topic]['tot_union']\n",
    "                break\n",
    "\n",
    "    total_considered_topics = 0\n",
    "    for gt_result in gt_labels: #count how many gt labels we have for each topic. Will be considered only the topics that have at least one gt label\n",
    "        for j,gt_label in enumerate(gt_result['labels,word_indexes']):\n",
    "            gt_topic = topic_gesture_description[[el for el in gt_label[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "            total_positive_topics[gt_topic] += 1\n",
    "    #for topic in topic_names:\n",
    "    #    if total_positive_topics[topic] > 0:\n",
    "    #        total_considered_topics += 1\n",
    "\n",
    "    iou_sum = 0\n",
    "    for topic in topic_names: #compute mean_iou\n",
    "        if total_positive_topics[topic] > 0:\n",
    "            total_considered_topics += 1\n",
    "            #print(\"TOPIC NAME:\", topic, \"TOPIC_IOU:\",topics_iou[topic]['iou'])\n",
    "            iou_sum += topics_iou[topic]['iou']\n",
    "    mean_IOU = iou_sum / total_considered_topics\n",
    "    #print(\"MEAN IOU:\", mean_IOU)\n",
    "\n",
    "    return topics_iou, mean_IOU\n",
    "\n",
    "\n",
    "def mean_average_precision(gt_labels, pd_labels, best_windows, type = 'moving_window'):\n",
    "\n",
    "    gt_labels_names = topic_gesture_description.keys()\n",
    "    topic_names = [topic_gesture_description[name][0] for name in gt_labels_names]\n",
    "    total_positive_topics = {topic_name: 0 for topic_name in topic_names} #counts all the positive topics\n",
    "    all_results = {topic_name: {th/20: {\"Confidences\":[],\"TP\":[], \"FP\":[], \"Acc_TP\":[], \"Acc_FP\":[], \"Precision\":[], \"Recall\":[]} for th in range(1,21,1)}\n",
    "                   for topic_name in topic_names }\n",
    "    average_precision = {topic_name:{th/20:0 for th in range(1,21,1)} for topic_name in topic_names}\n",
    "    final_average_precision = {topic_name:0 for topic_name in topic_names}\n",
    "    total_considered_topics = 0\n",
    "\n",
    "    for gt_result in gt_labels:\n",
    "        for j,gt_label in enumerate(gt_result['labels,word_indexes']):\n",
    "            gt_topic = topic_gesture_description[[el for el in gt_label[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "            total_positive_topics[gt_topic] += 1\n",
    "    for topic in topic_names:\n",
    "        if total_positive_topics[topic] > 0:\n",
    "            total_considered_topics += 1\n",
    "    print(\"total_topics:\",total_considered_topics)\n",
    "    for t in range(1,21,1): #compute all the info needed to compute average precision for all the possible thresholds of IOU\n",
    "        th = t/20\n",
    "        for topic in topic_names: #for each possible topic\n",
    "            print(topic,t)\n",
    "            if total_positive_topics[topic] == 0: #it does not make sense the computation of Average Precision (we can't compute the Recall)\n",
    "                continue\n",
    "            row = all_results[topic][th] #we will create rows in the results referred to the chosen topic and th. The final table will contain\n",
    "                                         #all the info needed to compute average precision for a topic with a given IOU threshold\n",
    "            acc_TP = 0\n",
    "            acc_FP = 0\n",
    "            n_comparisons = 0\n",
    "            #####for skip, (gt_result,pd_result) in enumerate(zip(gt_labels,pd_labels)): #for each sentence\n",
    "            #print(\"LEN:\", len(gt_labels))\n",
    "            for num,gt_result in enumerate(gt_labels):\n",
    "                #if t==1 and topic == 'r_greet':\n",
    "                    #print(num)\n",
    "                #break_flag = False\n",
    "                for pd_result in pd_labels:\n",
    "                    if pd_result['whole sentence'] != gt_result['sentence']: #check that the results are referred to the same sentence before continuing\n",
    "                        continue\n",
    "                    else:\n",
    "                        #print(pd_result)\n",
    "                        #print(pd_result['whole sentence'], gt_result['sentence'])\n",
    "                        start_index = 0 #we can take the results from the word in this index to avoid window overlapping\n",
    "                        pd_result_copy = take_elements_topic(pd_result, topic) ##all the results referred to some topic\n",
    "                        #print(pd_result_copy)\n",
    "                        #print(pd_result)\n",
    "                        new_pd_result = {'sentence': pd_result['whole sentence'], 'topics&ref':[],'word_indexes':[],'values':[]}\n",
    "                        n_comparisons += 1 #count how many GT-PD comparisons we made for debugging\n",
    "\n",
    "                        while start_index < len(pd_result_copy['whole sentence'].split()):\n",
    "                            best_match = {'value':0,'ref':'','matching_words':'','start_word_indexes':0,'window':1} #it contains the data related to the topic with highest confidence\n",
    "                            if type == 'moving_window':\n",
    "                                for window in list(pd_result_copy.keys())[1:-1]:\n",
    "                                    window = int(window.split(' ')[-1])\n",
    "                                    values = extract_values(start_index,pd_result_copy,window)\n",
    "                                    if values is False:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        #print(\"VALUES:\",values)\n",
    "                                        best_match = take_best_match(best_match,pd_result_copy,values,window,0)\n",
    "                                        #print(\"BEST MATCH:\",best_match)\n",
    "\n",
    "                            elif type == 'fixed_window':\n",
    "                                window = best_windows[topic]['best_window']\n",
    "                                #print(\"window\",window, \"len\", len(pd_result['whole sentence'].split()))\n",
    "                                if window > len(pd_result_copy['whole sentence'].split()):\n",
    "                                    break\n",
    "                                values = extract_values(start_index,pd_result_copy,window)\n",
    "                                #print(values)\n",
    "                                if values is False:\n",
    "                                    break\n",
    "                                else:\n",
    "                                    best_match = take_best_match(best_match,pd_result_copy,values,window,0)\n",
    "                            else:\n",
    "                                print(\"wrong type\")\n",
    "                                return\n",
    "                            #print(best_match)\n",
    "                            start_index += best_match['window']\n",
    "                            if best_match['value']==0:\n",
    "                                continue\n",
    "                            else:\n",
    "                                swi = best_match['start_word_indexes']\n",
    "                                new_pd_result['topics&ref'].append((topic, best_match['ref']))\n",
    "                                new_pd_result['word_indexes'].append([i for i in range(swi,swi + best_match['window'])])\n",
    "                                new_pd_result['values'].append(best_match['value'])\n",
    "                            #print(\"NEW PD RESULT:\",new_pd_result)\n",
    "\n",
    "                        #print(new_pd_result)\n",
    "                        #print(\"PD_COPY!!!!\", pd_result_copy['whole sentence'], len(pd_result_copy['whole sentence'].split()))\n",
    "                        #print(pd_result_copy[f\"window {best_match['window']}\"] if window<=len(pd_result_copy['whole sentence'].split()) else '')\n",
    "                        pd_result_copy.clear()\n",
    "                        #print(new_pd_result)\n",
    "                        detected_gt = {\"IOU\":[], \"Confidence\":[], 'gt_indexes':[]}\n",
    "                        for k,pd_match in enumerate(zip(new_pd_result['topics&ref'],new_pd_result['word_indexes'])):\n",
    "                            #print(\"I'm here\")\n",
    "                            #print(new_pd_result)\n",
    "                            #print(gt_result)\n",
    "                            #for k,pd_label_value in enumerate(tmp_values): #enumerate(zip(pd_result['topics&ref'],pd_result['word_indexes'])):\n",
    "                            pd_topic = pd_match[0][0]  #  #extract the pd_topic from the pd_label   #do the same for the predicted label\n",
    "                            if pd_topic != topic:\n",
    "                                #print(\"SKIPPING PREDICTED:\", k, pd_result)\n",
    "                                continue\n",
    "                            else: #right topic, check the matches with ground truths\n",
    "                            #print(\"NOT SKIPPED PD TOPIC\",skip,pd_result)\n",
    "                                n_intersections = 0\n",
    "                                tmp_IOU = 0\n",
    "                                tmp_conf = 0\n",
    "                                for j,gt_match in enumerate(gt_result['labels,word_indexes']):\n",
    "                                    gt_topic = topic_gesture_description[[el for el in gt_match[0].items()][0][1]][0] #extract the right topic from the gt label\n",
    "                                    if gt_topic != topic: #only if gt_topic and pd_topic are the same we can do the comparison\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        intersection = len(set(pd_match[1]).intersection(gt_match[1]))\n",
    "                                        if intersection != 0: #we append to a list that takes all the possible IOUs. Only the best one IOU is TP, for the other topics we have to check if they have predictions with the others GT\n",
    "                                            union = len(set(pd_match[1] + gt_match[1]))\n",
    "                                            IOU = intersection/union\n",
    "                                            n_intersections += 1\n",
    "                                            if IOU > tmp_IOU: #this allow to take only the best IOU between a PD and many GT, the others will be false negatives\n",
    "                                                tmp_conf = new_pd_result['values'][k]\n",
    "                                                tmp_IOU = IOU\n",
    "                                                gt_index = j\n",
    "                                        else:\n",
    "                                            continue\n",
    "                                if n_intersections == 0: #then the prediction has no intersections, so we have a false positive\n",
    "                                    row[\"Confidences\"].append(new_pd_result['values'][k])\n",
    "                                    row[\"FP\"].append(1)\n",
    "                                    row[\"TP\"].append(0)\n",
    "                                    #print(\"False Positive!!\",gt_result['labels,word_indexes'],new_pd_result['topics&ref'])\n",
    "                                else: #we have to do another check: for each GT, we have to check which is the predicted topic that has highest IOU\n",
    "                                      #with the current check we take only the GT that has highest IOU with a PD (if a GT has not a PD, then we will take into account later during the recall computation since this will be a FN)\n",
    "                                    detected_gt[\"IOU\"].append(tmp_IOU) #IOU\n",
    "                                    detected_gt[\"Confidence\"].append(tmp_conf) #confidence\n",
    "                                    detected_gt[\"gt_indexes\"].append(gt_index) #confidence\n",
    "                                    #print(\"Appending some result\")\n",
    "                        #print(detected_gt)\n",
    "                        if len(detected_gt[\"IOU\"]) == 0: #then we have no TP, only FP which is already considered and appended as rows\n",
    "                            #print(\"break\")\n",
    "                            break\n",
    "                        else:\n",
    "                            unique_indexes = set(detected_gt[\"gt_indexes\"])\n",
    "                            for idx in unique_indexes:\n",
    "                                gt_idx = [i for i, d in enumerate(detected_gt[\"gt_indexes\"]) if d == idx] #extract predictions related to same GT\n",
    "                                IOUs = [detected_gt[\"IOU\"][i] for i in gt_idx]\n",
    "                                max_IOU_idx = detected_gt[\"IOU\"].index(max(IOUs)) #take the best prediction related to a GT\n",
    "                                if max(IOUs) < th:\n",
    "                                    for i in gt_idx:\n",
    "                                        row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                        row[\"FP\"].append(1)\n",
    "                                        row[\"TP\"].append(0)\n",
    "                                else:\n",
    "                                    for i in gt_idx:\n",
    "                                        if i == max_IOU_idx: #True Positive\n",
    "                                            #print(detected_gt)\n",
    "                                            #print(pd_result_copy)\n",
    "                                            #print(gt_result)\n",
    "                                            row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                            row[\"FP\"].append(0)\n",
    "                                            row[\"TP\"].append(1)\n",
    "                                        else:\n",
    "                                            row[\"Confidences\"].append(detected_gt[\"Confidence\"][i])\n",
    "                                            row[\"FP\"].append(1)\n",
    "                                            row[\"TP\"].append(0)\n",
    "\n",
    "            #### Sort all the elements by confidence for each topic\n",
    "            sort_indexes = [i for i, x in sorted(enumerate(row[\"Confidences\"]), key=lambda x: x[1], reverse=True)]\n",
    "            #print(sort_indexes, row.keys())\n",
    "            for key in row.keys():\n",
    "                if key == \"Recall\" or key == \"Precision\" or key == \"Acc_FP\" or key == \"Acc_TP\":\n",
    "                    continue\n",
    "                row[key] = [row[key][i] for i in sort_indexes] #we order only Confidences, FP and TP\n",
    "            for i in range(len(row[\"Confidences\"])): #now compute Acc_TP, Acc_FP, Precisions and Recalls for each row\n",
    "                if row[\"TP\"][i] == 1:\n",
    "                    acc_TP += 1\n",
    "                else:\n",
    "                    acc_FP += 1\n",
    "                row[\"Acc_TP\"].append(acc_TP)\n",
    "                row[\"Acc_FP\"].append(acc_FP)\n",
    "                row[\"Precision\"].append(acc_TP / (acc_TP + acc_FP))\n",
    "                row[\"Recall\"].append(acc_TP / total_positive_topics[topic])\n",
    "\n",
    "    ######### COCO EVALUATOR\n",
    "    mean_average_precision = 0\n",
    "    #for iou in range(50,100,5): #### for each IOU, compute AP\n",
    "    iou = 50\n",
    "    th = iou/100\n",
    "    for topic in topic_names:\n",
    "        if total_positive_topics[topic] > 0:\n",
    "            topic_results = all_results[topic][th]\n",
    "            #fig = plt.figure()\n",
    "            #plt.plot(topic_results[\"Recall\"],topic_results[\"Precision\"])\n",
    "            #plt.pause(1)\n",
    "            if len(topic_results[\"Recall\"])<2:  #we need at least 2 points to find a curve and compute AUC\n",
    "                continue\n",
    "            AP = sklearn.metrics.auc(topic_results[\"Recall\"], topic_results[\"Precision\"]) * 100\n",
    "            #print(\"AP:\", AP)\n",
    "            average_precision[topic][th] = AP\n",
    "            final_average_precision[topic] += AP\n",
    "        else:\n",
    "            continue\n",
    "    for topic in topic_names:\n",
    "        final_average_precision[topic] = final_average_precision[topic] #/ 10 #now we have\n",
    "        mean_average_precision += final_average_precision[topic]\n",
    "    mean_average_precision = mean_average_precision/total_considered_topics\n",
    "    print(\"n_comparisons:\", n_comparisons)\n",
    "    #print(round(mean_average_precision,2),\"%\")\n",
    "\n",
    "    return average_precision, mean_average_precision\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "    #filename_best = \"new_all_processed_sentences_best.pkl\" #it contains all the possible results; a results is the best words match for each topic for each possible group of words inside the sentence, for all the sentences\n",
    "    filename_similarities = \"all_similarities_300.pkl\"#\"all_similarities_standard_base.pkl\"\n",
    "    filename_similarities_old = \"all_similarities_300_old.pkl\"\n",
    "    #filename_random = \"processed_sentences_random.pkl\" #it contains all the random labels\n",
    "    filename_labelled_sentences = \"labelled_sentences_final.pkl\" #it contains the ground truth labels\n",
    "    filename_labelled_sentences_old = \"labelled_sentences_final_old.pkl\" #it contains the ground truth labels\n",
    "    filename_windows = 'best_windows_sentences_standard_large_0_3.pkl'#'best_windows_sentences_standard_base_0_3.pkl'\n",
    "\n",
    "\n",
    "    if(os.path.exists(os.path.join(path,filename_windows))):    #load data already processed\n",
    "        b_windows = pickle.load(open(os.path.join(path,filename_windows),'rb'))\n",
    "    else:\n",
    "        print(\"Invalid path windows\")\n",
    "\n",
    "    if(os.path.exists(os.path.join(path,filename_similarities))):    #load data already processed\n",
    "        result = pickle.load(open(os.path.join(path,filename_similarities),'rb'))\n",
    "    else:\n",
    "        print(\"Invalid path for all the results\")\n",
    "\n",
    "    if(os.path.exists(os.path.join(path,filename_similarities_old))):    #load data already processed\n",
    "        result_old = pickle.load(open(os.path.join(path,filename_similarities_old),'rb'))\n",
    "    else:\n",
    "        print(\"Invalid path for all the results\")\n",
    "\n",
    "    if(os.path.exists(os.path.join(path,filename_labelled_sentences))):    #load data already processed\n",
    "        all_real_labels = pickle.load(open(os.path.join(path,filename_labelled_sentences),'rb'))\n",
    "        print(\"NEW\",len(all_real_labels))\n",
    "    else:\n",
    "        print(\"invalid real labels path\")\n",
    "\n",
    "    if(os.path.exists(os.path.join(path,filename_labelled_sentences_old))):    #load data already processed\n",
    "        all_real_labels_old = pickle.load(open(os.path.join(path,filename_labelled_sentences_old),'rb'))\n",
    "        print(\"OLD\",len(all_real_labels_old))\n",
    "    else:\n",
    "        print(\"invalid real labels path\")\n",
    "\n",
    "    #print(len(all_real_labels))\n",
    "\n",
    "    #print(avg_labels[0],\"\\n\",real_labels[0])\n",
    "    #all_AP_rl, mAP_rl = mean_average_precision(all_real_labels, all_random_labels)\n",
    "    #all_IOU_rl, mIOU_rl = mean_IOU(all_real_labels, all_random_labels)\n",
    "    all_real_labels = all_real_labels + all_real_labels_old\n",
    "    print(len(all_real_labels))\n",
    "    #print(result[0])\n",
    "    #print(b_windows)\n",
    "    all_AP_ml, mAP_ml = mean_average_precision(all_real_labels, result, b_windows, type = 'moving_window')\n",
    "    all_IOU_ml, mIOU_ml = mean_IOU(all_real_labels, result, b_windows, type = 'moving_window')\n",
    "    all_AP_fl, mAP_fl = mean_average_precision(all_real_labels, result, b_windows, type = 'fixed_window')\n",
    "    all_IOU_fl, mIOU_fl = mean_IOU(all_real_labels, result, b_windows, type = 'fixed_window')\n",
    "\n",
    "    #print(\"mAP random labels:\", round(mAP_rl,2),\"   mIOU random labels:\",round(mIOU_rl*100,2))\n",
    "    print(\"mAP fixed window labels:\", round(mAP_fl,2),\"     mIOU fixed window labels:\",round(mIOU_fl*100,2))\n",
    "    print(\"mAP moving window labels:\", round(mAP_ml,2),\"    mIOU moving window labels:\",round(mIOU_ml*100,2))\n",
    "    #print(all_real_labels[0],moving_window_labels[0])\n",
    "    #print(mAP)\n",
    "    #print(mIOU*100)\n",
    "    #print(\"m_IOU\",all_IOU)\n",
    "    #print(\"mAP\", all_AP)\n",
    "    #print(sentences_already_labelled[3:])\n",
    "    #print(avg_labels[0],\"\\n\",all_real_labels[199])\n",
    "    #for el in result:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "############ OLD. DO NOT CONSIDER\n",
    "\n",
    "### COMPUTE RESULTS FOR FIXED WINDOW\n",
    "\n",
    "# give priority to each topic. Use the best window of each topic to take data of the sentence. If, for a topic, we have overlapping windows, chose the one that returns the best value.\n",
    "# After we choose a topic given the value, the window size and the priority, all the words of that window can't be assigned to any other topic\n",
    "\n",
    "\n",
    "\n",
    "def process_results_priority_policy_best_windows(best_windows = b_windows ,result = results, new_th = 0.2, type = 'conversation'):\n",
    "\n",
    "    def take_elements_th(el, window_dim, new_th):\n",
    "        all_values = el[f\"window {window_dim}\"]['value']  #all the values (results of the matches between topics and parts of the sentence\n",
    "        th_indexes = [i for i in range(len(all_values)) if all_values[i] > new_th] #returns the indexes of the values that are greater than new_th\n",
    "        values = list(map(all_values.__getitem__,th_indexes)) #take all the values greater than new_th\n",
    "        refs = list(map(el[f\"window {window_dim}\"]['ref'].__getitem__,th_indexes)) #take all the references that return a value greater than new_th\n",
    "        matching_words = list(map(el[f\"window {window_dim}\"]['sentence'].__getitem__,th_indexes)) #take all the sentence parts that return a value greater than new_th\n",
    "        start_word_indexes = list(map(el[f\"window {window_dim}\"]['start_word_indexes'].__getitem__,th_indexes))\n",
    "\n",
    "        return values, refs, matching_words, start_word_indexes\n",
    "\n",
    "    final_labels=[]\n",
    "    topics_ordered = sorted(best_windows.items(), key=lambda x: x[1]['priority'], reverse=True) #order the topics on priority values\n",
    "    window_max_dim = 10\n",
    "    for k,el in enumerate(result):\n",
    "        sentence_labels = {'sentence':'', 'topics&ref':[],  'matching_words':[], 'values':[], 'priority_levels':[],\n",
    "                           'sentence_index':k, 'word_indexes':[]} #it contains a sentence labelled\n",
    "        #print(sentence_labels.keys())\n",
    "        if type == 'conversation':\n",
    "            sentence = el['whole sentence,speaker,turn'][0]\n",
    "            speaker = el['whole sentence,speaker,turn'][1]\n",
    "            turn = el['whole sentence,speaker,turn'][2]\n",
    "        elif type == 'sentences':\n",
    "            sentence = el['whole sentence']\n",
    "        else:\n",
    "            print(\"Undefined element\")\n",
    "            return\n",
    "\n",
    "        sentence_labels['sentence'] = sentence\n",
    "        sentence_splits = sentence.split()\n",
    "        final_word_index = len(sentence_splits) - 1 #index of the last word (starting from 0)\n",
    "        current_word_index = 0\n",
    "        #idx_last_word  = current_word_index\n",
    "        break_flag = False #if true,  we found the right label to a set of words in the sentence, so we can continue\n",
    "\n",
    "        topic_index = 0\n",
    "        #last_processed_word_idx = -1\n",
    "        #processed_words = sentence_splits[0]\n",
    "\n",
    "        if len(sentence_splits) < window_max_dim:\n",
    "            window_max_d = len(sentence_splits)\n",
    "        else:\n",
    "            window_max_d = window_max_dim\n",
    "\n",
    "        sentence_results = {}\n",
    "        last_match_indexes = {}\n",
    "        for i in range(1,window_max_d + 1): #take all the data related to the matches greater than a threshold for a given sentence\n",
    "            values, refs, matching_words, start_word_indexes = take_elements_th(el, i, new_th)\n",
    "            sentence_results[f\"window {i}\"] = {'matching_words':[], 'refs':[], 'values':[]}\n",
    "            sentence_results[f\"window {i}\"]['values'] = values\n",
    "            sentence_results[f\"window {i}\"]['matching_words'] = matching_words\n",
    "            sentence_results[f\"window {i}\"]['refs'] = refs\n",
    "            sentence_results[f\"window {i}\"]['start_word_indexes'] = start_word_indexes\n",
    "            last_match_indexes[f\"window {i}\"] = 0 #this dict will contain, for each window_dim, the index of the first element from which we will start the search in\n",
    "\n",
    "        while topic_index < len(topics_ordered) and current_word_index <= final_word_index:\n",
    "            topic_name = topics_ordered[topic_index][0]\n",
    "            window_dim = topics_ordered[topic_index][1]['best_window']\n",
    "            #start_search_index = last_match_indexes[f\"window {window_dim}\"] #we take the data from this index since the previous ones were already used\n",
    "            #print(processed_words, \"    idx last match\", idx_last_matches[f\"window {window_dim}\"], \"    len results\",len(s_results['matching_words']))\n",
    "            #print(\"window_dim:\",window_dim)\n",
    "            #print(\"processed words:\",processed_words, \"  window_dim:\", window_dim, \"  dim_max:\",max_dim, \"  start_search_index:\",start_search_index)\n",
    "            #print(\"RESULTS:\", s_results['matching_words'][start_search_index:], \"window_dim:\", window_dim, processed_words)\n",
    "            if window_dim == 0 or f\"window {window_dim}\" not in sentence_results.keys(): #window dim is 0 if we didn't find any match during avg window computation\n",
    "                topic_index +=  1\n",
    "                continue\n",
    "            s_results = sentence_results[f\"window {window_dim}\"]\n",
    "            words_in_window = ' '.join(sentence_splits[current_word_index : current_word_index + window_dim])\n",
    "            #it means that we have no more results for that window or that with that we reach the end of the sentence\n",
    "            if last_match_indexes[f\"window {window_dim}\"] > (len(s_results['matching_words']) - 1) or current_word_index + window_dim > final_word_index:\n",
    "                topic_index +=  1\n",
    "                continue\n",
    "\n",
    "            start_search_index = last_match_indexes[f\"window {window_dim}\"]\n",
    "            if words_in_window in s_results['matching_words'][start_search_index:]: #then we have a match with a topic and the words in the window\n",
    "                #win_results = el[f\"window {window_dim}\"] #all the results that we have for a given window dimension\n",
    "                #values = win_results['value']   #all the values referred to the topics that return result greater than a threshold\n",
    "                index = s_results['matching_words'][start_search_index:].index(words_in_window)  #since we have at least one topic > threshold, we take the first index of the first topic that match\n",
    "                n_possible_topics = 0\n",
    "                #print(\"HERE:\", s_results['matching_words'][start_search_index:][index:], processed_words)\n",
    "                while s_results['matching_words'][start_search_index:][index:][n_possible_topics] == words_in_window: #count how many topics match withe processed words\n",
    "                    n_possible_topics = n_possible_topics + 1\n",
    "                    #print(n_possible_topics)\n",
    "                    if n_possible_topics >= len(s_results['matching_words'][start_search_index:][index:]): #avoid taking elements that are outside the array\n",
    "                        break\n",
    "                # these values are associated with the topics that match the # processed words\n",
    "                all_match_values = s_results['values'][start_search_index:][index : index + n_possible_topics]\n",
    "                all_indexes = [i for i in range(index, index + n_possible_topics)]\n",
    "                #tmp_match = ordered_values[i]\n",
    "                    #print(tmp_match)\n",
    "                    #print(\"match:\",tmp_best_match,\"  window_dim:\",window_dim, \"  start_search_index\", start_search_index)\n",
    "                #all_best_values = tmp_values[index : index + n_possible_topics] #all the values of the topics that match with the words in the window\n",
    "                for value,ind in zip(all_match_values,all_indexes): #check if the current topic has a match. If no, skip it and go to the next one by following the priorities\n",
    "                    tmp_ref = s_results['refs'][start_search_index:][s_results['values'][start_search_index:].index(value)] #reference associated to best result\n",
    "                    tmp_topic = [key for key in all_refs.keys() for ref in all_refs[key] if tmp_ref == ref][0] #extract the topic associated with reference\n",
    "                    #ref = tmp_ref[ind] #reference associated to the result\n",
    "                    #tmp_topic = [key for key in all_refs.keys() for tmp_ref in all_refs[key] if ref == tmp_ref][0] #extract the topic associated with reference\n",
    "                    if topic_name != tmp_topic: #check if another topic is greater than new_th\n",
    "                        continue\n",
    "                    else:\n",
    "                        w_dim=1\n",
    "                        last_word_idx = current_word_index + window_dim\n",
    "                        while current_word_index + w_dim <= len(sentence_splits) - 1 and w_dim <= window_max_d:    #update search indexes\n",
    "                            s_res = sentence_results[f\"window {w_dim}\"]\n",
    "                            if len(s_res['start_word_indexes']) != 0:  #then\n",
    "                                #print(\"IDX LAST WORD:\",tmp_idx_last_word)\n",
    "                                tmp = [i for i in range(len(s_res['start_word_indexes'])) if s_res['start_word_indexes'][i] > last_word_idx]\n",
    "                                #print(f\"TMP INDEXES DIM :{window_dim}, RES:\", tmp_idx_last_matches[f\"window {window_dim}\"])\n",
    "                                if len(tmp) > 0: #it means that we have at least one value that will possibly match\n",
    "                                    last_match_indexes[f\"window {w_dim}\"] = tmp[0]\n",
    "                                else:\n",
    "                                    last_match_indexes[f\"window {w_dim}\"] = len(s_res['matching_words']) #we can avoid to search again\n",
    "                            w_dim = w_dim + 1\n",
    "                        #print(sentence_labels.keys())\n",
    "                        sentence_labels['topics&ref'].append((topic_name,tmp_ref)) #save the result\n",
    "                        sentence_labels['matching_words'].append(words_in_window)\n",
    "                        sentence_labels['values'].append(value)\n",
    "                        sentence_labels['priority_levels'].append(topic_index+1)\n",
    "                        sentence_labels['word_indexes'].append([i for i in range(current_word_index,current_word_index + window_dim)])\n",
    "                        current_word_index = current_word_index + window_dim + 1\n",
    "                        break_flag = True\n",
    "                        break #break the loop and restart with another topic\n",
    "                topic_index = topic_index + 1 #restart from next topic\n",
    "                if break_flag == True:\n",
    "                    topic_index = 0 #we found the topic, restart the topic search from the current_word_index and reset the break_flag\n",
    "                    break_flag = False\n",
    "            else:\n",
    "                topic_index = topic_index + 1\n",
    "                if topic_index >= len(topics_ordered):\n",
    "                    current_word_index = current_word_index + 1\n",
    "                    topic_index = 0\n",
    "                continue #try with another topic. Changing topic means that we change the size of window so we take different words to compare. If no topics match, go to the next word\n",
    "        final_labels.append(sentence_labels)\n",
    "    return final_labels\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\final_results\"\n",
    "filename = 'all_similarities_600.pkl'\n",
    "#filename_best = \"new_all_processed_sentences_best.pkl\"\n",
    "#filename_local = \"all_processed_sentences_local.pkl\"\n",
    "#filename_windows = \"best_windows_200_sentences.pkl\"\n",
    "filename_windows = 'best_windows_sentences.pkl'\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename))):    #load data already processed\n",
    "    results = pickle.load(open(os.path.join(path,filename),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path best\")\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_windows))):    #load data already processed\n",
    "    b_windows = pickle.load(open(os.path.join(path,filename_windows),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path windows\")\n",
    "\n",
    "labels = process_results_priority_policy_best_windows(best_windows=b_windows,result=results, new_th=.3, type = 'sentences')\n",
    "filename_labels =  \"results_priority_policy_best_windows.pkl\"\n",
    "print(\"writing to file...\")\n",
    "pickle.dump(labels  , open(os.path.join(path,filename_labels),'wb'))\n",
    "\n",
    "for label in labels:\n",
    "    print(label, \"\\n \\n \\n \\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "gestures",
   "language": "python",
   "display_name": "gestures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
